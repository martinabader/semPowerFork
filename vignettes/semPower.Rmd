---
title: "Power Analysis for Structural Equation Models: semPower 2 Manual"
author: "Morten Moshagen & Martina Bader"
date: "`r Sys.Date()`"
output:
#   bookdown::pdf_book
   bookdown::gitbook:
     config:
       toc:
         collapse: none
         scroll_highlight: yes
     split_by: none
     self_contained: true
     search:
       engine: lunr
       options: null
     sharing:
       facebook: false
       twitter: false
     base_format: rmarkdown::html_vignette

vignette: >
  %\VignetteIndexEntry{Power Analysis for Structural Equation Models: semPower 2 Manual}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---


```{=html}
<style type="text/css">
.book .book-body .page-wrapper .page-inner {
  max-width: 95%; !important;
}
.book .book-summary {
  width: 400px;
  position:absolute;
  top:0;
  left:-400px;
}
.book.with-summary .book-header.fixed {
    left: 400px;
}
.book.with-summary .book-body {
    left: 400px;
}
</style>
```

```{r load-packages, include=FALSE}
library(semPower)
```

Power Analysis for Structural Equation Models: semPower 2 Manual

Contact: <morten.moshagen@uni-ulm.de>

Please cite as:

* Moshagen, M., & Erdfelder, E. (2016). A new strategy for testing structural equation models. *Structural Equation Modeling, 23*, 54–60. [https://doi.org/10.1080/10705511.2014.950896](https://doi.org/10.1080/10705511.2014.950896)


##### Installation{-}

The `semPower` package can be installed via [CRAN](https://cran.r-project.org/package=semPower). The latest development version is available from github at [https://github.com/moshagen/semPower](https://github.com/moshagen/semPower) and can be installed as follows:

```{r eval=FALSE}
# install.packages("devtools")
devtools::install_github("moshagen/semPower")
```

(Very) basic functionality is also provided as a shiny app, which can be used online at [https://sempower.shinyapps.io/sempower](https://sempower.shinyapps.io/sempower).


# Introduction

`semPower` provides a collection of functions to perform power analyses for structural equation models. Statistical power is a concept arising in the context of classical (frequentist) null-hypothesis significance testing and is defined as the probability to reject a certain hypothesis, if this hypothesis is factually wrong. As a general rule, a hypothesis test is only meaningful to the extent that statistical power is reasonably high, because otherwise a non-significant test outcome carries little information regarding the veracity of the tested hypothesis.

For illustration, consider a simple two-factor CFA model and assume that the interest lies in detecting that the correlation between the factors differs from zero. To test the hypothesis that the factors are uncorrelated, one could fit a model that restricts the correlation between these factors to zero. If the model test turns out significant, the hypothesis of a zero correlation between the factors is rejected, in turn informing the conclusion that the correlation between the factors differs from zero. Statistical power now gives the probability that the outcome of the model test associated with this hypothesized model is significant on a certain alpha error level with a certain sample size. Suppose that the correlation between the factors is $r = .20$ in the population, $\alpha = .05$ and that the hypothesis of a zero correlation is tested based on a sample size of $N = 125$. Then, the probability of obtaining a significant test outcome to detect a correlation of $r \geq .20$ is just 20%. Stated differently, in 4 out of 5 random samples (each of size $N = 125$) one will *not* detect that the factors are actually correlated. Correspondingly, statistical power is an integral part in planning the required sample size and statistical hypothesis testing more generally. 

## Types of power analyses

Generally, statistical power depends on

* the extent to which the tested hypothesis is wrong (= the magnitude of effect)
* the sample size (N)
* the alpha error (alpha)
* the degrees of freedom (df)

and will be higher for a larger effect, a larger sample size, a higher alpha error, and fewer degrees of freedom. When performing a power-analysis, one of these quantities is computed as function of the other quantities, giving rise to different types of power analyses:

* **A priori power analysis**: Determines the required sample size to detect a certain effect with a desired power, given alpha and df. 
* **Post-hoc power analysis**: Determines the achieved power to detect a certain effect with a given sample size, alpha and df. 
* **Compromise power analysis**: Determines the alpha and the beta error, given the alpha/beta ratio, the sample size, a certain effect, and the df. 

Each type of power analysis has different aims. A priori power analyses are performed prior to data collection to inform the required number of observations to detect an expected effect with the desired power. Post-hoc power analyses are performed after data collection to judge whether the achieved power with the realized sample size is sufficiently high to allow for a meaningful test of a certain hypothesis. Compromise power analysis are used to determine which decision rule to apply in evaluating whether the outcome of a hypothesis test favors the null or the alternative hypothesis (see Moshagen & Erdfelder, 2016, for details).

## Types of hypotheses

Statistical power is always tied to a particular hypothesis, so power will typically differ even for the same (base-)model depending on which hypothesis is considered. A general statement such as "power was 80%" is meaningless, unless the hypothesis for which power was determined is also stated (such as "power to detect a correlation $\geq .2$ between the first and the second factor was 80%."). Moreover, power can be high for one hypothesis, but low for other hypotheses, so a power analysis should always be performed concerning (a) the focal hypotheses and (b) the hypothesis where the smallest effect is expected.

In SEM, the typical types of hypotheses that occur either implement equality constraints on two or more parameters (such as cross-group constraints) or assign a particular parameter a specific value (such as zero).^[A third type implements order constraints on parameters, but this is associated with certain intricacies concerning the limiting distributions and not covered by `semPower`.] For instance, consider a cross-lagged panel model (CLPM) with two constructs *X* and *Y* measured by three indicators each at three different waves (so the full model comprises six factors). Relevant (non-exhaustive) hypotheses could be 

* whether the model as a whole describes the data.
* whether measurement invariance over time concerning the indicators holds.
* whether the autoregressive effects of *X* are constant across waves.
* whether the autoregressive effects of *X* are different from zero.
* whether the autoregressive effects of *Y* are constant across waves.
* whether the autoregressive effects of *Y* are different from zero.
* whether the autoregressive effects of *X* are equal to those of *Y*.
* whether the cross-lagged effects of *X* on *Y* are constant across waves.
* whether the cross-lagged effects of *Y* on *X* are constant across waves.
* whether the cross-lagged effects of *X* on *Y* are different from zero.
* whether the cross-lagged effects of *Y* on *X* are different from zero.
* whether the cross-lagged effects of *X* on *Y* are equal to those of *Y* on *X*.
* whether the synchronous (residual) correlations between *X* and *Y* differ from zero.
* whether the synchronous (residual) correlations are equal over waves.

Any of these hypotheses will likely be associated with a different magnitude of effect and consequently with a different power to detect this effect. For example, concerning the hypothesis whether an autoregressive effect differs from zero, one will usually be satisfied to obtain sufficient power to detect a large regression coefficient (of, say, $b \geq .50$). Concerning the cross-lagged effects, however, one rather wants sufficient power to detect a small regression coefficient (of, say, $b \geq .10$). Furthermore, a cross-lagged effect of *X* on *Y* of .10 will not always be associated with the same power as a cross-lagged effect of *Y* on *X* of the same magnitude, because power to detect a cross-lagged effect also depends on all other parameters of the model, such as autoregressive effects and loadings. 

Correspondingly, one should carefully define a relevant hypothesis of interest when performing a power analysis.

## Performing power analyses

Next to defining a relevant hypothesis, performing a power analysis (obviously)  requires a decision on which type of power analysis to perform. The types of power analyses available in `semPower` are:

* [semPower.aPriori](#ap) to perform an a priori power analysis (i .e., determine the required sample size).
* [semPower.postHoc](#ph) to perform a post-hoc power analysis (i. e., determine the achieved power).
* [semPower.compromise](#cp) to perform a compromise power analysis (i. e., determine a reasonable decision rule).

Any power analysis requires to specify the magnitude of effect that is to be detected in a certain metric. The functions stated above understand the following [effect-size measures](#effects): F0, RMSEA, Mc, GFI, and AGFI. Because any of these effect-measures apply equally regardless of the particular type of model considered, the function above are also referred to as [model free power analyses](#modelFreePower). For example, the statement that power to reject a model involving 100 df exhibiting misspecifications corresponding to RMSEA $\geq .05$ on alpha = .05 with a sample size of 250 is 1 - beta = 97% is always true, regardless of whether the model under scrutiny is a CFA model, a CLPM, a multigroup model, or any other SEM model. It is the very nature of effect-sizes that they are agnostic with respect to how a particular model looks like. 

However, a common problem in performing a power analysis is that it is often difficult to translate a specific hypothesis into a specific value for a specific effect size (such as a specific value for the RMSEA). Consider the situation that one is interested in determining whether two factors in a CFA model are correlated. A suitable model to test this hypothesis would constrain the correlation between these factors to zero. When this constrained model fits the data as well as the unconstrained model (freely estimating the correlation), both factors can be assumed to be orthogonal. Otherwise, one would conclude that the factors are correlated. Suppose that a correlation between these factors of $r \geq .1$ is considered a meaningful deviation of orthogonality. In terms of power analyses, we thus want sufficient power to identify whether the correlation between the factors is at least $r = .1$. The misfit associated with a model assuming a correlation of 0 when, in reality, the true correlation is at least $r = .1$ is supposed to define the magnitude of effect. The problem is now that one cannot immediately say how this difference in a certain model parameter (the correlation between the factors) translates to an effect size such as the RMSEA. 

For this reason, `semPower` also provides various convenience functions that allow for a [model-based definition](#modelBasedPower) of the effect of interest in terms of model parameters as well as a more generic definition of the effect as function of [population and model-implied means and covariance matrices](#InputCovMatrix). In the example above, the relevant convenience function ([semPower.powerCFA](#powerCFA)) just requires the definition of the factor model and the specification of the to-be detected correlation as input, and plugs the associated effect-size (which is RMSEA = .05 in the scenario above when each factor is measured by three indicators loading by .5 each, and when df = 1) into one of the model-free power functions. Currently, `semPower` provides model-based power analyses for the following model types:

* Hypotheses arising in a [standard CFA model](#powerCFA).
* Hypotheses arising in models involving a [bifactor structure](#powerBifactor).
* Hypotheses related to [latent regressions](#powerRegression).
* Hypotheses related to [mediation models](#powerMediation).
* Hypotheses related to [generic path models](#powerPath).
* Hypotheses related to [measurement invariance across groups](#powerMI).
* Hypotheses arising in a [cross-lagged panel model](#powerCLPM).
* Hypotheses arising in a [random intercept cross-lagged panel model](#powerRICLPM).
* Hypotheses arising in a [generic structural equation model](#powerLav).

The remainder of this document provides some notes on the statistical background, a formal definition of decision errors in hypothesis testing, statistical power, and various effect-sizes, and a detailed description of the functions contained in this package.

# Statistical Background

This chapter provides a brief statistical background on hypothesis testing,  model estimation, and effect sizes in SEM.

## Hypothesis Testing and Statistical Power

The statistical evaluation of mathematical models often proceeds by considering a test-statistic that expresses the discrepancy between the observed data and the data as implied by the fitted model. In SEM, the relevant test statistic for a sample of size $N$ is given by $T = \hat{F}(N-1)$. $\hat{F}$ denotes the minimized sample value of the chosen discrepancy function (such as Maximum-Likelihood) and thereby indicates lack of fit of the model to the sample data. Thus,  $T$ permits a likelihood-ratio test of the null hypothesis (H0) that the model is correct. If the hypothesized model holds in the population, $T$ can be shown to follow asymptotically a central $\chi^2$(df) distribution with $df = .5 \cdot p(p+1) - q$ degrees of freedom, where $p$ is the number of manifest variables and $q$ denotes the number of free parameters. This is why $T$ is often referred to as “chi-square model test statistic” -- a convention which is followed here. 

Based on the observed value for the chi-square test statistic, a null hypothesis significance test can be performed to evaluate whether this value is larger than would be expected by chance alone. The usual test proceeds as follows: Given a certain specified alpha-error level (typically alpha = .05), a critical chi-square value is obtained from the asymptotic central $\chi^2$(df) distribution. If the observed value for the chi-square test statistic exceeds the critical value, the null hypothesis that the model fits the data is rejected. Otherwise, H0 is retained. A finding of the observed statistic exceeding the critical value (implying an upper-tail probability  that falls below the specified alpha level) thus leads to the statistical decision that the discrepancy between the hypothesized and the actual population covariance matrix is too large to be attributable to sampling error only. Accordingly, a statistically significant chi-square test statistic provides evidence against the validity of the hypothesized model.

When testing statistical hypotheses using this framework, two types of decision errors can occur: The alpha error (or type-I error) of incorrectly rejecting a true H0 (a correct model) and the beta error (or type-II error) of incorrectly retaining a false H0 (an incorrect model). Statistical power is defined as the complement of the beta-error probability (1 - beta) and thus gives the probability to reject an incorrect model. 

If the H0 is false, the chi-square test statistic is no longer central $\chi^2$(df) distributed, but can be shown to follow a noncentral $\chi^2$(df, $\lambda$) distribution with non-centrality parameter $\lambda$ and expected value df + $\lambda$ (MacCallum, Browne, & Sugawara, 1996). The non-centrality parameter $\lambda$ shifts the expected value of the non-central $\chi^2$(df, $\lambda$) distribution to the right of the corresponding central distribution. Having determined the critical value associated with the desired alpha probability from the central $\chi^2$(df) distribution, the beta-error probability can be computed by constructing the corresponding non-central $\chi^2$(df, $\lambda$) distribution with a certain non-centrality parameter $\lambda$ and obtaining the area (i.e., the integral) of this distribution to the left of the critical value:

$$ \beta = \int_{0}^{\chi^2_{crit}} f_{\chi^2(df, \lambda)}(x) \, dx$$
Correspondingly, statistical power is the area of the non-central $\chi^2$(df, $\lambda$) distribution to the right of the critical value, i. e., $= 1 - \beta$. The general situation is illustrated in the following figure.

```{r figure1, echo=FALSE, fig.cap = "Central (red) and non-central (blue) chi-square distributions."}
semPower.showPlot(chiCrit = 124.3421, ncp = 40.75, df = 100)
```

The figure depicts a central (solid)  $\chi^2$(df = 100) and a non-central (dashed)  $\chi^2$(df = 100, $\lambda = 40.75$). The area of the central distribution $\chi^2$(df) to the right of the critical value reflects the alpha error. The dotted line indicates a critical value of 124, which corresponds to alpha = .05. The area of $\chi^2$(df, $\lambda$) distribution to the left of the critical value is the beta-error probability, which in this example  takes a value of beta = .20. Statistical power is defined as 1 - beta, that is, the area under the noncentral $\chi^2$(df, $\lambda$) distribution to the right of the critical value.


## Measures of Effect {#effects}
As evident from the above, power depends on the applied critical value corresponding to a certain alpha error probability and on the distance between the central and the non-central $\chi^2(df)$ distributions as quantified by the non-centrality parameter $\lambda$. The non-centrality parameter $\lambda$, in turn, depends on the number of observations $N$ and on the degree to which the the tested H0 is factually wrong, i. e., on the discrepancy between the H0 and the H1 model (the effect size). 

To define the discrepancy between the H0 and the H1 model for power analysis, any non-centrality based measure of effect can be used. For [model-free power analyses](#modelFreePower), `semPower` understands the measures detailed below. Any [model-based power analysis](#modelBasedPower) is eventually converted into the [population minimum of the fit function](#effectsF0) as effect-size.

##### F0  {-#effectsF0}

$F_0$ is the population minimum of the Maximum-Likelihood fitting function, defined as

$$F_0 = \log|\Sigma| - \log|\hat{\Sigma}| + tr(\hat{\Sigma}\Sigma) - p  + (\mu - \hat{\mu}) \hat{\Sigma}^{-1} (\mu - \hat{\mu}) $$
where $\Sigma$ is the $p \cdot p$ population covariance matrix, $\hat{\Sigma}$ the $p \cdot p$ model-implied covariance matrix, $p$ the number of observed variables, $\mu$ the vector of population means, and $\hat{\mu}$ the model-implied means. If means are not part of the model, the second part becomes zero.

If the model is correct, $\hat{\Sigma} = \Sigma$, $\hat{\mu} = \mu$, and thus $F_0 = 0$. Otherwise, $F_0 > 0$ with higher values expressing a larger discrepancy (misfit) of the model to the data. 

If fitting a model to some sample data of size $N$, the estimated minimum of the fit-function, $\hat{F}$, is used to construct an asymptotically $\chi^2$-distributed model test-statistic, commonly simply called the chi-squared model test: 

$$\chi^2 = \hat{F}(N-1)$$
Note that $\hat{F}$ is a biased estimate of $F_0$. If $F_0 = 0$, the  expected value of $\hat{F}$ is df, i. e. the model degree of freedom. For a model with $q$ free parameters, the df are given by
$$ df = \dfrac{p(p+1)}{2} - q  $$

Whereas $F_0$ is the genuine measure of effect in SEM, the main disadvantage is that specific values are difficult to interpret, because of its logarithmic scaling and because specific values also depends on features unrelated to model fit such as a the number of manifest variables comprised on the model. For these reasons, various transformations of $F_0$ exist that are described in the following.


##### RMSEA {-} 
The Root-Mean-Squared Error of Approximation (RMSEA; Browne & Cudeck, 1992; Steiger & Lind, 1980) scales $F_0$ by the model degrees of freedom:

$$RMSEA = \sqrt{(F_0/df)}$$
so that the RMSEA is bounded by zero and lower values indicate better fit. The implied $F_0$ is:
$$F_0 = df \cdot RMSEA^2$$
Given that $F_0$ is scaled by the df, defining an effect in terms of the RMSEA requires specification of the degrees of freedom. 

##### Mc {-} 

McDonald's (1989) measure of non-centrality (Mc) is a transformation of $F_0$ on the interval 0-1 with higher values indicating better fit.

$$Mc = e^{-.5F_0}$$
so that
$$F_0 = -2\ln{Mc}$$



##### GFI {-} 

The Goodness-of-Fit Index (GFI; Jöreskog & Sörbom, 1984; Steiger, 1990) scales $F_0$ on the interval 0-1 with higher values indicating better fit:

$$GFI = \dfrac{p}{p+2F_0}$$

$$F_0 = \dfrac{p (1-GFI)}{2GFI}$$
As the GFI depends on the number of observed variables ($p$), this number need to be provided when defining an effect in terms of the GFI.


##### AGFI {-} 

The Adjusted Goodness-of-Fit Index (AGFI; Jöreskog & Sörbom, 1984; Steiger, 1990) modifies the GFI by including a penalty for the number of free parameters, as measured by the model degrees of freedom: 

$$AGFI = 1 - \dfrac{p(p+1)}{2df} \left(1 - \dfrac{p}{p+2F_0} \right)$$
$$F_0 = \dfrac{p (1-AGFI) df}{p(p+1) -2df(1-AGFI)}$$

Specifying an effect in terms of the AGFI requires specification of both the number of observed variables ($p$) and the model degrees of freedom ($df$).

&nbsp;


##### Measures not based on non-centrality  {-} 

Fit-indices that are not based on non-centrality have no straightforward relation to $F_0$ and are thus not well suited for power-analyses. However, when the input parameters include covariance matrices, `semPower` also reports the following measures.

###### SRMR {-}
The Standardized-Root-Mean-Square Residual (SRMR) is a measure of the (root of the) average (squared) difference between the (standardized) model-implied and population covariance matrices, so that it ranges from 0 to 1 with lower values indicating better fit. Let $E_0$ be the difference between the model-implied and the population covariance matrix, $E_0 = \Sigma - \hat{\Sigma}$, $vech$ denote the vectorization transformation, and $Q$ be a diagonal matrix of dimension $.5p(p+1)$ containing the inverse of the product of standard deviations of observed variables $i$ and $j$. Then, the SRMR can be defined as

$$SRMR = \sqrt{\dfrac{1}{.5p(p+1)}vech(E_0) \cdot Q \cdot vech(E_0)'}$$

The relation of the residual matrix $E_0$ to $F_0$ is complex and depends on the model-implied covariance matrix, so the SRMR is not well suited to define an effect in terms of $F_0$ (based on ML estimation): 
$$F_0 = -\ln|I + \hat{\Sigma}^{-.5} E_0 \hat{\Sigma}^{-.5}|$$


###### CFI {-}
The Comparative Fit Index (CFI) is an incremental index expressing the proportionate reduction of misfit associated with the hypothesized model ($F_{0H}$) in relation to the null-model ($F_{0N}$), defined as a model that constrains all covariances to zero. In the population, the CFI ranges from 0 to 1 with higher values indicating better fit.

$$CFI = \dfrac{F_{0N}-F_{0H}}{F_{0N}}$$

Whereas it is simple to obtain $F_0$ from the CFI, this requires knowledge of $F_{0N}$, which is rather difficult to determine a priori:

$$F_0 = F_{0N} - CFI \cdot F_{0N} $$

&nbsp;


# Model-free power analyses {#modelFreePower}

Performing a power analysis generally requires the specification of a measure and magnitude of effect that is to be detected and a provision of the model degrees of freedom (df). Further arguments are required depending on the type of power analysis.

The functions described in this chapter are "model-free" in the sense that the results depend on the df of a model, but are otherwise agnostic with respect to how a particular model looks like. For instance, the power to reject a model with df = 100 exhibiting an RMSEA $\geq$ .05 with N = 500 on alpha = .05 is always the same, regardless of whether the model is a CFA model, a mediation model, a CLPM, or a multigroup model.

By contrast, [model-based power analyses](#modelBasedPower) define the effect of interest in terms of particular model parameters, so different functions are required for different types of models. However, the functions performing model-based power analysis are actually only a high level interface and eventually transform a particular hypothesis concerning the model parameters into an effect size understood by model-free power analyses.

Thus, regardless of whether the effect of interest is directly defined in terms of an [effect size](#effects) understood by `semPower` or indirectly via constrains on particular model parameters, the actual power analysis is always performed by one of the following functions. 


## A-priori power analysis: Determine N{#ap}

The purpose of a-priori power analyses is to determine the required sample size to detect an effect with a certain probability on a specified alpha error. In the language of structural equation modeling, an a-priori power analysis asks: How many observations do I need to detect the effect of interest (i. e., falsify the model under scrutiny) with a certain probability (statistical power)?

Performing an a-priori power analyses requires the specification of:

* the alpha error (`alpha`)
* the desired power (`power`; or, equivalently, the acceptable beta error, `beta`)
* the type of effect (`effect.measure`) 
* the magnitude of effect (`effect`)
* the degrees of freedom of the model (`df`). See [how to obtain the df](#getDf) if you are unsure.

Depending on the chosen effect-size measure, it may also be required to define the number of observed variables (`p`). 

Suppose, one wants to determine the required sample size to detect misspecifications of a model (involving df = 100 degrees of freedom) with a power of 80% on an alpha error of .05., where the amount of misfit corresponds to an RMSEA of at least .05. To achieve this, the function `semPower.aPriori` is called with arguments `effect = .05`, `effect.measure = 'RMSEA'`, `alpha = .05`, `power = .80`, and `df = 100`. The results are stored in a list called `ap`. 

```{r}
ap <- semPower.aPriori(effect = .05, effect.measure = 'RMSEA', 
                        alpha = .05, power = .80, df = 100)
```

Equivalently, instead of calling the `semPower.aPriori`, one may also use the generic `semPower` function with the additional `type = 'a-priori'` argument: 

```{r eval=FALSE}
ap <- semPower(type = 'a-priori', 
               effect = .05, effect.measure = 'RMSEA', 
               alpha = .05, power = .80, df = 100)
summary(ap)
```

Calling the `summary` method on `ap` prints the results and a figure of the associated central and non-central chi-squared distributions.    

This shows that N = 164 yields a power of approximately 80% to detect the specified effect. The output further shows the `Critical Chi-Square`, the non-centrality parameter (`NCP`), and the ratio between the error probabilities (`Implied Alpha/Beta ratio`). In this example, the ratio between alpha and beta is 0.25, showing that committing an beta error is four-times as likely as committing an alpha error. This is obviously a consequence of the chosen input parameters, since a power (1 - beta) of .80 implies an beta error of .20, which is four times the chosen alpha error of .05.

`semPower` also converts the chosen effect into other effect size measures: An RMSEA of .05 (based on df = 100) corresponds to $F_0$ = 0.250 and Mc = .882. If one is also interested in obtaining the associated GFI and AGFI, the number of variables needs to be provided. When the model involves 20 observed variables, the call above can be modified by including the argument `p = 20`:

```{r eval=FALSE}
ap <- semPower.aPriori(effect = .05, effect.measure = 'RMSEA', 
                       alpha = .05, power = .80, df = 100, p = 20)
```
Now the GFI and AGFI equivalents of RMSEA = .05, assuming df = 100 and p = 20, are also provided.

If one is interested in how power changes for a range of sample sizes, it is useful to request a [power plot](#plots).


## Post-hoc power analysis: Determine power {#ph}

The purpose of post-hoc power analyses is to determine the actually achieved power to detect a specified effect with given sample size on a certain alpha error. In the language of structural equation modeling, a post-hoc power analysis asks: With my sample at hand, how large is the probability (power) to detect the effect of interest?

Performing a post-hoc power analyses requires the specification of:

* the alpha error (`alpha`)
* the sample size (`N`)
* the type of effect (`effect.measure`) 
* the magnitude of effect (`effect`)
* the degrees of freedom of the model (`df`). See [how to obtain the df](#getDf) if you are unsure.

Depending on the chosen effect-size measure, it may also be required to define the number of observed variables (`p`). 

Suppose, one wants to determine the actually achieved power with a sample size of N = 1000 to detect misspecifications of a model (involving df = 100 degrees of freedom) corresponding to RMSEA $\geq$ .05 on an alpha error of .05. To achieve this, the function `semPower.postHoc` is called with arguments `effect = .08`, `effect.measure = 'RMSEA'`, `alpha = .05`, `N = 1000`, and `df = 100`, and the results are stored in a list called `ph`. 

```{r}
ph <- semPower.postHoc(effect = .05, effect.measure = 'RMSEA', 
                      alpha = .05, N = 1000, df = 100)
summary(ph)
```

Equivalently, instead of calling the `semPower.aPriori`, one may also use the generic `semPower` function with the additional `type = 'post-hoc'` argument: 

```{r eval=FALSE}
ph <- semPower(type = 'post-hoc', 
               effect = .05, effect.measure = 'RMSEA',
               alpha = .05, N = 1000, df = 100)
summary(ph)
```

Calling the `summary` method on `ph` provides an output structured identically to the one produced by [`semPower.aPriori`](#ap) and shows that the power is very high (`power > .9999`). The associated error probabilities are provided in higher precision. Specifically, the beta error is `beta = 2.903302e-17` which translates into $2.9 \cdot 10^{-17} = 0.000000000000000029$. In practice, one would almost never miss a model with an actual RMSEA $\geq$ .05 (or F0 $\geq$ 0.25 or Mc $\leq$ .882) under these conditions. The implied alpha/beta ratio is 1.722177e+15, showing that committing an alpha error is about two quadrillion ($10^{15}$) times as likely as committing a beta error.

If one are interested in how power changes for a range of different magnitudes of effect (say, for RMSEAs ranging from .01 to .15), it is useful to request a [power plot](#plots).


## Compromise power analysis: Determine alpha and beta {#cp}

The purpose of compromise power analyses is to determine alpha and beta (and the associated critical value of the chi-squared test-statistic) given a specified effect, a certain sample size, and a desired ratio between alpha and beta (Moshagen & Erdfelder, 2016). In the language of structural equation modeling, a compromise analysis asks: With my sample at hand, how should the critical value for the chi-square model-test be defined to obtain proportionate alpha and beta errors in deciding whether my model is rather aligned with the hypothesis of perfect fit or with the hypothesis of unacceptable degree of misfit (as defined by the chosen effect)?

Performing a compromise power analyses requires the specification of:

* desired ratio between alpha and beta (`abratio`;  defaults to 1)
* the sample size (`N`)
* the type of effect (`effect.measure`) 
* the magnitude of effect (`effect`)
* the degrees of freedom of the model (`df`). See [how to obtain the df](#getDf) if you are unsure.

Depending on the chosen effect-size measure, it may also be required to define the number of observed variables (`p`). 

Suppose, one wants to determine the critical chi-square and the associated alpha and beta errors, forcing them to be equal (i. e., an ratio of 1). The model involves 100 df, the sample size is N = 1000, and the H1 model representing an unacceptable degree of misfit is defined as a model associated with an RMSEA of at least .08. Thus, the function `semPower.compromise` is called with  arguments `effect = .08`, `effect.measure = 'RMSEA'`, `abratio = 1`, `N = 1000`, and `df = 100`, the results are stored in a list called `cp`, and the `summary` method is called to obtain formatted results. 

```{r}
cp <- semPower.compromise(effect = .08, effect.measure = 'RMSEA', 
                           abratio = 1, N = 1000, df = 100)
summary(cp)
```

Equivalently, instead of calling `semPower.compromise`, one may also use the generic `semPower` function with the additional `type = 'post-hoc'` argument: 

```{r eval=FALSE}
cp <- semPower(type = 'compromise', 
               effect = .05, effect.measure = 'RMSEA', 
               abratio = 1, N = 1000, df = 100)
summary(cp)
```

The output is structured identically to the one produced by [`semPower.aPriori`](#ap) and shows that choosing a `Critical Chi-Square = 312` is associated with balanced error probabilities, `alpha = 1.212986e-23` and `beta = 1.212986e-23`. As requested, both error probabilities as just as large. In addition, committing either error is highly unlikely: an error of `1.212986e-23` translates into $1.2 \cdot 10^{-23} = 0.000000000000000000000012$. In practice, one almost never would make a wrong decision.   

If one rather prefers the error probabilities to differ (for example because one  considers falsely accepting an incorrect model to be 100 times as bad as falsely rejecting an correct model), this can be achieved by changing the `abratio` argument accordingly. For example, requesting the alpha error to be 100 times as large as the beta error proceeds by setting `abratio = 100`. 

```{r}
cp <- semPower.compromise(effect = .08, effect.measure = 'RMSEA', 
                           abratio = 100, N = 1000, df = 100)
```


## Power-analysis to detect an overall difference between two models {#diffPower}

A common scenario is to test two competing models against each other, where a more restrictive model (involving more df) is compared against a less restrictive model (involving less df). When the difference between these models lies just in a single (or few) particular parameter(s), the effect should be determined [in terms of the model parameters](#modelBasedPower). If, however, the difference between the models potentially spreads across multiple parameters (as, say, when comparing a 3-factor with a 5-factor model), one approach to power analysis it to define the models in terms of overall fit. 

For example, to obtain the required sample size to yield a power of 80% to discriminate a model with an associated RMSEA of .04 on 44 df from a model with an associated RMSEA of .05 on 41 df, define both the `effect` and `df` arguments as vectors (do not define lists!) comprising two elements:
```{r eval=FALSE}
ap <- semPower.aPriori(effect = c(.04, .05), effect.measure = 'RMSEA', 
                       alpha = .05, power = .80, df = c(44, 41))
summary(ap)
```
which shows that 340 observations are required to discriminate these models. Post-hoc and compromise power analyses are performed accordingly.

A similar situation often occurs in tests of measurement invariance, where one wants sufficient power to detect whether certain cross-group or cross-time constraints on the model parameters (such as equal loadings across groups) are violated. Again, the difference between such models can be defined through [particular parameters](#powerMI), for instance by assuming a single loading differs by .1 across groups. However, it is also reasonable to assume that non-invariance spreads across multiple parameters (say, across all loadings), so that one approach to power analysis it to define the models in terms of overall fit. 

The general syntax is the same as above, but we now also need to set the `N` argument, which gives the number of observations by group in compromise and post-hoc power analysis, and the group weights in a-priori power analysis. For example, the following asks for the required sample size to detect a change in the Mc of .01 in a three-group model, where all groups are equal-sized: 
```{r eval=FALSE}
ap <- semPower.aPriori(effect = c(.99, .98), effect.measure = 'Mc', 
                        alpha = .05, power = .80, df = c(69, 57), N = c(1, 1, 1))
summary(ap)
```
This shows that 858 observations (286 by group) are required for a power of 80%. 


## Define the effect through covariance matrices {#InputCovMatrix}

The previous sections assumed that the magnitude of effect is determined by defining a certain effect size metric (such as $F_0$ or RMSEA) and a certain magnitude of effect. Alternatively, the effect can also be determined by specifying the population ($\Sigma$) and the model-implied ($\hat{\Sigma}$) covariance matrices (and, if means are part of the model, $\mu$ and $\hat{\mu}$) directly. To determine the associated effect in terms of F0, `semPower` just plugs these matrices in the ML-fitting function: 

$$F_0 = \log|\Sigma| - \log|\hat{\Sigma}| + tr(\hat{\Sigma}\Sigma) - p  + (\mu - \hat{\mu}) \hat{\Sigma}^{-1} (\mu - \hat{\mu}) $$

Suppose, $\Sigma$ and $\hat{\Sigma}$ have been defined previously and are referred to by the variables `Sigma` and `SigmaHat` Then, any of the power-analysis functions is called setting the `Sigma` and `SigmaHat` arguments accordingly (and omitting the `effect` and `effect.measures` arguments). This could look as follows:

```{r eval=FALSE}
semPower.aPriori(alpha = .05, power = .80, df = 100, 
                 Sigma = Sigma, SigmaHat = SigmaHat)
semPower.postHoc(alpha = .05, N = 1000, df = 100, 
                 Sigma = Sigma, SigmaHat = SigmaHat)
semPower.compromise(abratio = 1, N = 1000, df = 100, 
                    Sigma = Sigma, SigmaHat = SigmaHat)
semPower.powerPlot.byN(alpha = .05, df = 100, power.min = .05, power.max = .99, 
                       Sigma = Sigma, SigmaHat = SigmaHat)
```

This feature is particularly useful when used in conjunction with other functions provided by `semPower` and is indeed internally used by all functions performing [model-based power analyses](#modelBasedPower). An example on how to obtain the relevant covariance matrices is provided in a [later chapter](#powerCov).




# Model-based power analyses {#modelBasedPower}
A general difficulty in [model-free power analysis](#modelFreePower) is that the relation between constrains on a particular model parameter and the resulting effect size is often not clear. For instance, obtaining the required N to detect a cross-lagged effect $\geq$ .10 in a CLPM with a certain power requires to translate the hypothesized cross-lagged effect to a non-centrality based effect size such as RMSEA, which is hardly feasible. `semPower` therefore provides various convenience function to simplify this process, which are described in this chapter.

The purpose of all convenience functions is to provide high-level interfaces that allow to specify the parameters of a certain model type (such as a CLPM) and a certain effect of interest in terms of the model parameters (such as a crossed-lagged effect). The convenience functions then obtain the relevant population and model-implied covariance matrices and perform the desired power analysis based on these matrices. All convenience functions therefore require that the `lavaan` package (Rosseel, 2012) is installed, and always require the specification of the relevant parameters for the [desired power analysis](#modelFreePower).

More precisely, all convenience functions internally obtain the population covariance matrix (and mean vector, if necessary) via [`semPower.genSigma`](#genSigma), define the H0 model (and optionally the H1 model), call [`semPower.powerLav`](#powerLav) to fit the models to the population values, which, in turn, obtains the model-implied covariance matrix (and mean vector), and plugs the population and model-implied matrices into one of the [model-free power analyses](#modelFreePower) functions. Since all of these low level functions are also exposed, an even higher level of flexibility can be achieved by calling these functions directly (see [this](#powerLav) and [this](#powerCov) chapter for illustrations).

Given that the functions performing a model-based power analysis operate on a factor model, it is always required to specify the factor model in terms of the number of factors, number of indicators, and loadings. For this reason, the chapter begins with a primer on how to [define the factor model](#factorDefinition).
 
## Definition of the factor model {#factorDefinition}

Although all convenience functions described in this chapter implement different model structures, all these structures involve latent factors, so the factor model always needs to be defined in terms of the number of factors, the number of indicators for each factors, and the loadings of each indicator on the factors. Indeed, the "factor" model also need to be specified if the model does not include any factor, but operates on observed variables (such as a CLPM based on observed scores, rather than on latent factors). There are several ways to achieve this, which are documented further below, for the impatient reader the easiest way is to add `Lambda = diag(p)`, where p is the number of variables contained in the model.

The magnitude of the factor loadings and the number of indicators per factor have a very large effect on statistical power (because both quantities increase factor determinacy and thus reduce random noise). For example, power to detect a factor correlation of $r \geq .2$ with N = 250 is 88% when both factors are measured by 10 indicators each and all loadings are .90, but only 11% when both factors are measured by 3 indicators each and all loadings are .30. Likewise, the dispersion of factor loadings also affects power, although to a (considerably) lesser degree. It is thus crucial to be careful in defining appropriate factor loadings, which should generally be based on previous empirical results.

Any `semPower` convenience function expects one of the following arguments defining the factor model:

* `Lambda` to define the loading matrix.
* `loadings` to define a reduced loading matrix that only contains the primary loadings.
*  `nIndicator` in conjunction with `loadM`  to define the number of indicators by factor along with a single loading for the indicators of each factor or a single loading to apply for all indicators. 

For example, suppose two factors measured by 3 and 4 indicators, respectively, and all loadings equal to .5 shall be defined. The first option to define this factor model is to provide a loading matrix as value to the `Lambda` argument:

```{r eval=FALSE}
Lambda <- matrix(c(
  c(.5, 0),
  c(.5, 0),
  c(.5, 0),
  c(0, .5),
  c(0, .5),
  c(0, .5),
  c(0, .5)
  ), ncol = 2, byrow = TRUE)
```

A second way to achieve this is to provide a list comprising two vectors as value to the `loadings` argument:

```{r eval=FALSE}
loadings <- list(
  c(.5, .5, .5),
  c(.5, .5, .5, .5)
  )
```
`loadings` must be a list of vectors, where each vector defines the loading of the indicators on the respective factor. One vector is needed for each factor, so in the example above the first vector comprises 3 and the second 4 elements, reflecting the desired loading matrix above. Note that the `loadings` argument assumes the absence of any secondary loading, so that each loading defined in the vectors refers to a single indicator. As another example, the following are two equivalent ways to define three factors with 3, 4, and 5 indicators loading by the specified values: 

```{r eval=FALSE}
Lambda <- matrix(c(
  c(.7, 0, 0),
  c(.6, 0, 0),
  c(.5, 0, 0),
  c(0, .5, 0),
  c(0, .8, 0),
  c(0, .6, 0),
  c(0, .3, 0),
  c(0, 0, .9),
  c(0, 0, .5),
  c(0, 0, .7),
  c(0, 0, .4),
  c(0, 0, .6)
  ), ncol = 3, byrow = TRUE)

loadings <- list(
  c(.7, .6, .5),
  c(.5, .8, .6, .3),
  c(.9, .5, .7, .4, .6)
  )
```


A third way to define two factors measured by 3 and 4 indicators, respectively, and all loadings equal to .5, is to provide both the `nIndicator` and the `loadM` arguments:
```{r eval=FALSE}
nIndicator <- c(3, 4)
loadM <- .5
```
`nIndicator` is a vector providing the number of indicators separately for each factor. `loadM` can be a single number (as in the example above) to say that all loadings have the same value. Alternatively, `loadM` can be a vector specifying the loadings separately for each factor, where each indicator of a specific factor takes the defined value as loading. Thus, specifying `loadM <- c(.5, .5)` would achieve the same result, whereas `loadM <- c(.5, .6)` would assign all indicators of the second factor a loading of .6. 

Which of these three ways to use to define the factor models (`Lambda`, `loadings` or `nIndicator` and `loadM`) in the example two factor model with equal loadings is of course arbitrary and ultimately a matter of taste. In general, the `nIndicator` and `loadM` arguments are usually the simplest approach, but do not allow for any dispersion of the loadings within a factor. This flexibility is offered by the `loadings` argument, which in most cases should suit all needs. Providing the complete loading matrix via the `Lambda` argument is only required for more complex loadings patterns where a single indicator is supposed to load on more than one factor.

To include an observed variable in a model, a dummy factor with a single indicator loading by 1 can be defined. For instance, the following defines an observed variable and a factor with 4 indicators loading by .5 each:
```{r eval=FALSE}
nIndicator <- c(1, 4)
loadM <- c(1, .5)
```

The "factor" model also needs to be defined, when the model does not include any factor, but only contains observed variables. Consider a CLPM with two waves based on observed variables only, so there are 4 observed variables in total. Requesting an observed only model can be achieved by using either of the following:

* `Lambda = diag(4)`
* `loadings = as.list(rep(1, 4))`
* `nIndicator = rep(1, 4)` and `loadM = 1`

For many (but not all) convenience functions, it is important to define the factors in the expected order. For instance, [`semPower.powerRegression`](#powerRegression) treats the first factor as criterion (*Y*) and the remaining factors as predictors (*X*). Thus, `nIndicator <- c(10, 5, 5)` says that the criterion is measured by 10 indicators, whereas both predictors are measured by 5 indicators. Using `nIndicator <- c(5, 10, 5)` instead would imply a criterion measured by 5 indicators. Details on the expected order of factors are provided in each specific convenience function.

## Arguments common to all convenience functions {#commonArgs}

Whereas all convenience functions expect certain arguments (or values provided as arguments) that are unique to a specific function, a number of arguments are expected by all functions. In particular, any convenience function expects arguments [specifying the factor model](#factorDefinition) and arguments detailing the specific type of power analysis ([a-priori](#ap), [post-hoc](#ph), or [compromise](#cp)) requested. Additional shared arguments are:

* `type`: The type of power analysis requested; one of `'a-priori'`, `'post-hoc'` or `'compromise'` (or shortcuts, i. e., `'ph'`, `'ap'`, `'cp'`), see also below. 
* `comparison`: The relevant comparison model; one of `'saturated'`, or `'restricted'` (the default). See below for details. 
* `nullEffect`: Defines the relevant hypothesis depending on the specific convenience function.
* `nullWhich`: Defines which parameters are targeted by the hypothesis specified in `nullEffect`.
* `nullWhichGroups`: Defines which groups are targeted when `nullEffect` refers to cross-group constrains.
* `simulatedPower`: Whether to perform a simulated (`TRUE`) rather than an analytical (`FALSE`; the default) power analysis. See the [chapter on simulated power](simulatedPower) for details.

Thus, a typical call could look as follows (here taking a CFA model as an example):

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(
  # define type of power analysis
  type = 'a-priori', alpha = .05, beta = .20,
  # set comparison model
  comparison = 'restricted',
  # arguments (and values) specific to semPower.powerCFA
  Phi = .25,
  nullEffect = 'cor = 0',
  nullWhich = c(1, 2),
  # define factor model
  nIndicator = c(4, 3), loadM = c(.5, .6)
  )
```

Whereas `nullEffect` and `nullWhich` need to be defined in any convenience function, the specifics depend on the model and hypothesis at hand, so these arguments are discussed in the relevant chapters. The following gives more details on the `type` and `comparison` arguments.


##### `type` {-}

To request an  [a-priori power analysis](#ap), use `type = 'a-priori'` and provide the alpha error (e. g., `alpha = .05`) and the desired beta error (e. g., `beta = .20`; or equivalently, the desired power, `power = .80`).  

To request a [post-hoc power analysis](#ph), use `type = 'post-hoc'` and provide the alpha error (e. g.,`alpha = .05`) and the number of observations (e. g.,`N = 250`).  

To request a [compromise hoc power analysis](#cp), use `type = 'compromise'` and provide the desired ratio between alpha and beta error (e. g., `abratio = 1`) and the number of observations (e. g., `N = 250`).  


##### `comparison` {-}
Power analyses always refer to a certain hypothesis (H0) that is to be rejected, so that an (implicit) alternative hypothesis (H1) shall be accepted. The `comparison` argument is used to set the relevant comparison (H1) model, which can either refer to a saturated model (`comparison = 'saturated'`) or to a model (`comparison = 'restricted'`) that omits the H0 constraints reflecting the hypothesis of interest, but is otherwise identical to the H0 model. 

For instance, consider a two factor CFA model, where each factor is measured by 4 indicators, and suppose power is to be determined to detect a factor correlation of $r \geq .20$. A standard CFA model freely estimating the factor correlation involves 19 df. The relevant H0 model, however, would constrain the factor correlation to zero, and thus involves 18 df. To determine power to reject the H0 model, there are now two relevant comparison models defining the H1. One option would be to compare the H0 model against the saturated model (`comparison = 'saturated'`), so power analysis is based on 18 df. Practically speaking, one would just ask whether the model $\chi^2$ test associated with H0 model turns out significant. Alternatively, one might want to compare the H0 model against a model that is identical to the H0, except that it freely estimates the factor correlation (`comparison = 'restricted'`). The difference in the df between these two models (and the difference in model fit) now enter power analyses, which then would be based on $19 - 18 = 1$ df.

Either approach is valid. Usually, however, it is more sensible to compare the H0 model against a less restricted H1 model that only differs in the parameter(s) relevant for the tested hypothesis, which has the added benefit of a generally higher power. Thus, `semPower` by default uses `comparison = 'restricted'`. Of note, the measures of effect given in the output of a power-analysis are based on the df provided to the respective power-analysis. Thus, in case of a restricted comparison model, these will generally differ from the ones obtained by just fitting the H0 model to some data, and will, in general, also not reflect the simple differences between the indices of the H0 and the H1 model.


## CFA models {#powerCFA}

`semPower.powerCFA` is used to perform power analyses to reject hypotheses arising in a standard CFA model involving several factors or a single factor and additional observed covariates. `semPower.powerCFA` provides interfaces to perform power analyses concerning the following hypotheses:

* whether a correlation differs from zero (`nullEffect = 'cor = 0'`). 
* whether two correlations differ from each other (`nullEffect = 'corX = corZ'`).
* whether a correlation differs across two or more groups (`nullEffect = 'corA = corB'`).

Note that `semPower.powerCFA` only addresses hypotheses concerning correlation(s) involving one or more factors. `semPower` provides other convenience functions for hypothesis arising in [latent regression models](#powerRegression), models involving a [bifactor structure](#powerBifactor), [mediation models](#powerMediation), [generic path models](#powerPath), and [multigroup measurement invariance](#powerMI). For hypothesis regarding global model fit, a [model-free power analysis](#modelFreePower) should be performed. 

As for any model-based power analysis included in `semPower`, beyond arguments unique to the type of model and type of hypothesis considered (see below), `semPower.powerCFA` additionally expects arguments [specifying the factor model](#factorDefinition), arguments related to the specific type of power analysis requested, and some optional additional arguments, see [arguments common to all convenience functions](#commonArgs).

The arguments specific to `semPower.powerCFA` are:

* `Phi`: Either a single number defining the correlation between exactly two factors or the factor correlation matrix. 
* `nullEffect`: Defines the hypothesis of interest;  one of `'cor = 0'`, `'corX = corZ'`, or  `'corA = corB'`.
* `nullWhich`: Defines which correlation(s) is targeted by the hypothesis defined in `nullEffect`.
* `nullWhichGroups`: Defines which groups are targeted, when `nullEffect = 'corA = corB'`.

`semPower.powerCFA` provides a list as result, which contains the following components:

* `power`: The results of the power analysis, which contains the same information as the corresponding model-free counterpart (see [a-priori power analysis](#ap), [post-hoc power analysis](#ph), and [compromise hoc power analysis](#cp)). Use the `summary` method  to obtain formatted results.
* `Sigma` and `mu`: Variance-covariance matrix and means in the population
* `SigmaHat` and `muHat`: Model implied variance-covariance matrix and means
* `modelH0` and `modelH1`: `lavaan` model strings defining the H0 and the H1 model (only if `type = 'restricted'`).


##### Detect whether a correlation differs from zero {-}

To perform a power analysis to detect whether a correlation between factors differs from zero, use `nullEffect = 'cor = 0'` (which is also the default hypothesis and thus could be omitted). 

In the simplest case, the model contains exactly two factors, so only the to-be detected factor correlation needs to be specified (along the factor model itself). For instance, the following requests the required sample (`type = 'a-priori'`) to detect that a factor correlation of at least .25 (`Phi = .25`) differs from zero (`nullEffect = 'cor = 0'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). The two factors are measured by 4 and 3 indicators (`nIndicator = c(4, 3)`), respectively, and all non-zero loadings on the first and second factor are equal to .5 and .6 (`loadM = c(.5, .6)`), respectively. See the chapter on [specifying a factor model](#factorDefinition) for alternative ways to specify the loadings on the factors.

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80,
                              Phi = .25,
                              nullEffect = 'cor = 0',
                              nIndicator = c(4, 3), loadM = c(.5, .6))
summary(powerCFA$power)
```
The results of the power analysis are printed by calling the `summary` method on `powerCFA$power`, which provides the same information as a model-free [a-priori power analysis](#ap).

If one is interested in detecting the correlation between a factor and an observed covariate, the only change refers to the definition of the factor model. `nIndicator = c(4, 1)` and `loadM = c(.5, 1)` now define a factor with 4 indicators loadings by .5 each and another dummy factor with a single indicator loading by 1 (which is then simply an observed variable):

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80,
                              Phi = .25,
                              nullEffect = 'cor = 0',
                              nIndicator = c(4, 1), loadM = c(.5, 1))
```

Alternatively, `Phi` can also be a factor correlation matrix, which is probably only useful when there are more than two factors. The following defines three factors correlated according to `Phi` and uses the `nullWhich = c(1, 3)` argument to determine the required sample size to detect a correlation of at least .30 between the first and the third factor:

```{r eval=FALSE}

Phi <- matrix(c(
   c(1.00, 0.20, 0.30),
   c(0.20, 1.00, 0.10),
   c(0.30, 0.10, 1.00)
 ), ncol = 3,byrow = TRUE)

powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80,
                              Phi = Phi,
                              nullEffect = 'cor = 0',
                              nullWhich = c(1, 3),
                              nIndicator = c(3, 3, 3), loadM = c(.5, .7, .6))
```


##### Detect whether two correlations differ from each other {-}

To perform a power analysis to detect whether two correlations differ from each other, use `nullEffect = 'corX = corZ'`. 

For instance, the following requests the required sample (`type = 'a-priori'`) to detect that the correlation between the first and second factor (of .20) differs from the correlation between the first and the third factor (of .30; `nullEffect = 'corX = corZ'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`), where all factors are measured by 3 indicators (`nIndicator = c(3, 3, 3)`), respectively, and all non-zero loadings on the first, second, and third factor are equal to .5, .7, and .6 (`loadM = c(.5, .7, .6)`), respectively (see [Definition of the factor model](#factorDefinition)).

```{r eval=FALSE}

Phi <- matrix(c(
   c(1.00, 0.20, 0.30),
   c(0.20, 1.00, 0.10),
   c(0.30, 0.10, 1.00)
 ), ncol = 3,byrow = TRUE)

powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80,
                              Phi = Phi,
                              nullEffect = 'corX = corZ',
                              nullWhich = list(c(1, 2), c(1, 3)),
                              nIndicator = c(3, 3, 3), loadM = c(.5, .7, .6))
```

Note that `nullWhich` is now a list comprising two vectors, jointly defining which correlations to set to equality. `nullWhich = list(c(1, 2), c(1, 3))` says that the correlation between the first and the second factor (`c(1, 2)`) and the correlation between the first and the third (`c(1, 3)`) factor are restricted to be equal. 

`nullWhich` can also comprise more than two elements to test for the equality of more than two correlations. For instance, using `nullWhich = list(c(1, 2), c(1, 3), c(2, 3))` in the scenario above constrains all factor correlations to be equal.

As before, it is also possible to include observed covariates instead of latent factors by just defining a dummy factor with a single indicator loading by 1. For example, to replace the first factor in the example above by an observed variable (thus asking for the power to detect whether two factors correlate differently to an observed outcome), the factor model is changed by altering `nIndicator` and `loadM`: 

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80,
                              Phi = Phi,
                              nullEffect = 'corX = corZ',
                              nullWhich = list(c(1, 2), c(1, 3)),
                              nIndicator = c(1, 3, 3), loadM = c(1, .7, .6))
```


##### Detect whether a correlation differs across two or more groups {-}

To perform a power analysis to detect whether a correlation differs across two or more groups, use `nullEffect = 'corA = corB'`. 

For instance, the following requests the required sample (`type = 'a-priori'`) to detect that the correlation between two factors in group 1 (of .20) differs from the one in group 2 (of .40; `nullEffect = 'corA = corB'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). The measurement model is identical in both groups: Both factors are measured by 5 indicators each (`nIndicator = c(5, 5)`), and all non-zero loadings on the first and second factor are equal to .7 and .5 (`loadM = c(.7, .5)`), respectively, in both groups (see [Definition of the factor model](#factorDefinition)). However, `Phi = list(.2, .4)` is now a list comprising two elements, the first defining the factor correlation in the first group to be .2, the second defining the correlation in the second group to be .4. In addition, `N` must also be a list, which in case of an a-priori power analysis gives the group weights. `N = list(1, 1)` requests equally sized groups.

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                              nullEffect = 'corA = corB',
                              Phi = list(.2, .4), 
                              loadM = c(.7, .5), 
                              nIndicator = c(5, 5))
```

If using `N = list(2, 1)` instead, the first group would be twice as large as the second group. If a post-hoc or compromise power analysis is requested, `N` is a list providing the number of observations for each group. `Phi` can also be a list of factor correlation matrices (instead of a list of single numbers). 

For instance, the following defines different factor correlation matrices for two groups (`Phi1` and `Phi2`) with 300 and 400 observations (`N = list(300, 400)`), respectively, and requests the achieved power (`type = 'post-hoc'`) on alpha = .05 (`alpha = .05`) to detect that the correlation between factor 1 and 3 (`nullWhich = c(1, 3)`) differs across groups.

```{r eval=FALSE}
Phi1 <- matrix(c(
    c(1.00, 0.20, 0.50),
    c(0.20, 1.00, 0.10),
    c(0.50, 0.10, 1.00)
 ), ncol = 3,byrow = TRUE)
Phi2 <- matrix(c(
    c(1.00, 0.20, 0.30),
    c(0.20, 1.00, 0.10),
    c(0.30, 0.10, 1.00)
 ), ncol = 3,byrow = TRUE)

powerCFA <- semPower.powerCFA(type = 'post-hoc', alpha = .05, N = list(300, 400),
                              Phi = list(Phi1, Phi2),
                              nullEffect = 'corA = corB',
                              nullWhich = c(1, 3),
                              nIndicator = c(3, 3, 3), loadM = c(.5, .5, .5))
```

If there are more than two groups, the targeted correlation is held equal across all groups by default. If the correlation should only be constrained to equality in specific groups, `nullWhichGroups` is used to identify the groups to which the equality restrictions apply. For instance, the following defines three equally sized groups with a distinct correlation between the two factors, but only asks for the required sample to detect that the correlation in group 1 (of .2) differs from the one in group 3 (of .3; `nullWhichGroups = c(1, 3)`). 

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80, N = list(1, 1, 1),
                              nullEffect = 'corA = corB',
                              Phi = list(.2, .4, .3), 
                              nullWhichGroups = c(1, 3),
                              loadM = c(.7, .5), 
                              nIndicator = c(5, 5))
```

## Models involving a bifactor structure {#powerBifactor}
to be written
`semPower.powerBifactor`

## Latent regression models {#powerRegression}
to be written
`semPower.powerRegression`

## Mediation models  {#powerMediation}
to be written
`semPower.powerMediation`

## Generic path models {#powerPath}
to be written
`semPower.powerPath`

## Multiple group models {#powerMI}
to be written
`semPower.powerMI`

When one does not expect (or is not interested in or does not have sufficiently specific hypotheses on) measurement non-invariance for certain parameters, but rather assumes that non-invariance spreads across multiple parameters (say, across all loadings), one should consider to perform model free power analysis concerning the [overall difference](#diffPower) between two models.

## CLPM models {#powerCLPM}
to be written
`semPower.powerCLPM`

## RI-CLPM models {#powerRICLPM}
to be written
`semPower.powerRICLPM`

## Generic model-based power analysis {#powerLav}

All of the functions described above implement a high-level approach towards performing model-based power analysis to facilitate the definition of the relevant models and hypotheses. At times, one might want to perform model-based power analysis that are not immediately covered by one of these functions. Therefore, `semPower` also provides a more generic way to perform a model-based power analysis via the `semPower.powerLav` function that requires the direct specification of the H0 and H1 `lavaan` model strings as well as either the population covariance matrix (and means) or the population model.

Consider the situation that one is interested in determining whether the observed responses on 8 items reflect two separate (but correlated) factors or can be described by assuming just a single factor. A suitable model to test this hypothesis would specify two factors and constrain their correlation to 1. When this constrained model fits the data, a single factor is sufficient. Otherwise, two factors are required.   

Suppose that a correlation between two factors of $r > .9$ is considered as implying that these are practically equivalent, so these could be collapsed into a single factor. The misfit associated with a model assuming a correlation of 1 when, in reality, the true correlation is $\leq$ .9 is supposed to define the magnitude of effect. This scenario is not covered by the [`semPower.powerCFA`](#powerCFA) function, so we need some manual work to achieve this. Three steps are required to perform a power analysis in this scenario: 

* Define the population model that describes the true situation in the population. Alternatively, we can also provide the population covariance matrix, which is described further below. 
* Define an (incorrect) analysis model that reflects the null hypothesis of interest.
* Optionally define a (correct) analysis model that reflects the alternative hypothesis.
 
##### Using a `lavaan` population model string {-}

First, the true affairs in the population need to be specified. We need to define the value of each single (non-zero) parameter, so we need to specify each loading, each residual variance, the variance of the factors as well as their covariance. Note that the magnitude of effect (and thus power) does not only depend on the correlation between the factors, but also varies with other parameters of the model, such as the loading magnitude, so some reasonable guess concerning the latter required. Let us assume that the standardized loadings vary between .4 and .8. Importantly, we define that two factors exists that correlate by .9. 

```{r eval=FALSE}
# define (true) population model
modelPop <- '
# define relations between factors and items in terms of loadings
f1 =~ .7*x1 + .7*x2 + .5*x3 + .5*x4
f2 =~ .8*x5 + .6*x6 + .6*x7 + .4*x8
# define unique variances of the items to be equal to 1-loading^2, 
# so that the loadings above are in a standardized metric
x1 ~~ .51*x1
x2 ~~ .51*x2
x3 ~~ .75*x3
x4 ~~ .75*x4
x5 ~~ .36*x5
x6 ~~ .64*x6
x7 ~~ .64*x7
x8 ~~ .84*x8
# define variances of f1 and f2 to be 1
f1 ~~ 1*f1   
f2 ~~ 1*f2   
# define covariance (=correlation, because factor variances are 1) 
# between the factors to be .9
f1 ~~ 0.9*f2 
'
```

Having defined the true state in the population, we now need to define the (incorrect) analysis model reflecting the null hypothesis of interest. This model should make at least one restriction which is factually wrong and thereby defines the effect of interest, in the present case the restriction that the two factors correlate to 1.  
```{r eval=FALSE}
# define (wrong) analysis model
modelH0 <- '
f1 =~ NA*x1 + x2 + x3 + x4
f2 =~ NA*x5 + x6 + x7 + x8
# define variances of f1 and f2 to be 1
f1 ~~ 1*f1   
f2 ~~ 1*f2   
# set correlation between the factors to 1
f1 ~~ 1*f2
' 
```

Whereas the population model and the H0 model are sufficient to perform a power analysis, we also define an explicit H1 model which is to be compared against the H0 model. Because here we define the H1 model in a way that it is correct, this only affects the df for the power analysis. 

```{r eval=FALSE}
# define (correct) comparison model
modelH1 <- '
f1 =~ NA*x1 + x2 + x3 + x4
f2 =~ NA*x5 + x6 + x7 + x8
# define variances of f1 and f2 to be 1
f1 ~~ 1*f1   
f2 ~~ 1*f2   
# freely estimate the correlation between the factors to 1
f1 ~~ f2
' 
```

Finally, all these strings are plugged into `semPower.powerLav()`, in this example requesting an a-priori power analysis:

```{r eval=FALSE}
ap <- semPower.powerLav(type = 'a-priori', 
                        modelPop = modelPop, modelH0 = modelH0, modelH1 = modelH1, 
                        alpha = .05, power = .80)
summary(ap$power)
```

The output shows that 323 observations are required to detect a correlation between the factors of $r \leq .9$ with a power of 80% on alpha = .05. Note that here we have only a single df, because we also defined the H1 model, which just differs from the H0 model in a single parameter (namely the freely estimated correlation between the factors). If the H1 model is omitted from the function call, power will be determined relative to the saturated model, which in this example leads to 20 df and thus to a very different sample size required to detect the specified effect.

Beyond the results of power analysis, the result returned by `semPower.powerLav` also contains a number of additional information, in particular the generated population covariance matrix $\Sigma$ (`Sigma`) and the model-implied covariance matrix $\hat{\Sigma}$ (`SigmaHat`). It is a good idea to check whether the population model actually defined all parameters in line with our expectations. To verify, we can just fit the correct H1 model to the population covariance matrix which should yield a perfect fit and give the same parameter estimates we used in the definition of the model string.
```{r eval=FALSE}
library(lavaan)
summary(sem(modelH1, sample.cov = ap$Sigma, 
            sample.nobs = 1000, sample.cov.rescale = FALSE), 
        stand = TRUE, fit = TRUE)
```

##### Providing a population covariance matrix  {-}

Instead of defining the true state in the population via a `lavaan` model string, it is also possible to provide the population covariance matrix directly. A useful utility function supporting this process is [semPower.genSigma](#genSigma). For instance, using 
```{r eval=FALSE}
generated <- semPower.genSigma(Phi = .90, 
                               loadings = list(
                                 c(.7, .7, .5, .5),
                                 c(.8, .6, .6, .4)))
ap <- semPower.powerLav(type = 'a-priori', 
                        Sigma = generated$Sigma, modelH0 = modelH0, modelH1 = modelH1, 
                        alpha = .05, power = .80)
summary(ap$power)
```
gives exactly the same results as described in detail above. See [semPower.genSigma](#genSigma) for more information.


##### Exploiting `semPower.powerCFA` to obtain model strings and the population covariance matrix {-}

Finally, another alternative option is to rely on the [`semPower.powerCFA`](#powerCFA) function and to obtain the population covariance matrix and just to provide the H0 model of interest. Note that here we use `semPower.powerCFA` just to obtain Sigma (`pcfa$Sigma`), but otherwise ignore the power-related output.
```{r eval=FALSE}
pcfa <- semPower.powerCFA(type = 'post-hoc',
                          Phi = .9, 
                          loadings = list(
                            c(.7, .7, .5, .5),
                            c(.8, .6, .6, .4)), 
                          alpha = .05, N = 1000)

ap <- semPower.powerLav(type = 'a-priori', 
                        Sigma = pcfa$Sigma, modelH0 = modelH0, modelH1 = modelH1, 
                        alpha = .05, power = .80)
summary(ap$power)
```

This approach has the benefit that the result variable `pcfa` also contains generated model strings, which could be modified to reflect the actual hypothesis of interest. Instead of writing the complete H0 and H1 model strings as tediously done in full detail above, the very same may also be achieved through

```{r eval=FALSE}
pcfa <- semPower.powerCFA(type = 'post-hoc',
                          Phi = .9, 
                          loadings = list(
                            c(.7, .7, .5, .5),
                            c(.8, .6, .6, .4)), 
                          alpha = .05, N = 1000)
# define modelH0 as function of the returned modelH1 plus the 
# additional constraint of interest
modelH0 <- paste(pcfa$modelH1, 'f1 ~~ 1*f2', sep = '\n')
ap <- semPower.powerLav(type = 'a-priori', 
                        Sigma = pcfa$Sigma, modelH0 = modelH0, modelH1 = pcfa$modelH1, 
                        alpha = .05, power = .80)
```





# Power Plots {#plots}

Power plots show the implied power as function of some other variable. `semPower` provides two different types of power plots. One can either plot the achieved power to detect a certain effect over a range of different sample sizes (`semPower.powerPlot.byN`). Alternatively, one can plot the achieved power with a given $N$ to detect a range of different effect size magnitudes (`semPower.powerPlot.byEffect`).   

## Power by N for a given effect {#plotByN}
The function `semPower.powerPlot.byN` creates a plot showing the achieved power to detect a given effect on a given alpha error over a range of sample sizes. However, because it is difficult to specify diagnostic sample sizes for a given effect, the `semPower.powerPlot.byN` instead asks to provide the desired power range. For example, suppose we are interested in how the power to detect an effect corresponding to RMSEA = .05 changes as function of the number of observations N. We are interested in a power ranging from .05 to .99 (note that the power cannot be smaller than alpha). This is achieved by setting the arguments `power.min = .05` and `power.max = .99`. In addition, as in any a-priori power analysis, the type and magnitude of effect, the df, and the alpha error need to be defined: `effect = .05`, `effect.measure = 'RMSEA'`,`alpha = .05`, `df = 100`.

```{r figurePowerPlotByN, fig.cap = "Power as function of the N to detect RMSEA >= .05"}
semPower.powerPlot.byN(effect = .05, effect.measure = 'RMSEA', 
                       alpha = .05, df = 100, power.min = .05, power.max = .99)
```

This shows that a model with an associated RMSEA = .05 is rejected with a very high power when N > 250, whereas power is small when N < 100. 

&nbsp;

## Power by the magnitude of effect for a given N {#plotByEffect}
The function `semPower.powerPlot.byEffect` creates a plot showing the achieved power at a given sample size over a range of effect sizes. For example, suppose we are interested in how the power at N = 500 changes as function of effect size magnitude, corresponding to an RMSEA ranging from .001 to .10. This is achieved by setting the arguments `effect.measure = 'RMSEA'`, `effect.min = .001` and `effect.max = .10`. In addition, as in any post-hoc power analysis, the sample size, the df, and the alpha error need to be defined: `effect = .05`, `effect.measure = 'RMSEA'`, `alpha = .05`, `df = 100`.


```{r figurePowerPlotByEffect, fig.cap = "Power as function of the RMSEA with N = 500."}
semPower.powerPlot.byEffect(effect.measure = 'RMSEA', alpha = .05, N = 500, 
                            df = 100, effect.min = .001, effect.max = .10)
```

This shows that with N = 500, a model with an associated RMSEA > .04 is detected with a very high power, whereas power for RMSEA < .03 is rather modest. 

&nbsp;

# Further topics {#furtherTopics}
to be written

## Obtain the model degrees of freedom {#getDf}

Knowledge of the degrees of freedom (df) is required to perform a power analysis. When power refers to the comparison of two explicitly specified, nested models, the resulting df are just the difference between the df of the two models (or equivalently, the number of free parameters removed by the more restrictive model). When power is requested for the comparison of a hypothesized model to the saturated model, the model df are given by 

$$df = p\cdot(p+1)/2 - q$$

where $p$ is the number of observed variables and $q$ is the number of free parameters of the hypothesized model. To obtain the latter in a typical SEM, one needs to count (a) loadings, (b) item-residual variances, and (c) covariance/regression parameters between factors and between item residuals.^[Note that factor (residual) variances can be omitted, because each factor needs to be assigned a scale, so one free parameter is lost for each factor anyway. Concerning free parameters, it does not matter whether factors are identified by fixing their variance or by fixing a loading.] For instance, consider a correlated two factor CFA model with each factor measured by 4 items and no secondary loadings or residual correlations. Thus, there are (a) $2\cdot4$ loadings, (b) 8 item-residual variances, and (c) 1 covariance between the factors, which results in 17 free parameters. The df are thus $8\cdot9/2 - 17 = 19$.

If you are unsure or have a more complicated model (or both), `semPower` also includes a utility function called `semPower.getDf` that determines the df for a model provided in the `lavaan` syntax (note that this function requires the `lavaan` package). For the model sketched above, running 

```{r eval=FALSE}
# define model using standard lavaan syntax
lavmodel <- '
f1 =~ x1 + x2 + x3 + x4
f2 =~ x5 + x6 + x7 + x8
'
# obtain df
semPower.getDf(lavmodel)
```

gives 19 as result, which matches what we calculated by hand. `semPower.getDf` can also be used to obtain the df in multigroup settings by setting the arguments `nGroups` (and `group.equal` for models involving equality constraints).

```{r eval=FALSE}
# configural invariance
semPower.getDf(lavmodel, nGroups = 3)
# metric invariance
semPower.getDf(lavmodel, nGroups = 3, group.equal = c('loadings'))
# scalar invariance
semPower.getDf(lavmodel, nGroups = 3, group.equal = c('loadings', 'intercepts'))
```

In determining the df, `semPower.getDf` also accounts for any additional restrictions on (defined) parameters and should generally match the df reported by `lavaan`, excempt the case that corrected model fit statistics are used that include a correction of the df (such as the third-moment adjustment by Lin and Bentler, 2012).

## Multiple Group Models {#multipleGroups}
All convenience methods described above also have support for hypothesis arising in multiple group settings.

here we only want an own chapter for multigroups (for impatient readesr), but should directly refer to the relevant functions above. maybe as a list, say:

* See [powerCFA](powerCFA) to detect that the correlation between two factors differs across groups.
* See [powerMI](powerMI) for various hypothesis concerning measurement invariance across groups.
* ...


## Simulated power {#simulatedPower}
to be written




## Power analyses based on covariance matrices {#powerCov}

All [model-free power analyses](#modelFreePower) provided in `semPower` also accept [covariance matrices as input](#InputCovMatrix) and then determine the associated effect through trough the supplied covariance matrices. This offers a very high degree of flexibility, but obviously requires a specification of proper covariance matrices. In this section, various options to obtain these matrices are illustrated in detail, ordered by most to least cumbersome.

##### The full way {-}
One way is to employ some other SEM software to generate the population and the model-implied covariance matrix (note that a proper definition of the latter usually requires fitting the model to the population data). For illustration, let's consider how this could be done using `lavaan`. 

```{r eval=FALSE}
library(lavaan)

# define (true) population model
modelPop <- '
f1 =~ .8*x1 + .7*x2 + .6*x3
f2 =~ .7*x4 + .6*x5 + .5*x6
f1 ~~ 1*f1
f2 ~~ 1*f2
f1 ~~ 0.5*f2
x1 ~~ .36*x1
x2 ~~ .51*x2
x3 ~~ .64*x3
x4 ~~ .51*x4
x5 ~~ .64*x5
x6 ~~ .75*x6
'
# define (wrong) H0 model
modelH0<- '
f1 =~ x1 + x2 + x3
f2 =~ x4 + x5 + x6
f1 ~~ 0*f2
'
```

After loading the `lavaan` package, two lavaan model strings are defined. The first model string (`modelPop`) is used to define the population covariance matrix, so all model parameters are defined (at least implicitly, consult the lavaan documentation for defaults). Here, we define a model comprising two latent factors (each with variance of 1), that are correlated by .5. Each latent factor is measured through three observed indicators (x1 to x6) with (standardized) loadings ranging from .5 to .8. The second model string (`modelH0`) is used to define the hypothesized (wrong) H0 model. To obtain the model-implied covariance matrix, we need to fit this model to the population data; so this is basically an ordinary CFA model string with many free parameters. Note that this model contraints the correlation between the two latent factors to zero. When fitting this model to the population data defined previously, the model is thus wrong, since we defined the correlation between these factors in the population to be .50.

Having defined the model strings, we proceed by actually obtaining the relevant covariance matrices.

```{r eval=FALSE}
# get population covariance matrix; equivalent to a perfectly fitting model
covPop <- fitted(sem(modelPop))$cov

# get covariance matrix as implied by H0 model; note the nobs are arbitrary
fitH0 <- sem(modelH0, sample.cov = covPop, 
              sample.nobs = 250, sample.cov.rescale = FALSE, 
              likelihood='wishart')
df <- fitH0@test[[1]]$df
covH0 <- fitted(fitH0)$cov
```

`covPop` is now the covariance matrix in the population ($\Sigma$). To obtain the model implied covariance matrix ($\hat{\Sigma}$), we need to fit our hypothesized, wrong, H0 model (`modelH0`) to the population data (`covPop`). The model-implied covariance matrix then can be obtained by calling `covH0 <- fitted(fitH0)$cov`.

We are now in the position to use the obtained covariance matrix in power analyses. Let's do a post-hoc power-analysis assuming N = 1000 and alpha =.05 by calling `semPower.postHoc` with the arguments `SigmaHat = covH0` and `Sigma = covPop`.

```{r eval=FALSE}
ph <- semPower.postHoc(SigmaHat = covH0, Sigma = covPop, alpha = .05, N = 250, df = df)
summary(ph)
```

The output (which is omitted here) indicates that fitting the hypothesized model to the population data is  associated with a discrepancy of $F_0$ = 0.133 (or RMSEA = .121 or SRMR = .140 or ...) and that the power to reject the H0 model is very high, 1 - beta = .993. 

It is instructive to compare the expected chi-square (computed via the obtained $F_0$) with the chi-square model test statistics as reported by `lavaan` when fitting the H0 model using the same number of observations as requested in the power analysis:

```{r eval=FALSE}
fitmeasures(fitH0, 'chisq')
ph$fmin * (ph$N-1)
```

`fitmeasures(fitH0, 'chisq')` prints the model chi-square test as obtained by `lavaan` when fitting `modelH1` to the population data (`covPop`, see above). The line `ph$fmin * (ph$N-1)` computes the expected chi-square, i. e.,  $F_0$ multiplied by $(N - 1)$. Obviously, both values match (= 33.17).

##### Using semPower.powerLav {-}

The process illustrated above can be simplified by calling [`semPower.powerLav`](#powerLav). In essence, all we need now are the population model string (`modelPop`) and the H0 model string (`modelH0`). This leads to:

```{r eval=FALSE}
ph <- semPower.powerLav(type = 'post-hoc',
                        modelPop = modelPop, modelH0 = modelH0,
                        alpha = .05, N = 250)
summary(ph$power)
```

which - of course - yields the same results.

#####  Using semPower.powerLav in conjunction with semPower.genSigma  {-}

Instead of defining the population covariance matrix through a `lavaan` model string, It is often easier to obtain the population covariance matrix through the [`semPower.genSigma`](#genSigma) function, because this takes care for many intricacies such as the correct definition of the residual variances. In the scenario above, the following leads to the same results:

```{r eval=FALSE}
generated <- semPower.genSigma(Phi = .50, 
                               loadings = list(
                                 c(.8, .7, .6),
                                 c(.7, .6, .5)))
ph <- semPower.powerLav(type = 'post-hoc', 
                        Sigma = generated$Sigma, modelH0 = modelH0, 
                        alpha = .05, N = 250)
summary(ph$power)
```

##### Using semPower.powerCFA {-}

As the scenario above involves a model and a hypothesis that can be immediately handled by the [`semPower.powerCFA`](#powerCFA) convenience function, actually none of the above is required and could be achieved by a simple call to `semPower.powerCFA`:

```{r eval=FALSE}
ph <- semPower.powerCFA(type = 'post-hoc', 
                        comparison = 'saturated',
                        Phi = .5,
                        loadings = list(
                          c(.8, .7, .6),
                          c(.7, .6, .5)),
                        alpha = .05, N = 250)
summary(ph$power)
```

In the background, `semPower.powerCFA` performs all the necessary steps outlined above, so the results will - of course - be the same. Note the argument `comparison = 'saturated'` was set so to obtain power to reject the H0 model when compared against the saturated model, which was also done in all examples above.


## Generate a covariance matrix and lavaan model strings {#generateCov}
Internally, all convenience functions performing a [model-based power analysis](#modelBasedPower) generate a population variance-covariance matrix and plug it along with proper model strings into the [`semPower.powerLav`](#powerLav) function, which in turn transforms a model-based power analysis into a [model-free power analyses](#modelFreePower) providing [population and model-implied covariance matrices](#InputCovMatrix) as input. 

For situations not covered in the `semPower` convenience functions, greater flexibility is offered when calling either [`semPower.powerLav`](#powerLav) or a model free power analysis providing [population and model-implied means and covariance matrices](#InputCovMatrix) directly. A useful utility function described in this section is [semPower.generateSigma](#genSigma), which offers several ways to generate a population variance-covariance matrix (and means) based on the model matrices or model features, respectively.

`semPower.genSigma` expects model matrices as input parameters, so different matrices need to be provided depending on whether a CFA or a SEM model is specified. 

##### CFA model{-}
In the CFA model, the model implied variance-covariance matrix is given by
$$\Sigma = \Lambda \Phi \Lambda' + \Theta$$
where $\Lambda$ is the $p \cdot m$ loading matrix, $\Phi$ is the variance-covariance matrix of the $m$ factors, and $\Theta$ is the residual variance-covariance matrix of the observed variables. The means are
$$\mu = \tau + \Lambda \alpha$$
with the $p$ indicator intercepts $\tau$ and the $m$ factor means $\alpha$.

Thus, to generate a CFA model implied covariance matrix, `Phi` and `Lambda` (or respective  [shortcuts](#factorDefinition)) need to be provided. For instance, the following generates the implied covariance matrix for a three factor model with factor-correlations as defined in `Phi` and the loading matrix as defined in `Lambda`:

```{r eval=FALSE}
  Phi <- matrix(c(
    c(1.0, 0.5, 0.1),
    c(0.5, 1.0, 0.2),
    c(0.1, 0.2, 1.0)
  ), byrow = T, ncol=3)
  Lambda <- matrix(c(
    c(0.4, 0.0, 0.0),
    c(0.7, 0.0, 0.0),
    c(0.8, 0.0, 0.0),
    c(0.0, 0.6, 0.0),
    c(0.0, 0.7, 0.0),
    c(0.0, 0.4, 0.0),
    c(0.0, 0.0, 0.8),
    c(0.0, 0.0, 0.7),
    c(0.0, 0.0, 0.8)
  ), byrow = T, ncol = 3)
  
  gen <- semPower.genSigma(Phi = Phi, Lambda = Lambda)
```

`semPower.genSigma` returns a list comprising all model matrices (in the example above, `Lambda`, `Phi`, and the variance-covariance matrix of the manifest residuals, `Theta`) and the  model-implied variance-covariance matrix `Sigma`. In addition, various `lavaan` model strings are also returned:

* `modelPop` model string defining a population model corresponding to the model matrices. 
* `modelTrue` analysis model (yielding a perfect model fit) .
* `modelTrueCFA` only for SEM models: a pure CFA analyses model string omitting any regression relationships between the latent factors and rather allowing all latent factors to be correlated.  

Thus, plugging the generated variance-covariance matrix and the generated true model string into  `lavaan` yields estimates that mirror the defined population matrices: 

```{r eval=FALSE}
library(lavaan)
summary(sem(gen$modelTrue, 
            sample.cov = gen$Sigma, 
            sample.nobs = 1000,
            sample.cov.rescale = FALSE))

```

Instead of providing the complete loading matrix as argument for `Lambda`,  `semPower.genSigma`  also understands the shortcuts as described in detail [here](#factorDefinition).

If any of the arguments is provided as a list (e. g., `Lambda = list(Lambda1, Lambda2)`), multiple implied covariance matrices are returned. This is particularly useful for multigroup analyses, where the covariance matrices differ across groups. For instance, the following defines the same measurement model for two groups (by providing a single argument to `Lambda`), but defines different factor correlations for the groups, so that two model-implied covariance matrices are returned:

```{r eval=FALSE}
  Phi1 <- matrix(c(
     c(1.0, 0.5, 0.1),
     c(0.5, 1.0, 0.2),
     c(0.1, 0.2, 1.0)
  ), byrow = T, ncol=3)
  Phi2 <- matrix(c(
     c(1.0, 0.6, 0.2),
     c(0.6, 1.0, 0.3),
     c(0.2, 0.3, 1.0)
  ), byrow = T, ncol=3)
  
  gen <- semPower.genSigma(Phi = list(Phi1, Phi2), Lambda = Lambda)
```

##### SEM model{-}
If the model-implied covariance-matrix is rather to be determined from a structural equation model (instead a CFA model), `semPower.genSigma` expects as arguments 
`Beta`, `Psi`, and `Lambda` (or respective  [shortcuts](#factorDefinition)). In the structural equation model, the model-implied covariance matrix is given by

$$\Sigma = \Lambda (I - \mathbf{B})^{-1} \Psi [(\mathbf{I} - \mathbf{B})^{-1}]'  \Lambda' + \Theta$$
where $\mathbf{B}$ is the $m \cdot m$ matrix containing the regression slopes and $\Psi$ is the (residual) variance-covariance matrix of the $m$ factors. The means are
$$\mu = \tau + \Lambda (\mathbf{I} - \mathbf{B})^{-1} \alpha$$

The structural part of the model is primarily defined through Beta ($\mathbf{B}$). As an example, suppose there are four factors ($X_1$, $X_2$, $X_3$, $X_4$), and Beta is defined as follows:
$$
\begin{array}{lrrr}
    & X_1 & X_2 & X_3 & X_4\\
X_1 & 0.0 & 0.0 & 0.0 & 0.0 \\
X_2 & 0.0 & 0.0 & 0.0 & 0.0  \\
X_3 & 0.2 & 0.3 & 0.0 & 0.0  \\
X_4 & 0.3 & 0.5 & 0.0 & 0.0  \\
\end{array}
$$
Each row specifies how a particular factor is predicted by the available factors,
so the above implies the following regression relations:

$$
X_1 = 0.0 \cdot X_1 +  0.0 \cdot X_2 + 0.0 \cdot X_3 + 0.0 \cdot X_4 \\
X_2 = 0.0 \cdot X_1 +  0.0 \cdot X_2 + 0.0 \cdot X_3 + 0.0 \cdot X_4 \\
X_3 = 0.2 \cdot X_1 +  0.3 \cdot X_2 + 0.0 \cdot X_3 + 0.0 \cdot X_4 \\
X_4 = 0.3 \cdot X_1 +  0.5 \cdot X_2 + 0.0 \cdot X_3 + 0.0 \cdot X_4
$$

which simplifies to

$$
X_3 = 0.2 \cdot X_1 + 0.3 \cdot X_2 \\
X_4 = 0.3 \cdot X_1 + 0.5 \cdot X_2
$$

Psi ($\Psi$) defines whether there are (residual-)correlations between the factors. Suppose that $\Psi$ is
$$
\begin{array}{lrrr}
    & X_1 & X_2 & X_3 & X_4\\
X_1 & 1.0 & 0.3 & 0.0 & 0.0 \\
X_2 & 0.3 & 1.0 & 0.0 & 0.0 \\
X_3 & 0.0 & 0.0 & 1.0 & 0.2 \\
X_4 & 0.0 & 0.0 & 0.2 & 1.0 \\
\end{array}
$$

which implies a correlation between $X_1$ and $X_2$ of .3 and a residual correlation
between $X_3$ and $X_4$ of .2.

The scenario just described can be achieved by defining `Beta` and `Psi` accordingly and plugging these as input to `semPower.genSigma`:

```{r eval=FALSE}
Beta <- matrix(c(
  c(.00, .00, .00, .00),
  c(.00, .00, .00, .00),
  c(.20, .30, .00, .00),
  c(.30, .50, .00, .00)
), byrow = TRUE, ncol = 4)
Psi <- matrix(c(
  c(1, .30, .00, .00),
  c(.30, 1, .00, .00),
  c(.00, .00, 1, .20),
  c(.00, .00, .20, 1)
), byrow = TRUE, ncol = 4)

gen <- semPower.genSigma(Beta = Beta, Psi = Psi, Lambda = diag(ncol(Beta)))

```

In the example above, a manifest-variable only model is defined by setting `Lambda = diag(ncol(Beta))`. If providing a loading matrix or any of the shortcuts instead, a genuine SEM model is defined, as in the following example:

```{r eval=FALSE}
gen <- semPower.genSigma(Beta = Beta, Psi = Psi, 
                         loadM = .5, nIndicator = c(5, 4, 5, 6))

```

`semPower.genSigma` again returns a list comprising all model matrices (here: `Lambda`, `Beta`, `Psi`, and the variance-covariance matrix of the manifest residuals, `Theta`) and the  model-implied variance-covariance matrix `Sigma`. In addition, the same `lavaan` model strings as in the CFA case are returned. Thus, again, plugging the generated variance-covariance matrix and the generated true model string into  `lavaan` yields estimates that mirror the defined population matrices: 

```{r eval=FALSE}
summary(sem(gen$modelTrue, 
            sample.cov = gen$Sigma, 
            sample.nobs = 1000,
            sample.cov.rescale = FALSE))
```

In the SEM case, `semPower.genSigma` also returns a pure CFA model which just estimates the factors and allows these to correlate, but discards any regression relationships between the factors:

```{r eval=FALSE}
summary(sem(gen$modelTrueCFA, 
            sample.cov = gen$Sigma, 
            sample.nobs = 1000,
            sample.cov.rescale = FALSE))
```



# References {-}

* Browne, M. W., & Cudeck, R. (1992). Alternative ways of assessing model fit. *Sociological Methods & Research, 21*, 230–258.

* Jöreskog, K. G., & Sörbom, D. (1984). *LISREL VI user’s guide* (3rd ed.). Mooresville: Scientific Software.

* Lin, J., & Bentler, P. M. (2012). A third moment adjusted test statistic for small sample factor analysis. *Multivariate Behavioral Research, 47(3),* 448-462.

* McDonald, R. P. (1989). An index of goodness-of-fit based on noncentrality. *Journal of Classification, 6*, 97–103.

* MacCallum, R. C., Browne, M. W., & Sugawara, H. M. (1996). Power analysis and determination of sample size for covariance structure modeling. *Psychological Methods, 1*, 130–149.

* Moshagen, M., & Erdfelder, E. (2016). A new strategy for testing structural equation models. *Structural Equation Modeling, 23*, 54–60.

* Rosseel, Y. (2012). lavaan: An R package for structural equation modeling. *Journal of Statistical Software, 48,* 1-36.

* Steiger, J. H. (1990). Structural model evaluation and modification: An interval estimation approach. *Multivariate Behavioral Research, 25*, 173–180.

* Steiger, J. H., & Lind, J. C. (1980). *Statistically based tests for the number of common factors*. Presented at the Annual meeting of the Psychometric Society, Iowa City.

