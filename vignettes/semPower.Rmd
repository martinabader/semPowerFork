---
title: "Power Analysis for Structural Equation Models: semPower 2 Manual"
author: "Morten Moshagen & Martina Bader"
date: "`r Sys.Date()`"
output:
#   bookdown::pdf_book
   bookdown::gitbook:
     base_format: rmarkdown::html_vignette
     split_by: none
     self_contained: true
     config:
       toc:
         collapse: none
         scroll_highlight: yes
         before: |
            <li><a href="./">semPower 2 Manual</a></li>                  
       search:
         engine: lunr
       sharing:
         facebook: false
         twitter: false
       info: false

vignette: >
  %\VignetteIndexEntry{Power Analysis for Structural Equation Models: semPower 2 Manual}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---


```{=html}
<style type="text/css">
.book .book-body .page-wrapper .page-inner {
  max-width: 95%; !important;
}
.book .book-summary {
  width: 380px;
  position:absolute;
  top:0;
  left:-380px;
}
.book.with-summary .book-header.fixed {
    left: 380px;
}
.book.with-summary .book-body {
    left: 380px;
}
</style>
```

```{r load-packages, include=FALSE}
library(semPower)
```

Contact: <morten.moshagen@uni-ulm.de>

Please cite as:

* Moshagen, M., & Erdfelder, E. (2016). A new strategy for testing structural equation models. *Structural Equation Modeling, 23*, 54–60. [https://doi.org/10.1080/10705511.2014.950896](https://doi.org/10.1080/10705511.2014.950896)


##### Version history{-}
The first iteration of `semPower` was developed as a java program in 2016 and was ported as a slightly extended version to R a year afterwards. These versions provided support for a priori, post-hoc, and compromise [model-free power analyses](#modelFreePower) based on common effect-size measures such as the RMSEA. The R package was subsequently extended to support covariance matrices as input as an alternative way to define the effect. The current, second version of `semPower` expands this approach by providing many convenience functions to define the effect in [terms of model parameters](#modelBasedPower), covering many commonly encountered model structures, and also supports [simulated power](#simulatedPower) estimation in addition to analytical power analyses.

##### Installation{-}

The `semPower` package can be installed via [CRAN](https://cran.r-project.org/package=semPower). The latest development version is available from github at [https://github.com/moshagen/semPower](https://github.com/moshagen/semPower) and can be installed as follows:

```{r eval=FALSE}
# install.packages("devtools")
devtools::install_github("moshagen/semPower")
```

(Very) basic functionality is also provided as a shiny app, which can be used online at [https://sempower.shinyapps.io/sempower](https://sempower.shinyapps.io/sempower).


# Introduction

`semPower` provides a collection of functions to perform power analyses for structural equation models. Statistical power is a concept arising in the context of classical (frequentist) null-hypothesis significance testing and is defined as the probability to reject a certain hypothesis, if this hypothesis is factually wrong. As a general rule, a hypothesis test is only meaningful to the extent that statistical power is reasonably high, because otherwise a non-significant test outcome carries little information regarding the veracity of the tested hypothesis.

For illustration, consider a simple two-factor CFA model and assume that the interest lies in detecting that the correlation between the factors differs from zero. To test the hypothesis that the factors are uncorrelated, one can compare a model that freely estimates the factor correlation with an otherwise identical model that restricts the correlation between these factors to zero. If the restricted model fits the data significantly worse, the hypothesis of a zero correlation between the factors is rejected, in turn informing the conclusion that the correlation between the factors differs from zero. 

Statistical power now gives the probability that the outcome of the model test associated with this hypothesized model turns out significant on a certain alpha error level with a certain sample size. Suppose that the correlation between the factors is $r = .20$ in the population, each factor is measured by 3 indicators and all (non-zero) loadings equal .5, $\alpha = .05$, and a sample size of $N = 125$. Then, the probability of obtaining a significant test outcome to detect a correlation of $r \geq .20$ is just 20%. Stated differently, in 4 out of 5 random samples (each of size $N = 125$) one will *not* detect that the factors are actually correlated. Indeed, to obtain a more reasonable power of 80% in this scenario, a sample size of $N = 783$ is required. 

Correspondingly, statistical power is an integral part in planning the required sample size and statistical hypothesis testing more generally. Of note, however, a sufficiently large sample to obtain a certain power does not always imply a sufficiently large sample to enable model estimation: At times, the required sample size to yield a sufficiently high power might still be too small for the model at hand, so that sample size considerations should also be based on other factors beyond statistical power.


## Types of power analyses

Generally, statistical power depends on

* the extent to which the tested hypothesis is wrong (= the magnitude of effect)
* the sample size (N)
* the alpha error (alpha)
* the degrees of freedom (df)

and will be higher for a larger effect, a larger sample size, a higher alpha error, and fewer degrees of freedom. When performing a power-analysis, one of these quantities is computed as function of the other quantities, giving rise to different types of power analyses:

* **A priori power analysis**: Determines the required sample size to detect a certain effect with a desired power, given alpha and df. 
* **Post-hoc power analysis**: Determines the achieved power to detect a certain effect with a given sample size, alpha and df. 
* **Compromise power analysis**: Determines the alpha and the beta error, given the alpha/beta ratio, the sample size, a certain effect, and the df. 

Each type of power analysis has different aims. A priori power analyses are performed prior to data collection to inform the required number of observations to detect an expected effect with the desired power. Post-hoc power analyses are performed after data collection to judge whether the achieved power with the realized sample size is sufficiently high to allow for a meaningful test of a certain hypothesis. Compromise power analysis are used to determine which decision rule to apply in evaluating whether the outcome of a hypothesis test favors the null or the alternative hypothesis (see Moshagen & Erdfelder, 2016, for details).

## Types of hypotheses

Statistical power is always tied to a particular hypothesis, so power will typically differ even for the same (base-)model depending on which hypothesis is considered. A general statement such as "power was 80%" is meaningless, unless the hypothesis for which power was determined is also stated (such as "power to detect a correlation $\geq .2$ between the first and the second factor was 80%."). Moreover, power can be high for one hypothesis, but low for other hypotheses, so a power analysis should always be performed concerning (a) the focal hypotheses and (b) the hypothesis where the smallest effect is expected.

In SEM, the typical types of hypotheses that occur either implement equality constraints on two or more parameters (such as cross-group constraints) or assign a particular parameter a specific value (such as zero).^[A third type implements order constraints on parameters, but this is associated with certain intricacies concerning the limiting distributions and not covered by `semPower`.] For instance, consider a cross-lagged panel model (CLPM) with two constructs *X* and *Y* measured by three indicators each at three different waves (so the full model comprises six factors). Relevant (non-exhaustive) hypotheses could be 

* whether the model as a whole describes the data.
* whether measurement invariance over time concerning the indicators holds.
* whether the autoregressive effects of *X* are constant across waves.
* whether the autoregressive effects of *X* are different from zero.
* whether the autoregressive effects of *Y* are constant across waves.
* whether the autoregressive effects of *Y* are different from zero.
* whether the autoregressive effects of *X* are equal to those of *Y*.
* whether the cross-lagged effects of *X* on *Y* are constant across waves.
* whether the cross-lagged effects of *Y* on *X* are constant across waves.
* whether the cross-lagged effects of *X* on *Y* are different from zero.
* whether the cross-lagged effects of *Y* on *X* are different from zero.
* whether the cross-lagged effects of *X* on *Y* are equal to those of *Y* on *X*.
* whether the synchronous (residual) correlations between *X* and *Y* differ from zero.
* whether the synchronous (residual) correlations are equal over waves.

Any of these hypotheses will likely be associated with a different expectation of what reflects the relevant magnitude of effect and consequently with a different power to detect this effect. For example, concerning the hypothesis whether an autoregressive effect differs from zero, one will usually be satisfied to obtain sufficient power to detect a large regression coefficient (of, say, $b \geq .50$). Concerning the cross-lagged effects, however, one rather wants sufficient power to detect a small regression coefficient (of, say, $b \geq .10$). Furthermore, a cross-lagged effect of *X* on *Y* of .10 will not always be associated with the same power as a cross-lagged effect of *Y* on *X* of the same magnitude, because power to detect a cross-lagged effect also depends on all other parameters of the model, such as autoregressive effects and loadings. 

To give a number of examples, the following provides the required samples to yield a power of 80% on $\alpha = .05$, assuming that *X* and *Y* are measured by three indicators each (in each wave),  all loadings on *X* are .5 and those on *Y* are .7, and all autoregressive and cross-lagged effects are constant across waves and equal for *X* and *Y*:

* N = 238 to detect that the model exhibits misfit corresponding to RMSEA $\geq$ .05. 
* N = 124 to detect that the autoregressive effect of *X* is $\geq$ .50
* N = 54 to detect that the autoregressive effect of *Y* is $\geq$ .50
* N = 1836 to detect that the cross-lagged effect of *X* on *Y* is $\geq$ .10
* N = 2110 to detect that the cross-lagged effect of *Y* on *X* is $\geq$ .10

Correspondingly, one should carefully define a relevant hypothesis of interest when performing a power analysis (and again note that the required N to achieve sufficient power is not necessarily sufficient to support model estimation).

## Performing power analyses

Next to defining a relevant hypothesis, performing a power analysis (obviously)  requires a decision on which type of power analysis to perform. The types of power analyses available in `semPower` are:

* [semPower.aPriori](#ap) to perform an a priori power analysis (i .e., determine the required sample size).
* [semPower.postHoc](#ph) to perform a post-hoc power analysis (i. e., determine the achieved power).
* [semPower.compromise](#cp) to perform a compromise power analysis (i. e., determine a reasonable decision rule).

Any power analysis requires to specify the magnitude of effect that is to be detected in a certain metric. The functions stated above understand the following [effect-size measures](#effects): F0, RMSEA, Mc, GFI, and AGFI. Because any of these effect-measures apply equally regardless of the particular type of model considered, the function above are also referred to as [model free power analyses](#modelFreePower). For example, the statement that power to reject a model involving 100 df exhibiting misspecifications corresponding to RMSEA $\geq .05$ on alpha = .05 with a sample size of 250 is 1 - beta = 97% is always true, regardless of whether the model under scrutiny is a CFA model, a CLPM, a multigroup model, or any other SEM model. It is the very nature of effect-sizes that they are agnostic with respect to how a particular model looks like. 

However, a common problem in performing a power analysis is that it is often difficult to translate a specific hypothesis into a specific value for a specific effect size (such as a specific value for the RMSEA). Consider the situation that one is interested in determining whether two factors in a CFA model are correlated. A suitable model to test this hypothesis would constrain the correlation between these factors to zero. When this constrained model fits the data as well as the unconstrained model (freely estimating the correlation), both factors can be assumed to be orthogonal. Otherwise, one would conclude that the factors are correlated. Suppose that a correlation between these factors of $r \geq .1$ is considered a meaningful deviation of orthogonality. In terms of power analyses, we thus want sufficient power to identify whether the correlation between the factors is at least $r = .1$. The misfit associated with a model assuming a correlation of 0 when, in reality, the true correlation is at least $r = .1$ is supposed to define the magnitude of effect. The problem is now that one cannot immediately say how this difference in a certain model parameter (the correlation between the factors) translates to an effect size such as the RMSEA. 

For this reason, `semPower` also provides various convenience functions that allow for a [model-based definition](#modelBasedPower) of the effect of interest in terms of model parameters as well as a more generic definition of the effect as function of [population and model-implied means and covariance matrices](#InputCovMatrix). In the example above, the relevant convenience function ([semPower.powerCFA](#powerCFA)) just requires the definition of the factor model and the specification of the to-be detected correlation as input, and plugs the associated effect-size (which is RMSEA = .05 in the scenario above when each factor is measured by three indicators loading by .5 each, and when df = 1) into one of the model-free power functions. Currently, `semPower` provides model-based power analyses for the following model types:

* Hypotheses arising in a [standard CFA model](#powerCFA).
* Hypotheses arising in models involving a [bifactor structure](#powerBifactor).
* Hypotheses related to [latent regressions](#powerRegression).
* Hypotheses related to [mediation models](#powerMediation).
* Hypotheses related to [generic path models](#powerPath).
* Hypotheses related to [measurement invariance across groups](#powerMI).
* Hypotheses arising in a [cross-lagged panel model](#powerCLPM).
* Hypotheses arising in a [random intercept cross-lagged panel model](#powerRICLPM).
* Hypotheses arising in a [generic structural equation model](#powerLav).

The remainder of this document provides some notes on the statistical background, a formal definition of decision errors in hypothesis testing, statistical power, and various effect-sizes, and a detailed description of the functions contained in this package.

# Statistical Background

This chapter provides a brief statistical background on hypothesis testing,  model estimation, and effect sizes in SEM.

## Hypothesis Testing and Statistical Power

The statistical evaluation of mathematical models often proceeds by considering a test-statistic that expresses the discrepancy between the observed data and the data as implied by the fitted model. In SEM, the relevant test statistic for a sample of size $N$ is given by $T = \hat{F}(N-1)$. $\hat{F}$ denotes the minimized sample value of the chosen discrepancy function (such as Maximum-Likelihood) and thereby indicates lack of fit of the model to the sample data. Thus,  $T$ permits a likelihood-ratio test of the null hypothesis (H0) that the model is correct. If the hypothesized model holds in the population, $T$ can be shown to follow asymptotically a central $\chi^2$(df) distribution with $df = .5 \cdot p(p+1) - q$ degrees of freedom, where $p$ is the number of manifest variables and $q$ denotes the number of free parameters. This is why $T$ is often referred to as “chi-square model test statistic” -- a convention which is followed here. 

Based on the observed value for the chi-square test statistic, a null hypothesis significance test can be performed to evaluate whether this value is larger than would be expected by chance alone. The usual test proceeds as follows: Given a certain specified alpha-error level (typically alpha = .05), a critical chi-square value is obtained from the asymptotic central $\chi^2$(df) distribution. If the observed value for the chi-square test statistic exceeds the critical value, the null hypothesis that the model fits the data is rejected. Otherwise, H0 is retained. A finding of the observed statistic exceeding the critical value (implying an upper-tail probability  that falls below the specified alpha level) thus leads to the statistical decision that the discrepancy between the hypothesized and the actual population covariance matrix is too large to be attributable to sampling error only. Accordingly, a statistically significant chi-square test statistic provides evidence against the validity of the hypothesized model.

When testing statistical hypotheses using this framework, two types of decision errors can occur: The alpha error (or type-I error) of incorrectly rejecting a true H0 (a correct model) and the beta error (or type-II error) of incorrectly retaining a false H0 (an incorrect model). Statistical power is defined as the complement of the beta-error probability (1 - beta) and thus gives the probability to reject an incorrect model. 

If the H0 is false, the chi-square test statistic is no longer central $\chi^2$(df) distributed, but can be shown to follow a noncentral $\chi^2$(df, $\lambda$) distribution with non-centrality parameter $\lambda$ and expected value df + $\lambda$ (MacCallum, Browne, & Sugawara, 1996). The non-centrality parameter $\lambda$ shifts the expected value of the non-central $\chi^2$(df, $\lambda$) distribution to the right of the corresponding central distribution. Having determined the critical value associated with the desired alpha probability from the central $\chi^2$(df) distribution, the beta-error probability can be computed by constructing the corresponding non-central $\chi^2$(df, $\lambda$) distribution with a certain non-centrality parameter $\lambda$ and obtaining the area (i.e., the integral) of this distribution to the left of the critical value:

$$ \beta = \int_{0}^{\chi^2_{crit}} f_{\chi^2(df, \lambda)}(x) \, dx$$
Correspondingly, statistical power is the area of the non-central $\chi^2$(df, $\lambda$) distribution to the right of the critical value, i. e., $= 1 - \beta$. The general situation is illustrated in the following figure.

```{r figure1, echo=FALSE, fig.cap = "Central (red) and non-central (blue) chi-square distributions."}
semPower.showPlot(chiCrit = 124.3421, ncp = 40.75, df = 100)
```

The figure depicts a central (solid)  $\chi^2$(df = 100) and a non-central (dashed)  $\chi^2$(df = 100, $\lambda = 40.75$). The area of the central distribution $\chi^2$(df) to the right of the critical value reflects the alpha error. The dotted line indicates a critical value of 124, which corresponds to alpha = .05. The area of $\chi^2$(df, $\lambda$) distribution to the left of the critical value is the beta-error probability, which in this example  takes a value of beta = .20. Statistical power is defined as 1 - beta, that is, the area under the noncentral $\chi^2$(df, $\lambda$) distribution to the right of the critical value.


## Measures of Effect {#effects}
As evident from the above, power depends on the applied critical value corresponding to a certain alpha error probability and on the distance between the central and the non-central $\chi^2(df)$ distributions as quantified by the non-centrality parameter $\lambda$. The non-centrality parameter $\lambda$, in turn, depends on the number of observations $N$ and on the degree to which the the tested H0 is factually wrong, i. e., on the discrepancy between the H0 and the H1 model (the effect size). 

To define the discrepancy between the H0 and the H1 model for power analysis, any non-centrality based measure of effect can be used. For [model-free power analyses](#modelFreePower), `semPower` understands the measures detailed below. Any [model-based power analysis](#modelBasedPower) is eventually converted into the [population minimum of the fit function](#effectsF0) as effect-size.

##### F0  {-#effectsF0}

$F_0$ is the population minimum of the Maximum-Likelihood fitting function, defined as

$$F_0 = \log|\Sigma| - \log|\hat{\Sigma}| + tr(\hat{\Sigma}\Sigma) - p  + (\mu - \hat{\mu}) \hat{\Sigma}^{-1} (\mu - \hat{\mu}) $$
where $\Sigma$ is the $p \cdot p$ population covariance matrix, $\hat{\Sigma}$ the $p \cdot p$ model-implied covariance matrix, $p$ the number of observed variables, $\mu$ the vector of population means, and $\hat{\mu}$ the model-implied means. If means are not part of the model, the second part becomes zero.

If the model is correct, $\hat{\Sigma} = \Sigma$, $\hat{\mu} = \mu$, and thus $F_0 = 0$. Otherwise, $F_0 > 0$ with higher values expressing a larger discrepancy (misfit) of the model to the data. 

If fitting a model to some sample data of size $N$, the estimated minimum of the fit-function, $\hat{F}$, is used to construct an asymptotically $\chi^2$-distributed model test-statistic, commonly simply called the chi-squared model test: 

$$\chi^2 = \hat{F}(N-1)$$
Note that $\hat{F}$ is a biased estimate of $F_0$. If $F_0 = 0$, the  expected value of $\hat{F}$ is df, i. e. the model degree of freedom. For a model with $q$ free parameters, the df are given by
$$ df = \dfrac{p(p+1)}{2} - q  $$

Whereas $F_0$ is the genuine measure of effect in SEM, the main disadvantage is that specific values are difficult to interpret, because of its logarithmic scaling and because specific values also depends on features unrelated to model fit such as a the number of manifest variables comprised on the model. For these reasons, various transformations of $F_0$ exist that are described in the following.


##### RMSEA {-} 
The Root-Mean-Squared Error of Approximation (RMSEA; Browne & Cudeck, 1992; Steiger & Lind, 1980) scales $F_0$ by the model degrees of freedom:

$$RMSEA = \sqrt{(F_0/df)}$$
so that the RMSEA is bounded by zero and lower values indicate better fit. The implied $F_0$ is:
$$F_0 = df \cdot RMSEA^2$$
Given that $F_0$ is scaled by the df, defining an effect in terms of the RMSEA requires specification of the degrees of freedom. 

##### Mc {-} 

McDonald's (1989) measure of non-centrality (Mc) is a transformation of $F_0$ on the interval 0-1 with higher values indicating better fit.

$$Mc = e^{-.5F_0}$$
so that
$$F_0 = -2\ln{Mc}$$



##### GFI {-} 

The Goodness-of-Fit Index (GFI; Jöreskog & Sörbom, 1984; Steiger, 1990) scales $F_0$ on the interval 0-1 with higher values indicating better fit:

$$GFI = \dfrac{p}{p+2F_0}$$

$$F_0 = \dfrac{p (1-GFI)}{2GFI}$$
As the GFI depends on the number of observed variables ($p$), this number need to be provided when defining an effect in terms of the GFI.


##### AGFI {-} 

The Adjusted Goodness-of-Fit Index (AGFI; Jöreskog & Sörbom, 1984; Steiger, 1990) modifies the GFI by including a penalty for the number of free parameters, as measured by the model degrees of freedom: 

$$AGFI = 1 - \dfrac{p(p+1)}{2df} \left(1 - \dfrac{p}{p+2F_0} \right)$$
$$F_0 = \dfrac{p (1-AGFI) df}{p(p+1) -2df(1-AGFI)}$$

Specifying an effect in terms of the AGFI requires specification of both the number of observed variables ($p$) and the model degrees of freedom ($df$).



##### Measures not based on non-centrality  {-} 

Fit-indices that are not based on non-centrality have no straightforward relation to $F_0$ and are thus not well suited for power-analyses. However, when the input parameters include covariance matrices, `semPower` also reports the following measures.

###### SRMR {-}
The Standardized-Root-Mean-Square Residual (SRMR) is a measure of the (root of the) average (squared) difference between the (standardized) model-implied and population covariance matrices, so that it ranges from 0 to 1 with lower values indicating better fit. Let $E_0$ be the difference between the model-implied and the population covariance matrix, $E_0 = \Sigma - \hat{\Sigma}$, $vech$ denote the vectorization transformation, and $Q$ be a diagonal matrix of dimension $.5p(p+1)$ containing the inverse of the product of standard deviations of observed variables $i$ and $j$. Then, the SRMR can be defined as

$$SRMR = \sqrt{\dfrac{1}{.5p(p+1)}vech(E_0) \cdot Q \cdot vech(E_0)'}$$

The relation of the residual matrix $E_0$ to $F_0$ is complex and depends on the model-implied covariance matrix, so the SRMR is not well suited to define an effect in terms of $F_0$ (based on ML estimation): 
$$F_0 = -\ln|I + \hat{\Sigma}^{-.5} E_0 \hat{\Sigma}^{-.5}|$$


###### CFI {-}
The Comparative Fit Index (CFI) is an incremental index expressing the proportionate reduction of misfit associated with the hypothesized model ($F_{0H}$) in relation to the null-model ($F_{0N}$), defined as a model that constrains all covariances to zero. In the population, the CFI ranges from 0 to 1 with higher values indicating better fit.

$$CFI = \dfrac{F_{0N}-F_{0H}}{F_{0N}}$$

Whereas it is simple to obtain $F_0$ from the CFI, this requires knowledge of $F_{0N}$, which is rather difficult to determine a priori:

$$F_0 = F_{0N} - CFI \cdot F_{0N} $$




# Model-free power analyses {#modelFreePower}

Performing a power analysis generally requires the specification of a measure and magnitude of effect that is to be detected and a provision of the model degrees of freedom (df). Further arguments are required depending on the type of power analysis.

The functions described in this chapter are "model-free" in the sense that the results depend on the df of a model, but are otherwise agnostic with respect to how a particular model looks like. For instance, the power to reject a model with df = 100 exhibiting an RMSEA $\geq$ .05 with N = 500 on alpha = .05 is always the same, regardless of whether the model is a CFA model, a mediation model, a CLPM, or a multigroup model.

By contrast, [model-based power analyses](#modelBasedPower) define the effect of interest in terms of particular model parameters, so different functions are required for different types of models. However, the functions performing model-based power analysis are actually only a high level interface and eventually transform a particular hypothesis concerning the model parameters into an effect size understood by model-free power analyses.

Thus, regardless of whether the effect of interest is directly defined in terms of an [effect size](#effects) understood by `semPower` or indirectly via constrains on particular model parameters, the actual power analysis is always performed by one of the following functions. 


## A-priori power analysis: Determine N{#ap}

The purpose of a-priori power analyses is to determine the required sample size to detect an effect with a certain probability on a specified alpha error. In the language of structural equation modeling, an a-priori power analysis asks: How many observations do I need to detect the effect of interest (i. e., falsify the model under scrutiny) with a certain probability (statistical power)?

Performing an a-priori power analyses requires the specification of:

* the alpha error (`alpha`)
* the desired power (`power`; or, equivalently, the acceptable beta error, `beta`)
* the type of effect (`effect.measure`) 
* the magnitude of effect (`effect`)
* the degrees of freedom of the model (`df`). See [how to obtain the df](#getDf) if you are unsure.

Depending on the chosen effect-size measure, it may also be required to define the number of observed variables (`p`). 

Suppose, one wants to determine the required sample size to detect misspecifications of a model (involving df = 50 degrees of freedom) with a power of 80% on an alpha error of .05, where the amount of misfit corresponds to an RMSEA of at least .05. To achieve this, the function `semPower.aPriori` is called with arguments `effect = .05`, `effect.measure = 'RMSEA'`, `alpha = .05`, `power = .80`, and `df = 50`. The results are stored in a list called `ap`. 

```{r eval=FALSE}
ap <- semPower.aPriori(effect = .05, effect.measure = 'RMSEA', 
                        alpha = .05, power = .80, df = 50)
```

Equivalently, instead of calling the `semPower.aPriori`, one may also use the generic `semPower` function with the additional `type = 'a-priori'` argument: 

```{r}
ap <- semPower(type = 'a-priori', 
               effect = .05, effect.measure = 'RMSEA', 
               alpha = .05, power = .80, df = 50)
```

Calling the `summary` method on `ap` prints the results and a figure of the associated central and non-central chi-squared distributions.    

```{r}
summary(ap)
```

This shows that N = 243 yields a power of approximately 80% to detect the specified effect. 

Note that whereas N $\geq$ 243 is sufficient to a yield the desired power, larger samples might be required to enable successful model estimation, for instance when the model under scrutiny comprises a large number of free parameters. The required sample size determined in an a priori power analysis merely gives the lower bound concerning the desired power, but does not consider aspects such as proper convergence and accuracy of parameter recovery when estimating the model under scrutiny. As a more extreme example, the following yields a required N of 14:
```{r eval=FALSE}
ap <- semPower(type = 'a-priori', 
               effect = .08, effect.measure = 'RMSEA', 
               alpha = .05, power = .80, df = 2000)
```
N = 14 is entirely correct in terms of the desired power (of 80%) for the stated hypothesis (reject a model exhibiting RMSEA $\geq$ .08 on 2000 df), but will generally be far from sufficient to support the estimation of a model involving 2000 df. As another extreme example, the following yields a required N of 78,516:
```{r eval=FALSE}
ap <- semPower(type = 'a-priori', 
               effect = .01, effect.measure = 'RMSEA', 
               alpha = .05, power = .80, df = 1)
```
N = 78,516 is also entirely correct when the aim is to yield a power of 80% to reject a model exhibiting an RMSEA $\geq$ .01 on 1 df. However, parameter estimation will likely succeed when the sample is much smaller. Correspondingly, the required sample size to obtain a certain power and the required sample size to obtain trustworthy parameter estimates are different issues. Power is just one aspect in sample size planning.

The output printed above further shows the `Critical Chi-Square`, the non-centrality parameter (`NCP`), and the ratio between the error probabilities (`Implied Alpha/Beta ratio`). In this example, the ratio between alpha and beta is approximately 0.25, showing that committing an beta error is four-times as likely as committing an alpha error. This is obviously a consequence of the chosen input parameters, since a power (1 - beta) of .80 implies an beta error of .20, which is four times the chosen alpha error of .05.

`semPower` also converts the chosen effect into other effect size measures: An RMSEA of .05 (based on df = 50) corresponds to $F_0$ = 0.125 and Mc = .939. If one is also interested in obtaining the associated GFI and AGFI, the number of variables needs to be provided. When the model involves 20 observed variables, the call above can be modified by including the argument `p = 20`:

```{r eval=FALSE}
ap <- semPower.aPriori(effect = .05, effect.measure = 'RMSEA', 
                       alpha = .05, power = .80, df = 50, p = 20)
```
Now the GFI and AGFI equivalents of RMSEA = .05, assuming df = 50 and p = 20, are also provided.

If one is interested in how power changes for a range of sample sizes, it is useful to request a [power plot](#plots).


## Post-hoc power analysis: Determine power {#ph}

The purpose of post-hoc power analyses is to determine the actually achieved power to detect a specified effect with given sample size on a certain alpha error. In the language of structural equation modeling, a post-hoc power analysis asks: With my sample at hand, how large is the probability (power) to detect the effect of interest?

Performing a post-hoc power analyses requires the specification of:

* the alpha error (`alpha`)
* the sample size (`N`)
* the type of effect (`effect.measure`) 
* the magnitude of effect (`effect`)
* the degrees of freedom of the model (`df`). See [how to obtain the df](#getDf) if you are unsure.

Depending on the chosen effect-size measure, it may also be required to define the number of observed variables (`p`). 

Suppose, one wants to determine the actually achieved power with a sample size of N = 1000 to detect misspecifications of a model (involving df = 100 degrees of freedom) corresponding to RMSEA $\geq$ .05 on an alpha error of .05. To achieve this, the function `semPower.postHoc` is called with arguments `effect = .08`, `effect.measure = 'RMSEA'`, `alpha = .05`, `N = 1000`, and `df = 100`, and the results are stored in a list called `ph`. 

```{r eval=FALSE}
ph <- semPower.postHoc(effect = .05, effect.measure = 'RMSEA', 
                      alpha = .05, N = 1000, df = 100)
```

Equivalently, instead of calling the `semPower.aPriori`, one may also use the generic `semPower` function with the additional `type = 'post-hoc'` argument: 

```{r}
ph <- semPower(type = 'post-hoc', 
               effect = .05, effect.measure = 'RMSEA',
               alpha = .05, N = 1000, df = 100)
summary(ph)
```

Calling the `summary` method on `ph` provides an output structured identically to the one produced by [`semPower.aPriori`](#ap) and shows that the power is very high (`power > .9999`). The associated error probabilities are provided in higher precision. Specifically, the beta error is `beta = 2.903302e-17` which translates into $2.9 \cdot 10^{-17} = 0.000000000000000029$. In practice, one would almost never miss a model with an actual RMSEA $\geq$ .05 (or F0 $\geq$ 0.25 or Mc $\leq$ .882) under these conditions. The implied alpha/beta ratio is 1.722177e+15, showing that committing an alpha error is about two quadrillion ($10^{15}$) times as likely as committing a beta error.

If one are interested in how power changes for a range of different magnitudes of effect (say, for RMSEAs ranging from .01 to .15), it is useful to request a [power plot](#plots).


## Compromise power analysis: Determine alpha and beta {#cp}

The purpose of compromise power analyses is to determine alpha and beta (and the associated critical value of the chi-squared test-statistic) given a specified effect, a certain sample size, and a desired ratio between alpha and beta (Moshagen & Erdfelder, 2016). In the language of structural equation modeling, a compromise analysis asks: With my sample at hand, how should the critical value for the chi-square model-test be defined to obtain proportionate alpha and beta errors in deciding whether my model is rather aligned with the hypothesis of perfect fit or with the hypothesis of unacceptable degree of misfit (as defined by the chosen effect)?

Performing a compromise power analyses requires the specification of:

* desired ratio between alpha and beta (`abratio`;  defaults to 1)
* the sample size (`N`)
* the type of effect (`effect.measure`) 
* the magnitude of effect (`effect`)
* the degrees of freedom of the model (`df`). See [how to obtain the df](#getDf) if you are unsure.

Depending on the chosen effect-size measure, it may also be required to define the number of observed variables (`p`). 

Suppose, one wants to determine the critical chi-square and the associated alpha and beta errors, forcing them to be equal (i. e., an ratio of 1). The model involves 100 df, the sample size is N = 1000, and the H1 model representing an unacceptable degree of misfit is defined as a model associated with an RMSEA of at least .08. Thus, the function `semPower.compromise` is called with  arguments `effect = .08`, `effect.measure = 'RMSEA'`, `abratio = 1`, `N = 1000`, and `df = 100`, the results are stored in a list called `cp`, and the `summary` method is called to obtain formatted results. 

```{r eval=FALSE}
cp <- semPower.compromise(effect = .08, effect.measure = 'RMSEA', 
                           abratio = 1, N = 1000, df = 100)
```

Equivalently, instead of calling `semPower.compromise`, one may also use the generic `semPower` function with the additional `type = 'post-hoc'` argument: 

```{r}
cp <- semPower(type = 'compromise', 
               effect = .05, effect.measure = 'RMSEA', 
               abratio = 1, N = 1000, df = 100)
summary(cp)
```

The output is structured identically to the one produced by [`semPower.aPriori`](#ap) and shows that choosing a `Critical Chi-Square = 312` is associated with balanced error probabilities, `alpha = 1.212986e-23` and `beta = 1.212986e-23`. As requested, both error probabilities as just as large. In addition, committing either error is highly unlikely: an error of `1.212986e-23` translates into $1.2 \cdot 10^{-23} = 0.000000000000000000000012$. In practice, one almost never would make a wrong decision.   

If one rather prefers the error probabilities to differ (for example because one  considers falsely accepting an incorrect model to be 100 times as bad as falsely rejecting an correct model), this can be achieved by changing the `abratio` argument accordingly. For example, requesting the alpha error to be 100 times as large as the beta error proceeds by setting `abratio = 100`. 

```{r}
cp <- semPower.compromise(effect = .08, effect.measure = 'RMSEA', 
                           abratio = 100, N = 1000, df = 100)
```


## Power-analysis to detect an overall difference between two models {#diffPower}

A common scenario is to test two competing models against each other, where a more restrictive model (involving more df) is compared against a less restrictive model (involving less df). When the difference between these models lies just in a single (or few) particular parameter(s), the effect should be determined [in terms of the model parameters](#modelBasedPower). If, however, the difference between the models potentially spreads across multiple parameters (as, say, when comparing a 3-factor with a 5-factor model), one approach to power analysis it to define the models in terms of overall fit. 

For example, to obtain the required sample size to yield a power of 80% to discriminate a model with an associated RMSEA of .04 on 44 df from a model with an associated RMSEA of .05 on 41 df, define both the `effect` and `df` arguments as vectors (do not define lists!) comprising two elements:
```{r eval=FALSE}
ap <- semPower.aPriori(effect = c(.04, .05), effect.measure = 'RMSEA', 
                       alpha = .05, power = .80, df = c(44, 41))
summary(ap)
```
which shows that 340 observations are required to discriminate these models. Post-hoc and compromise power analyses are performed accordingly.

A similar situation often occurs in tests of measurement invariance, where one wants sufficient power to detect whether certain cross-group or cross-time constraints on the model parameters (such as equal loadings across groups) are violated. Again, the difference between such models can be defined through [particular parameters](#powerMI), for instance by assuming a single loading differs by .1 across groups. However, it is also reasonable to assume that non-invariance spreads across multiple parameters (say, across all loadings), so that one approach to power analysis it to define the models in terms of overall fit. 

The general syntax is the same as above, but we now also need to set the `N` argument, which gives the number of observations by group in compromise and post-hoc power analysis, and the group weights in a-priori power analysis. For example, the following asks for the required sample size to detect a change in the Mc of .01 in a three-group model, where all groups are equal-sized (`N = c(1, 1, 1)`): 
```{r eval=FALSE}
ap <- semPower.aPriori(effect = c(.99, .98), effect.measure = 'Mc', 
                        alpha = .05, power = .80, df = c(69, 57), N = c(1, 1, 1))
summary(ap)
```
This shows that 858 observations (286 by group) are required for a power of 80%. 


## Define the effect through covariance matrices {#InputCovMatrix}

The previous sections assumed that the magnitude of effect is determined by defining a certain effect size metric (such as $F_0$ or RMSEA) and a certain magnitude of effect. Alternatively, the effect can also be determined by specifying the population ($\Sigma$) and the model-implied ($\hat{\Sigma}$) covariance matrices (and, if means are part of the model, $\mu$ and $\hat{\mu}$) directly. To determine the associated effect in terms of F0, `semPower` just plugs these matrices in the ML-fitting function: 

$$F_0 = \log|\Sigma| - \log|\hat{\Sigma}| + tr(\hat{\Sigma}\Sigma) - p  + (\mu - \hat{\mu}) \hat{\Sigma}^{-1} (\mu - \hat{\mu}) $$

Suppose, $\Sigma$ and $\hat{\Sigma}$ have been defined previously and are referred to by the variables `Sigma` and `SigmaHat` Then, any of the power-analysis functions is called setting the `Sigma` and `SigmaHat` arguments accordingly (and omitting the `effect` and `effect.measures` arguments). This could look as follows:

```{r eval=FALSE}
semPower.aPriori(alpha = .05, power = .80, df = 100, 
                 Sigma = Sigma, SigmaHat = SigmaHat)
semPower.postHoc(alpha = .05, N = 1000, df = 100, 
                 Sigma = Sigma, SigmaHat = SigmaHat)
semPower.compromise(abratio = 1, N = 1000, df = 100, 
                    Sigma = Sigma, SigmaHat = SigmaHat)
semPower.powerPlot.byN(alpha = .05, df = 100, power.min = .05, power.max = .99, 
                       Sigma = Sigma, SigmaHat = SigmaHat)
```

This feature is particularly useful when used in conjunction with other functions provided by `semPower` and is indeed internally used by all functions performing [model-based power analyses](#modelBasedPower). An example on how to obtain the relevant covariance matrices is provided in a [later chapter](#powerCov).




# Model-based power analyses {#modelBasedPower}
A general difficulty in [model-free power analysis](#modelFreePower) is that the relation between constrains on a particular model parameter and the resulting effect size is often not clear. For instance, obtaining the required N to detect a cross-lagged effect $\geq$ .10 in a CLPM with a certain power requires to translate the hypothesized cross-lagged effect to a non-centrality based effect size such as RMSEA, which is hardly feasible. `semPower` therefore provides various convenience function to simplify this process, which are described in this chapter.

The purpose of all convenience functions is to provide high-level interfaces that allow to specify the parameters of a certain model type (such as a CLPM) and a certain effect of interest in terms of the model parameters (such as a crossed-lagged effect). The convenience functions then obtain the relevant population and model-implied covariance matrices and perform the desired power analysis based on these matrices. All convenience functions therefore require that the `lavaan` package (Rosseel, 2012) is installed, and always require the specification of the relevant parameters for the [desired power analysis](#modelFreePower).

More precisely, all convenience functions internally obtain the population covariance matrix (and mean vector, if necessary) via [`semPower.genSigma`](#genSigma), define the H0 model (and optionally the H1 model), call [`semPower.powerLav`](#powerLav) to fit the models to the population values, which, in turn, obtains the model-implied covariance matrix (and mean vector), and plugs the population and model-implied matrices into one of the [model-free power analyses](#modelFreePower) functions. Since all of these low level functions are also exposed, an even higher level of flexibility can be achieved by calling these functions directly (see [this](#powerLav) and [this](#powerCov) chapter for illustrations).

Given that the functions performing a model-based power analysis operate on a factor model, it is always required to specify the factor model in terms of the number of factors, number of indicators, and loadings. For this reason, the chapter begins with a primer on how to [define the factor model](#factorDefinition).
 
## Definition of the factor model {#factorDefinition}

Although all convenience functions described in this chapter implement different model structures, all these structures involve latent factors, so the factor model always needs to be defined in terms of the number of factors, the number of indicators for each factors, and the loadings of each indicator on the factors. Indeed, the "factor" model also need to be specified if the model does not include any factor, but operates on observed variables (such as a CLPM based on observed scores, rather than on latent factors). There are several ways to achieve this, which are documented further below, for the impatient reader the easiest way is to add `Lambda = diag(p)`, where p is the number of variables contained in the model.

The magnitude of the factor loadings and the number of indicators per factor have a very large effect on statistical power (because both quantities increase factor determinacy and thus reduce random noise). For example, power to detect a factor correlation of $r \geq .2$ with N = 250 is 88% when both factors are measured by 10 indicators each and all loadings are .90, but only 11% when both factors are measured by 3 indicators each and all loadings are .30. Likewise, the dispersion of factor loadings also affects power, although to a (considerably) lesser degree. It is thus crucial to be careful in defining appropriate factor loadings, which should generally be based on previous empirical results.

Any `semPower` convenience function expects one of the following arguments defining the factor model:

* `Lambda` to define the loading matrix.
* `loadings` to define a reduced loading matrix that only contains the primary loadings.
*  `nIndicator` in conjunction with `loadM`  to define the number of indicators by factor along with a single loading for the indicators of each factor or a single loading to apply for all indicators. 

In general, the `nIndicator` and `loadM` arguments are usually the simplest approach, but do not allow for any dispersion of the loading magnitude within a factor. This flexibility is offered by the `loadings` argument, which in most cases should suit all needs. Providing the complete loading matrix via the `Lambda` argument is only required for more complex loadings patterns where a single indicator is supposed to load on more than one factor.

##### Provide the full loading matrix (`Lambda`)  {-}

Suppose there are two factors measured by 3 and 4 indicators, respectively, and all loadings are equal to .5. The first option to define this factor model is to provide a loading matrix as value to the `Lambda` argument:

```{r eval=FALSE}
Lambda <- matrix(c(
  c(.5, 0),
  c(.5, 0),
  c(.5, 0),
  c(0, .5),
  c(0, .5),
  c(0, .5),
  c(0, .5)
  ), ncol = 2, byrow = TRUE)
```

##### Provide the primary factor loadings only (`loadings`)  {-}

A second way to define the model above is to provide a list comprising two vectors as value to the `loadings` argument:

```{r eval=FALSE}
loadings <- list(
  c(.5, .5, .5),
  c(.5, .5, .5, .5)
  )
```
`loadings` must be a list of vectors, where each vector defines the loading of the indicators on the respective factor. One vector is needed for each factor, so in the example above the first vector comprises 3 and the second 4 elements, reflecting the desired loading matrix above. Note that the `loadings` argument assumes the absence of any secondary loading, so that each loading defined in the vectors refers to a single indicator. As another example, the following are two equivalent ways to define three factors with 3, 4, and 5 indicators loading by the specified values: 

```{r eval=FALSE}
Lambda <- matrix(c(
  c(.7, 0, 0),
  c(.6, 0, 0),
  c(.5, 0, 0),
  c(0, .5, 0),
  c(0, .8, 0),
  c(0, .6, 0),
  c(0, .3, 0),
  c(0, 0, .9),
  c(0, 0, .5),
  c(0, 0, .7),
  c(0, 0, .4),
  c(0, 0, .6)
  ), ncol = 3, byrow = TRUE)

loadings <- list(
  c(.7, .6, .5),
  c(.5, .8, .6, .3),
  c(.9, .5, .7, .4, .6)
  )
```


##### Provide a single loading to apply to indicator of a specific factor (`nIndicator` and `loadM`)  {-}

A third way to define two factors measured by 3 and 4 indicators, respectively, and all loadings equal to .5, is to provide both the `nIndicator` and the `loadM` arguments:
```{r eval=FALSE}
nIndicator <- c(3, 4)
loadM <- .5
```
`nIndicator` is a vector providing the number of indicators separately for each factor. `loadM` can be a single number (as in the example above) to say that all loadings have the same value. Alternatively, `loadM` can be a vector specifying the loadings separately for each factor, where each indicator of a specific factor takes the defined value as loading. Thus, specifying `loadM <- c(.5, .5)` would achieve the same result, whereas `loadM <- c(.5, .6)` would assign all indicators of the second factor a loading of .6. 

##### Factor models involving observed covariates {-}
To include additional observed variables (that do not act as a factor indicator) in a model, a dummy factor with a single indicator loading by 1 can be defined. For instance, each of the following options defines an observed variable and a factor with 4 indicators loading by .5 each:
```{r eval=FALSE}
nIndicator <- c(1, 4)
loadM <- c(1, .5)

loadings <- list(
  c(1),
  c(.5, .5, .5, .5)
  )

Lambda <- matrix(c(
  c(1, 0),
  c(0, .5),
  c(0, .5),
  c(0, .5),
  c(0, .5)
  ), ncol = 2, byrow = TRUE)
```

##### Models including observed variables only {-}
The "factor" model also needs to be defined, when the model does not include any factor, but only contains observed variables. Consider a CLPM with two waves based on observed variables only, so there are 4 observed variables in total. Requesting an observed only model can be achieved by using either of the following:

* `Lambda = diag(4)`
* `loadings = as.list(rep(1, 4))`
* `nIndicator = rep(1, 4)` and `loadM = 1`

##### Ordering of factors {-}
For many (but not all) convenience functions, it is important to define the factors in the expected order. For instance, [`semPower.powerRegression`](#powerRegression) treats the first factor as criterion (*Y*) and the remaining factors as predictors (*X*). Thus, `nIndicator <- c(10, 5, 5)` says that the criterion is measured by 10 indicators, whereas both predictors are measured by 5 indicators. Using `nIndicator <- c(5, 10, 5)` instead would imply a criterion measured by 5 indicators. Details on the expected order of factors are provided in each specific convenience function.

##### Multiple group models {-}
The definition of the factor model in the case of multiple groups proceeds in the same way as described above, with the sole exception that the relevant arguments must be provided as a list, where each component corresponds to a specific group. For instance, below are two equivalent ways to define a two-factor model with 3 and 4 indicators, respectively, for two groups. In the first group, all loadings on the first and second factor are .5 and .6, respectively, in the second group all loadings on the first and second factor are .4 and .7, respectively.
```{r eval=FALSE}
# using the loadings argument
loadings <- list(
  # loadings for group 1
  list(
    c(.5, .5, .5),      # factor 1
    c(.6, .6, .6, .6)   # factor 2
  ),
  # loadings for group 2
  list(
    c(.4, .4, .4),      # factor 1
    c(.7, .7, .7, .7)   # factor 2
  )
)

# using nIndicator and loadM
nIndicator <- list(
  # nIndicators for group 1
  c(3, 4),
  # nIndicators for group 2
  c(3, 4)
)
loadM <- list(
  # loadings for group 1
  c(.5, .6),
  # loadings for group 1
  c(.4, .7)
)
```
Because the number of indicators by factor is usually assumed to be identical across groups, the list structure for `nIndicator` can also be omitted (e.g., `nIndicator = c(3, 4)`). Similarly, when assuming all loadings are equal as well, the list structure for `loadM` may be omitted as well, provided that at least one additional argument referring to the factor model (such as `Phi` , `Alpha`, or `tau`) is a list.


## Arguments common to all convenience functions {#commonArgs}

Whereas all convenience functions expect certain arguments (or values provided as arguments) that are unique to a specific function, a number of arguments are expected by all functions:

* The type of power analysis requested (`type`):
    + Use `type = 'a-priori'` (or `'ap'`) to request an [a-priori power analysis](#ap) and provide the alpha error (e. g., `alpha = .05`) and the desired beta error (e. g., `beta = .20`; or equivalently, the desired power, `power = .80`).  
    + Use `type = 'post-hoc'` (or `'ph'`) to request a [post-hoc power analysis](#ph) and provide the alpha error (e. g.,`alpha = .05`) and the number of observations (e. g.,`N = 250`).  
    + Use `type = 'compromise'` (or `'cp'`)  to request a [compromise hoc power analysis](#cp) and provide the desired ratio between alpha and beta error (e. g., `abratio = 1`) and the number of observations (e. g., `N = 250`). 
* Arguments [defining the factor model](#factorDefinition), one of:
    + `Lambda` to provide a loading matrix.
    + `loadings` to provide a list of vectors defining the primary factor loadings.
    + `nIndicator` and `loadM` to define the number of indicators by factor and a single loading to apply for all indicators of a specific factor.
* `comparison`: The relevant comparison model; one of `'saturated'`, or `'restricted'` (the default). See the [chapter on the definition of a comparison model](#comparisonModel) for details. 
* `nullEffect`: Defines the relevant hypothesis depending on the specific convenience function.
* `nullWhich`: Defines which parameters are targeted by the hypothesis specified in `nullEffect`.
* `nullWhichGroups`: Defines which groups are targeted when `nullEffect` refers to cross-group constrains.
* `simulatedPower`: Whether to perform a simulated (`TRUE`) rather than an analytical (`FALSE`; the default) power analysis. See the [chapter on simulated power](#simulatedPower) for details.

Thus, a typical call could look as follows (here taking a CFA model as an example):

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(
  # define type of power analysis
  type = 'a-priori', alpha = .05, beta = .20,
  # set comparison model
  comparison = 'restricted',
  # arguments (and values) specific to semPower.powerCFA
  Phi = .25,
  nullEffect = 'cor = 0',
  nullWhich = c(1, 2),
  # define factor model
  nIndicator = c(4, 3), loadM = c(.5, .6)
  )
```


## CFA models {#powerCFA}

`semPower.powerCFA` is used to perform power analyses to reject hypotheses arising in a standard CFA model involving several factors or factor(s) and one or more additional observed covariates. `semPower.powerCFA` provides interfaces to perform power analyses concerning the following hypotheses:

* whether a correlation differs from zero (`nullEffect = 'cor = 0'`). 
* whether two correlations differ from each other (`nullEffect = 'corX = corZ'`).
* whether a correlation differs across two or more groups (`nullEffect = 'corA = corB'`).

`semPower.powerCFA` only addresses hypotheses concerning correlation(s) involving one or more factors. `semPower` provides other convenience functions for hypothesis arising in [latent regression models](#powerRegression), models involving a [bifactor structure](#powerBifactor), [mediation models](#powerMediation), [generic path models](#powerPath), and [multigroup measurement invariance](#powerMI). For hypotheses regarding global model fit, a [model-free power analysis](#modelFreePower) should be performed. 

`semPower.powerCFA` expects the following arguments:

* `Phi`: Either a single number defining the correlation between exactly two factors or the factor correlation matrix. 
* `nullEffect`: Defines the hypothesis of interest; one of `'cor = 0'`, `'corX = corZ'`, or `'corA = corB'`.
* `nullWhich`: Defines which correlation(s) is targeted by the hypothesis defined in `nullEffect`.
* `nullWhichGroups`: Defines which groups are targeted when `nullEffect = 'corA = corB'`.
* additional arguments [specifying the type of power analysis](#commonArgs).
* additional arguments [defining the factor model](#factorDefinition). 

`semPower.powerCFA` provides a list as result, which contains the following components:

* `power`: The results of the power analysis, which contains the same information as the corresponding model-free counterpart (see [a-priori power analysis](#ap), [post-hoc power analysis](#ph), and [compromise hoc power analysis](#cp)). Use the `summary` method  to obtain formatted results.
* `Sigma` and `mu`: Variance-covariance matrix and means in the population.
* `SigmaHat` and `muHat`: Model implied variance-covariance matrix and means.
* `modelH0` and `modelH1`: `lavaan` model strings defining the H0 and the H1 model (only if `type = 'restricted'`).


##### Detect whether a correlation differs from zero {-}

To perform a power analysis to detect whether a correlation between factors differs from zero, use `nullEffect = 'cor = 0'` (which is also the default hypothesis and thus could be omitted). 

In the simplest case, the model contains exactly two factors, so only the to-be detected factor correlation needs to be specified (along the factor model itself). For instance, the following requests the required sample (`type = 'a-priori'`) to detect that a factor correlation of at least .25 (`Phi = .25`) differs from zero (`nullEffect = 'cor = 0'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). The first factors is measured by 4 indicators, the second factor by 3 indicators (`nIndicator = c(4, 3)`). All indicators of the first factor load by .5, whereas all indicators of the second factor load by .6 (`loadM = c(.5, .6)`). See the chapter on [specifying a factor model](#factorDefinition) for alternative (more flexible) ways to define the factor loadings.

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(
                              # define type of power analysis
                              type = 'a-priori', alpha = .05, power = .80,
                              # define hypothesis
                              Phi = .25,
                              nullEffect = 'cor = 0',
                              # define measurement model
                              nIndicator = c(4, 3), loadM = c(.5, .6))
summary(powerCFA$power)
```
The results of the power analysis are printed by calling the `summary` method on `powerCFA$power`, which in this example provides the same information as a model-free [a-priori power analysis](#ap) counterpart.

If a post-hoc power analysis is desired, the arguments related to the power analysis need to be adapted accordingly:
```{r eval=FALSE}
powerCFA <- semPower.powerCFA(
                              # define type of power analysis
                              type = 'post-hoc', alpha = .05, N = 300,
                              # define hypothesis
                              Phi = .25,
                              nullEffect = 'cor = 0',
                              # define measurement model
                              nIndicator = c(4, 3), loadM = c(.5, .6))
```
Now, `summary(powerCFA$power)` provides the same information as a model-free [post-hoc power analysis](#ph) counterpart. A  [compromise power analysis](#cp) (`type = 'compromise'`) is performed analogously.

In the examples above, a power analysis was performed by comparing the implied H0 model against a less restrictive H1 model (by omitting the `comparison` argument which defaults to `'restricted'`). If one rather wants to compare the H0 model against the saturated model, use `comparison = 'saturated'`. See the chapter on [the definition of the comparison model](#comparisonModel) for a detailed discussion. 

If one is interested in detecting the correlation between a factor and an observed covariate, the only change refers to the definition of the factor model. Below, `nIndicator = c(4, 1)` and `loadM = c(.5, 1)` define a factor with 4 indicators loadings by .5 each and another dummy factor with a single indicator loading by 1 (which is then simply an observed variable):

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80,
                              Phi = .25,
                              nullEffect = 'cor = 0',
                              nIndicator = c(4, 1), loadM = c(.5, 1))
```

Alternatively, `Phi` can also be a factor correlation matrix. The following defines three factors correlated according to `Phi` and uses the `nullWhich = c(1, 3)` argument to determine the required sample size to detect a correlation of at least .30 between the first and the third factor:

```{r eval=FALSE}

Phi <- matrix(c(
   c(1.00, 0.20, 0.30),
   c(0.20, 1.00, 0.10),
   c(0.30, 0.10, 1.00)
 ), ncol = 3,byrow = TRUE)

powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80,
                              Phi = Phi,
                              nullEffect = 'cor = 0',
                              nullWhich = c(1, 3),
                              nIndicator = c(3, 3, 3), loadM = c(.5, .7, .6))
```


##### Detect whether two correlations differ from each other {-}

To perform a power analysis to detect whether two correlations differ from each other, use `nullEffect = 'corX = corZ'`. 

For instance, the following requests the required sample (`type = 'a-priori'`) to detect that the correlation between the first and second factor (of .20) differs from the correlation between the first and the third factor (of .30; `nullEffect = 'corX = corZ'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`), where all factors are measured by 3 indicators (`nIndicator = c(3, 3, 3)`) and all non-zero loadings on the first, second, and third factor are equal to .5, .7, and .6 (`loadM = c(.5, .7, .6)`), respectively (see [Definition of the factor model](#factorDefinition)).

```{r eval=FALSE}

Phi <- matrix(c(
   c(1.00, 0.20, 0.30),
   c(0.20, 1.00, 0.10),
   c(0.30, 0.10, 1.00)
 ), ncol = 3,byrow = TRUE)

powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80,
                              Phi = Phi,
                              nullEffect = 'corX = corZ',
                              nullWhich = list(c(1, 2), c(1, 3)),
                              nIndicator = c(3, 3, 3), loadM = c(.5, .7, .6))
```

Note that `nullWhich` is now a list comprising two vectors, jointly defining which correlations to set to equality. `nullWhich = list(c(1, 2), c(1, 3))` says that the correlation between the first and the second factor (`c(1, 2)`) and the correlation between the first and the third (`c(1, 3)`) factor are restricted to be equal. 

`nullWhich` can also comprise more than two elements to test for the equality of more than two correlations. For instance, using `nullWhich = list(c(1, 2), c(1, 3), c(2, 3))` in the scenario above constrains all factor correlations to be equal.

As before, it is also possible to include observed covariates instead of latent factors by just defining a dummy factor with a single indicator loading by 1. For example, to replace the first factor in the example above by an observed variable (thus asking for the required N to detect that two factors correlate differently to an observed outcome), the factor model is changed by altering `nIndicator` and `loadM`: 

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80,
                              Phi = Phi,
                              nullEffect = 'corX = corZ',
                              nullWhich = list(c(1, 2), c(1, 3)),
                              nIndicator = c(1, 3, 3), loadM = c(1, .7, .6))
```


##### Detect whether a correlation differs across two or more groups {-}

To perform a power analysis to detect whether a correlation differs across two or more groups, use `nullEffect = 'corA = corB'`. 

For instance, the following requests the required sample (`type = 'a-priori'`) to detect that the correlation between two factors in group 1 (of .20) differs from the one in group 2 (of .40; `nullEffect = 'corA = corB'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). The measurement model is identical in both groups: Both factors are measured by 5 indicators each (`nIndicator = c(5, 5)`), and all non-zero loadings on the first and second factor are equal to .7 and .5 (`loadM = c(.7, .5)`), respectively, in both groups (see [Definition of the factor model](#factorDefinition)). `Phi = list(.2, .4)` is now a list comprising two elements, the first defining the correlation between the factors in the first group to be .2, the second defining the correlation between the factors in the second group to be .4. In addition, `N` must also be a list, which in case of an a-priori power analysis gives the group weights. `N = list(1, 1)` requests equally sized groups.

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                              nullEffect = 'corA = corB',
                              Phi = list(.2, .4), 
                              loadM = c(.7, .5), 
                              nIndicator = c(5, 5))
```

If using `N = list(2, 1)` instead, the first group would be twice as large as the second group. If a post-hoc or compromise power analysis is requested, `N` is a list providing the number of observations for each group. 

`Phi` can also be a list of factor correlation matrices (instead of a list of single numbers). For instance, the following defines different factor correlation matrices for two groups (`Phi1` and `Phi2`) with 300 and 400 observations (`N = list(300, 400)`), respectively, and requests the achieved power (`type = 'post-hoc'`) on alpha = .05 (`alpha = .05`) to detect that the correlation between factor 1 and 3 (`nullWhich = c(1, 3)`) differs across groups.

```{r eval=FALSE}
Phi1 <- matrix(c(
    c(1.00, 0.20, 0.50),
    c(0.20, 1.00, 0.10),
    c(0.50, 0.10, 1.00)
 ), ncol = 3,byrow = TRUE)
Phi2 <- matrix(c(
    c(1.00, 0.20, 0.30),
    c(0.20, 1.00, 0.10),
    c(0.30, 0.10, 1.00)
 ), ncol = 3,byrow = TRUE)

powerCFA <- semPower.powerCFA(type = 'post-hoc', alpha = .05, N = list(300, 400),
                              Phi = list(Phi1, Phi2),
                              nullEffect = 'corA = corB',
                              nullWhich = c(1, 3),
                              nIndicator = c(3, 3, 3), loadM = c(.5, .5, .5))
```

If there are more than two groups, the targeted correlation is held equal across all groups by default. If the correlation should only be constrained to equality in specific groups, `nullWhichGroups` is used to identify the groups to which the equality restrictions apply. For instance, the following defines three equally sized groups with a distinct correlation between the two factors, but only asks for the required sample to detect that the correlation in group 1 (of .2) differs from the one in group 3 (of .3; `nullWhichGroups = c(1, 3)`). 

```{r eval=FALSE}
powerCFA <- semPower.powerCFA(type = 'a-priori', alpha = .05, power = .80, N = list(1, 1, 1),
                              nullEffect = 'corA = corB',
                              Phi = list(.2, .4, .3), 
                              nullWhichGroups = c(1, 3),
                              loadM = c(.7, .5), 
                              nIndicator = c(5, 5))
```

## Models involving a bifactor structure {#powerBifactor}

`semPower.powerBifactor` is used to perform power analyses to reject correlational hypotheses arising in a model involving one bifactor and at least one additional variable, which can also be a bifactor, a standard factor, or an observed covariate. `semPower.powerBifactor` provides interfaces to perform power analyses concerning the following hypotheses:

* whether a correlation differs from zero (`nullEffect = 'cor = 0'`). 
* whether two correlations differ from each other (`nullEffect = 'corX = corZ'`).
* whether a correlation differs across two or more groups (`nullEffect = 'corA = corB'`).

`semPower.powerBifactor` only addresses hypotheses concerning correlation(s) involving one or more bifactors. `semPower` provides other convenience functions for hypothesis arising in [standard CFA models](#powerCFA). For hypotheses regarding global model fit, a [model-free power analysis](#modelFreePower) should be performed. 

`semPower.powerBifactor` expects the following arguments:

* `bfLoadings`: A single vector or a list containing one or more vectors giving the loadings on each bifactor. 
* `bfWhichFactors`: A list containing one or more vectors defining which (specific) factors are part of the bifactor structure.
* `Phi`: Either a single number defining the correlation between exactly two factors or the factor correlation matrix. Must only contain the bifactor(s) and covariate(s), but not any specific factor. `Phi` assumes the following order (bifactor_1, bifactor_2, ..., bifactor_j, covariate_1,  covariate_2, ...,  covariate_k).
* `nullEffect`: Defines the hypothesis of interest; one of `'cor = 0'`, `'corX = corZ'`, or `'corA = corB'`.
* `nullWhich`: Defines which correlation(s) is targeted by the hypothesis defined in `nullEffect`.
* `nullWhichGroups`: Defines which groups are targeted when `nullEffect = 'corA = corB'`.
* additional arguments [specifying the type of power analysis](#commonArgs).
* additional arguments [defining the factor model](#factorDefinition), which may only refer to specific factors (that are part of the bifactor) and additional covariate(s).

`semPower.powerBifactor` provides a list as result, which contains the following components:

* `power`: The results of the power analysis, which contains the same information as the corresponding model-free counterpart (see [a-priori power analysis](#ap), [post-hoc power analysis](#ph), and [compromise hoc power analysis](#cp)). Use the `summary` method  to obtain formatted results.
* `Sigma` and `mu`: Variance-covariance matrix and means in the population.
* `SigmaHat` and `muHat`: Model implied variance-covariance matrix and means.
* `modelH0` and `modelH1`: `lavaan` model strings defining the H0 and the H1 model (only if `type = 'restricted'`).


##### Detect whether a correlation differs from zero {-}

To perform a power analysis to detect whether a correlation between factors differs from zero, use `nullEffect = 'cor = 0'` (which is also the default hypothesis and thus could be omitted). 

to be written


In the examples above, a power analysis was performed by comparing the implied H0 model against a less restrictive H1 model (by omitting the `comparison` argument which defaults to `'restricted'`). If one rather wants to compare the H0 model against the saturated model, use `comparison = 'saturated'`. See the chapter on [the definition of the comparison model](#comparisonModel) for a detailed discussion. 


##### Detect whether two correlations differ from each other {-}

To perform a power analysis to detect whether two correlations differ from each other, use `nullEffect = 'corX = corZ'`. 

to be written

##### Detect whether a correlation differs across two or more groups {-}

To perform a power analysis to detect whether a correlation differs across two or more groups, use `nullEffect = 'corA = corB'`. 

to be written


## Latent regression models {#powerRegression}
`semPower.powerRegression` is used to perform power analyses for SEM models involving a simple linear regression relation of the form $\hat{Y} = \beta_1 \cdot X_1 + ... + \beta_k \cdot X_k$, where $Y$ and $X_i$ can be factors or observed variables. `semPower.powerRegression`provides interfaces to perform power analyses concerning the following hypotheses:

* whether a slope ($\beta_i$) differs from zero (`nullEffect = 'slope = 0'`). 
* whether two slopes ($\beta_i$, $\beta_j$) differ from each other (`nullEffect = 'slopeX = slopeZ'`).
* whether a slope ($\beta_{i,m}$, $\beta_{i,n}$) differs across two or more groups (`nullEffect = 'slopeA = slopeB'`).

`semPower.powerRegression` only addresses hypotheses concerning slope(s) in a latent regression. `semPower` provides other convenience functions for hypothesis arising in [mediation models](#powerMediation), [generic path models](#powerPath), and [cross-lagged panel models](#powerCLPM). For hypotheses regarding global model fit, a [model-free power analysis](#modelFreePower) should be performed. 

`semPower.powerRegression` expects the following arguments:

* `slopes`: Vector of slopes (or a single number for a single slope) for the predictors $X_1$ to $X_k$ in the prediction of $Y$. 
* `corXX`: Correlation(s) (or covariances) between the predictors. Either a single number defining the correlation between exactly two predictors, or a $k \cdot k$ correlation matrix, or `NULL` for uncorrelated predictors (the default). 
* `nullEffect`: Defines the hypothesis of interest; one of `'slope = 0'`, `'slopeX = slopeZ'`, or `'slopeA = slopeB'`.
* `nullWhich`: Defines which slope(s) is targeted by the hypothesis defined in `nullEffect`.
* `nullWhichGroups`: Defines which groups are targeted when `nullEffect = 'slopeA = slopeB'`.
* `standardized`: Whether the arguments provided to `slopes` and `corXX` are standardized (`TRUE`, the default) or unstandardized (`FALSE`).
* additional arguments [specifying the type of power analysis](#commonArgs). 
* additional arguments [defining the factor model](#factorDefinition), where the first factor represents the criterion $Y$ and the remaining factors the predictors $X_1$ to $X_k$. 

`semPower.powerRegression` provides a list as result, which contains the following components:

* `power`: The results of the power analysis, which contains the same information as the corresponding model-free counterpart (see [a-priori power analysis](#ap), [post-hoc power analysis](#ph), and [compromise hoc power analysis](#cp)). Use the `summary` method  to obtain formatted results.
* `Sigma` and `mu`: Variance-covariance matrix and means in the population.
* `SigmaHat` and `muHat`: Model implied variance-covariance matrix and means.
* `modelH0` and `modelH1`: `lavaan` model strings defining the H0 and the H1 model (only if `type = 'restricted'`).


##### Detect whether a slope differs from zero {-}
To perform a power analysis to detect whether a slope differs from zero, use `nullEffect = 'slope = 0'` (which is also the default and thus could be omitted). 

For instance, the following sets up three factors measured by 3, 4, and 5 indicators (`nIndicator = c(3, 4, 5)`). All indicators of the first factor load by .5, all indicators of the second factor by .6, and all of the third factor by .7 (`loadM = c(.5, .6, .7)`). See the chapter on [specifying a factor model](#factorDefinition) for alternative (more flexible) ways to define the factor loadings. `semPower.powerRegression` treats the first defined factor as criterion ($Y$) and the remaining factors as predictors (here: $X_1$ and $X_2$), so in the present example the criterion is measured by 3 indicators loading by .5 each. The slopes of $X_1$ and $X_2$ in the prediction of $Y$ are defined to be .2 and .3, respectively (`slopes = c(.2, .3)`) and the correlation between the predictors is defined to be .4 (`corXX = .4`). Finally, the the required sample (`type = 'a-priori'`) is requested to detect that the first slope (`nullWhich = 1`) differs from zero (`nullEffect = 'slope = 0'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). 
```{r eval=FALSE}
powerReg <- semPower.powerRegression(
                                     # define type of power analysis
                                     type = 'a-priori', alpha = .05, power = .80,
                                     # define hypothesis
                                     slopes = c(.2, .3), 
                                     corXX = .4, 
                                     nullEffect = 'slope = 0',
                                     nullWhich = 1,
                                     # define measurement model
                                     nIndicator = c(3, 5, 4), 
                                     loadM = c(.5, .6, .7))
summary(powerReg$power)
```
The results of the power analysis are printed by calling the `summary` method on `powerReg$power`, which in this example provides the same information as a model-free [a-priori power analysis](#ap) counterpart.

If a post-hoc power analysis is desired, the arguments related to the power analysis need to be adapted accordingly:
```{r eval=FALSE}
powerReg <- semPower.powerRegression(
                                     # define type of power analysis
                                     type = 'a-priori', alpha = .05, power = .80,
                                     # define hypothesis
                                     slopes = c(.2, .3), 
                                     corXX = .4, 
                                     nullEffect = 'slope = 0',
                                     nullWhich = 1,
                                     # define measurement model
                                     nIndicator = c(3, 5, 4), 
                                     loadM = c(.5, .6, .7))
```
Now, `summary(powerReg$power)` provides the same information as a model-free [post-hoc power analysis](#ph) counterpart. A  [compromise power analysis](#cp) (`type = 'compromise'`) is performed analogously.

In the examples above, a power analysis was performed by comparing the implied H0 model against a less restrictive H1 model (by omitting the `comparison` argument which defaults to `'restricted'`). If one rather wants to compare the H0 model against the saturated model, use `comparison = 'saturated'`. See the chapter on [the definition of the comparison model](#comparisonModel) for a detailed discussion. 

Also, all slopes were treated as completely standardized parameters (by omitting the `standardized` argument, which defaults to `TRUE`). This implies that `semPower` defines the residual variances (in $\Psi$) such that all variances are 1. If the slopes should rather be treated as unstandardized, set `standardized = FALSE`, which implies a diagonal matrix for $\Psi$. 

If there are more than two predictors, the predictor intercorrelation matrix must be provided as argument to `corXX`. For instance, consider the regression $\hat{Y} = .3 \cdot X_1 + .2 \cdot X_2 + .1 \cdot X_3$, where all factors are measured by 3 indicators, all loadings equal .5, the interest lies in detecting that the slope for $X_2$ differs from zero (`nullWhich = 2`), and the predictors are correlated according to `corXX`:

```{r eval=FALSE}
corXX <- matrix(c(
 #   X1    X2    X3
 c(1.00, 0.20, 0.30),  # X1
 c(0.20, 1.00, 0.10),  # X2
 c(0.30, 0.10, 1.00)   # X3
), ncol = 3, byrow = TRUE)
powerReg <- semPower.powerRegression(
                                     # define type of power analysis
                                     type = 'a-priori', alpha = .05, power = .80,
                                     # define hypothesis
                                     slopes = c(.3, .2, .1), 
                                     corXX = corXX, 
                                     nullEffect = 'slope = 0',
                                     nullWhich = 2,
                                     # define measurement model
                                     nIndicator = c(3, 3, 3, 3), 
                                     loadM = c(.5, .5, .5, .5))
```
If `corXX` is omitted or `NULL`, all predictors are assumed to be uncorrelated.

If the criterion or (one of) the predictors is an observed variable (instead of a factor), the only change refers to the [definition of the factor model](#factorDefinition). For instance, the following defines the first "factor", which is always treated as the criterion $Y$, as a dummy factor with a single indicator (`nIndicator = c(1, 5, 4)`) and a loading of 1 (`loadM = c(1, .6, .7)`), so it becomes equivalent to an observed variable:

```{r eval=FALSE}
powerReg <- semPower.powerRegression(
                                     # define type of power analysis
                                     type = 'a-priori', alpha = .05, power = .80,
                                     # define hypothesis
                                     slopes = c(.2, .3), 
                                     corXX = .4, 
                                     nullWhich = 1,
                                     # define measurement model
                                     nIndicator = c(1, 5, 4), 
                                     loadM = c(1, .6, .7))
```
Similarly, using `nIndicator = c(6, 1, 4)` and `loadM = c(.5, 1, .7)` would make the first predictor ($X_1$) an observed variable, and a regression involving observed variables only could be defined using `nIndicator = c(1, 1, 1)` and `loadM = c(1, 1, 1)` or, more simply, by just providing `Lambda = diag(3)` instead of `nIndicator` and `loadM`. A MIMIC model with a factor measured by 5 indicators all loading by .7 would thus correspond to `nIndicator = c(5, 1, 1)` and `loadM = c(.7, 1, 1)`.


##### Detect whether two slopes differ from each other {-}
To perform a power analysis to detect whether two correlations differ from each other, use `nullEffect = 'slopeX = slopeZ'`.

For instance, the following sets up three factors measured by 3, 4, and 5 indicators (`nIndicator = c(3, 4, 5)`). All indicators of the first factor load by .5, all indicators of the second factor by .6, and all of the third factor by .7 (`loadM = c(.5, .6, .7)`; see [definition of a factor model](#factorDefinition)). Recall that the first factor is treated as criterion ($Y$). Having defined the criterion and the predictors, the slopes of $X_1$ and $X_2$ in the prediction of $Y$ are defined to be .1 and .4, respectively (`slopes = c(.1, .4)`) and the correlation between the predictors is defined to be .25 (`corXX = .25`). Finally, the the required sample (`type = 'a-priori'`, `nullWhich = c(1, 2)`) is requested to detect that the first slope differs from the second slope (`nullEffect = 'slopeX = slopeZ'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). 

```{r eval=FALSE}
powerReg <- semPower.powerRegression(
                                     # define type of power analysis
                                     type = 'a-priori', alpha = .05, power = .80,
                                     # define hypothesis
                                     slopes = c(.1, .4), 
                                     corXX = .25, 
                                     nullEffect = 'slopeX = slopeZ',
                                     nullWhich = c(1, 2),
                                     # define measurement model
                                     nIndicator = c(3, 5, 4), 
                                     loadM = c(.5, .6, .7))
```

Note that `nullWhich` is now a vector defining which slopes should be set to equality. `nullWhich = c(1, 2)` says that the first and the second slope shall be equal. 

`nullWhich` can also comprise more than two elements to test for the equality of more than two slopes For instance, when there are 4 predictors, using `nullWhich = c(1, 2, 4)` would constrain the first, second, and fourth (but not the third) slope to equality.

##### Detect whether a slope differs across two or more groups {-}

To perform a power analysis to detect whether a slope differs across two or more groups, use `nullEffect = 'slopeA = slopeB'`. 

For instance, the following sets up four factors measured by 4, 5, 3, and 6 indicators (`nIndicator = c(4, 5, 3, 6)`). All indicators of the first factor load by .5, all indicators of the second factor by .6, all of the third factor by .7, and all of the fourth factor by .6 (`loadM = c(.5, .6, .7, .6)`; see [definition of a factor model](#factorDefinition)). Recall that the first factor is treated as criterion ($Y$), and the remaining factors as predictors. The predictors are correlated according to the matrix defined in `corXX`. Note that this part of the factor model is identical across groups, which is indeed a prerequesite to allow for meaningful cross-group comparisons of slopes. However, seperate regression relationships are defined for each group by using a list structure as value to the `slopes` argument: In the first group, the regression equation is $\hat{Y} = .2 \cdot X_1 + .3 \cdot X_2 + .4 \cdot X_3$ (`c(.2, .3, .4)`), whereas in the second group it is $\hat{Y} = .2 \cdot X_1 + .05 \cdot X_2 + .4 \cdot X_3$ (`c(.2, .05, .4)`). Finally, the the required sample (`type = 'a-priori'`) is requested to detect that the second slope ($\beta_2$, `nullWhich = 2`) differs across groups (`nullEffect = 'slopeA = slopeB'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). Furthermore, in multiple group models the `N` argument also needs to be provided as a list, which in case of an a-priori power analysis gives the group weights. `N = list(1, 1)` requests equally sized groups. If using `N = list(2, 1)` instead, the first group would be twice as large as the second group. If a post-hoc or compromise power analysis is requested, `N` is a list providing the number of observations for each group. 

```{r eval=FALSE}
corXX <- matrix(c(
  #   X1    X2    X3
  c(1.00, 0.20, 0.30),  # X1
  c(0.20, 1.00, 0.10),  # X2
  c(0.30, 0.10, 1.00)   # X3
), ncol = 3,byrow = TRUE)
powerReg <- semPower.powerRegression(
                                     # define type of power analysis
                                     type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                                     # define hypothesis
                                     slopes = list(
                                       # slopes in group 1 
                                       c(.2, .3, .4), 
                                       # slopes in group 2 
                                       c(.2, .05, .4)
                                       ),
                                     corXX = corXX,
                                     nullEffect = 'slopeA = slopeB',
                                     nullWhich = 2,
                                     # define measurement model
                                     nIndicator = c(4, 5, 3, 6),
                                     loadM = c(.5, .6, .7, .6)
                                     )
```


If there are more than two groups, the targeted slope is held equal across all groups by default. If the slope should only be constrained to equality in specific groups, `nullWhichGroups` is used to identify the groups to which the equality restrictions apply. For instance, the following defines three equally sized groups with a distinct slope for $X_2$, but only asks for the required sample to detect that the second slope (`nullWhich = 2`) in group 1 (of .3) differs from the second slope in group 3 (of .45; `nullWhichGroups = c(1, 3)`). 

```{r eval=FALSE}
powerReg <- semPower.powerRegression(
                                     # define type of power analysis
                                     type = 'a-priori', alpha = .05, power = .80, N = list(1, 1, 1),
                                     # define hypothesis
                                     slopes = list(
                                       # slopes in group 1 
                                       c(.2, .3, .4), 
                                       # slopes in group 2 
                                       c(.2, .05, .4),
                                       # slopes in group 3 
                                       c(.2, .45, .4)
                                       ),
                                     corXX = NULL,  # independent predictors
                                     nullEffect = 'slopeA = slopeB',
                                     nullWhich = 2,
                                     nullWhichGroups = c(1, 3),
                                     # define measurement model
                                     nIndicator = c(4, 5, 3, 6),
                                     loadM = c(.5, .6, .7, .6)
                                     )
```


## Mediation models  {#powerMediation}

`semPower.powerMediation` is used to perform power analyses to reject hypotheses arising in a mediation context involving factors and/or observed variables. This includes the simple case or a single variable $M$ mediating the relation between $X$ and $Y$ (`X -> M -> Y`), but may also refer to more complex mediation chains involving several mediators. Note that power for mediation effects involving latent variables is only approximated. `semPower.powerMediation` provides interfaces to perform power analyses concerning the following hypotheses:

* whether an indirect effect differs from zero (`nullEffect = 'ind = 0'`). 
* whether an indirect effect differs across two or more groups (`nullEffect = 'indA = indB'`). This is currently only possible for models without latent variables.

`semPower.powerMediation` only addresses hypotheses in a mediation context. `semPower` provides other convenience functions for hypothesis arising in [latent regression models](#powerRegression), [generic path models](#powerPath), and [cross-lagged panel models](#powerCLPM). For hypotheses regarding global model fit, a [model-free power analysis](#modelFreePower) should be performed. 

`semPower.powerMediation` expects the following arguments:

* Either (assuming a simple mediation of the form `X -> M -> Y`):
    + `bYX`: the slope for $X$ in the prediction of $Y$ (`X -> Y`).
    +  `bMX`: the slope for $X$ in the prediction of $M$ (`X -> M`).
    +  `bYM`: the slope for $M$ in the prediction of $Y$ (`M -> Y`).
* or (for more complex mediation mechanisms):
    + `Beta`: Matrix of regression weights connecting the latent factors (all-Y notation). Exogenous variables must be in the first row(s), so the upper triangular of Beta must be zero. See [this chapter](semDefinition) for details.
    + `indirect`: A list of vectors of size 2 indicating the elements of `Beta` that define the indirect effect.
* `nullEffect`: Defines the hypothesis of interest; one of `'ind = 0'` or `'indA = indB'`.
* `nullWhichGroups`: Defines which groups are targeted when `nullEffect = 'indA = indB'`.
* `standardized`: Defines whether all parameters are standardized (`TRUE`, the default) or unstandardized (`FALSE`).
* additional arguments [specifying the type of power analysis](#commonArgs).
* additional arguments [defining the factor model](#factorDefinition), assuming the order $X$, $M$, $Y$ in case of a simple mediation or the order given in `Beta`.  

`semPower.powerMediation` provides a list as result, which contains the following components:

* `power`: The results of the power analysis, which contains the same information as the corresponding model-free counterpart (see [a-priori power analysis](#ap), [post-hoc power analysis](#ph), and [compromise hoc power analysis](#cp)). Use the `summary` method  to obtain formatted results.
* `Sigma` and `mu`: Variance-covariance matrix and means in the population.
* `SigmaHat` and `muHat`: Model implied variance-covariance matrix and means.
* `modelH0` and `modelH1`: `lavaan` model strings defining the H0 and the H1 model (only if `type = 'restricted'`).


##### Detect whether an indirect effect differs from zero {-}
To perform a power analysis to detect whether a mediation effect (= an indirect effect) differs from zero, use `nullEffect = 'ind = 0'`, which is also the default and thus could be omitted. 

In the simple case of a mediation of the form `X -> M -> Y`, the relevant arguments specifying the size of the slopes and thus the magnitude of the mediation effect are `bYX`, `bMX`, and `bYM`. For instance, the following sets up three factors measured by 3, 4, and 5 indicators (`nIndicator = c(3, 4, 5)`). All indicators of the first factor load by .5, all indicators of the second factor by .6, and all of the third factor by .7 (`loadM = c(.5, .6, .7)`). See the chapter on [specifying a factor model](#factorDefinition) for alternative (more flexible) ways to define the factor loadings. `semPower.powerMediation` treats the first factor as predictor $X$, the second factor as mediator $M$, and the third factor as criterion $Y$, so in the present example the mediator is measured by 4 indicators loading by .6 each. The slopes for the relations `X -> Y`, `X -> M`, `M -> Y`are defined to be .25 (`bYX = .25`), .3  (`bMX = .3`), and .4 (`bYM = .4`), respectively. Finally, the the required sample (`type = 'a-priori'`) is requested to detect that the indirect effect differs from zero  (`nullEffect = 'ind = 0'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). 
```{r eval=FALSE}
powerMed <- semPower.powerMediation(
                                     # define type of power analysis
                                     type = 'a-priori', alpha = .05, power = .80,
                                     # define hypothesis
                                     bYX = .25, 
                                     bMX = .3, 
                                     bYM = .4,
                                     nullEffect = 'ind = 0',
                                     # define measurement model
                                     nIndicator = c(3, 4, 5),
                                     loadM = c(.5, .6, .7)
                                     )
summary(powerMed$power)
```
The results of the power analysis are printed by calling the `summary` method on `powerMed$power`, which in this example provides the same information as a model-free [a-priori power analysis](#ap) counterpart.

If a post-hoc power analysis is desired, the arguments related to the power analysis need to be adapted accordingly:
```{r eval=FALSE}
powerMed <- semPower.powerMediation(
                                     # define type of power analysis
                                     type = 'post-hoc', alpha = .05, N = 300,
                                     # define hypothesis
                                     bYX = .25, 
                                     bMX = .3, 
                                     bYM = .4,
                                     nullEffect = 'ind = 0',
                                     # define measurement model
                                     nIndicator = c(3, 4, 5),
                                     loadM = c(.5, .6, .7)
                                     )
```
Now, `summary(powerMed$power)` provides the same information as a model-free [post-hoc power analysis](#ph) counterpart. A  [compromise power analysis](#cp) (`type = 'compromise'`) is performed analogously.

In the examples above, a power analysis was performed by comparing the implied H0 model against a less restrictive H1 model (by omitting the `comparison` argument which defaults to `'restricted'`). If one rather wants to compare the H0 model against the saturated model, use `comparison = 'saturated'`. See the chapter on [the definition of the comparison model](#comparisonModel) for a detailed discussion. 

Also, all slopes were treated as completely standardized parameters (by omitting the `standardized` argument, which defaults to `TRUE`). This implies that `semPower` defines the residual variances (in $\Psi$) such that all variances are 1. If the slopes should rather be treated as unstandardized, set `standardized = FALSE`, which implies a diagonal matrix for $\Psi$. 

If one or all of the involved variables are observed variables (instead of factors), the only change refers to the [definition of the factor model](#factorDefinition). For instance, the following defines the first "factor", which is treated as the predictor $X$, as a dummy factor with a single indicator (`nIndicator = c(1, 4, 5)`) and a loading of 1 (`loadM = c(1, .6, .7)`), so it becomes equivalent to an observed variable:
```{r eval=FALSE}
powerMed <- semPower.powerMediation(
                                     # define type of power analysis
                                     type = 'a-priori', alpha = .05, power = .80,
                                     # define hypothesis
                                     bYX = .25, 
                                     bMX = .3, 
                                     bYM = .4,
                                     nullEffect = 'ind = 0',
                                     # define measurement model
                                     nIndicator = c(1, 4, 5),
                                     loadM = c(1, .6, .7)
                                     )
```
Similarly, using `nIndicator = c(3, 1, 5)` and `loadM = c(.5, 1, .7)` would make the mediator  ($M$) an observed variable, and a mediation model involving observed variables only could be defined using `nIndicator = c(1, 1, 1)` and `loadM = c(1, 1, 1)` or, more simply, by just providing `Lambda = diag(3)` instead of `nIndicator` and `loadM`.

`semPower.powerMediation` provides an alternative way to specify mediation structures that go beyond the simple case of a `X -> M -> Y` mediation. For illustration, suppose there are four variables, the hypothesized structure is `X -> M1 -> M2 -> Y`, and that the indirect effect of interest is given by the slopes connecting `X -> M1`, `M1 -> M2`, and `M2 -> Y`. To reflect this type of mediation, the `Beta` and `indirect` arguments need to be set. Below, the regression relations between the factors are defined in `Beta` (see [this chapter](semDefinition) for details), implying a slope for `X -> M1` of .2, for `M1 -> M2` of .3, and for `M2 -> Y` of .4 (and all other slopes being equal to zero). `indirect` is a list of vectors of size two indicating the elements of `Beta` that define the indirect effect, so in this example, the indirect effect of interest comprise `X -> M1` (`c(2, 1)`),  `M1 -> M2` (`c(3, 2)`),  `Y -> M2` (`c(4, 3)`). Further, the assumed order of the factors mirrors the order given in `Beta`, so $X$ and $Y$ are defined as factors involving 5 indicators that all load by .5, and $M_1$ and $M_2$ as factors that are indicated by 6 indicators loading by .6 (`nIndicator = c(5, 6, 6, 5)` and `loadM = c(.5, .6, .6, .5)`).   

```{r eval=FALSE}
Beta <- matrix(c(
  c(.00, .00, .00, .00),       # X  = .00*X + .00*M1 + .00*M1 + .00*Y 
  c(.20, .00, .00, .00),       # M1 = .20*X + .00*M1 + .00*M1 + .00*Y 
  c(.00, .30, .00, .00),       # M2 = .00*X + .30*M1 + .00*M1 + .00*Y 
  c(.00, .00, .40, .00)        # Y  = .00*X + .00*M1 + .40*M1 + .00*Y 
), byrow = TRUE, ncol = 4)
powerMed <- semPower.powerMediation(
                                     # define type of power analysis
                                     type = 'a-priori', alpha = .05, power = .80,
                                     # define hypothesis
                                     Beta = Beta, 
                                     indirect = list(c(2, 1), c(3, 2), c(4, 3)),
                                     nullEffect = 'ind = 0',
                                     # define measurement model
                                     nIndicator = c(5, 6, 6, 5),
                                     loadM = c(.5, .6, .6, .5)
                                     )
```



##### Detect whether an indirect differs across groups {-}
To perform a power analysis to detect whether a mediation effect (= an indirect effect) differs across groups, use `nullEffect = 'indA = indB'`. This is currently only possible for models involving observed variables only.

For instance, the following requests the required sample (`type = 'a-priori'`) to detect that the indirect effect in group 1 differs from the one in group 2 (`nullEffect = 'indA = indB'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). `bYX`, `bMX`, and `bYM` are now lists comprising two elements, the first defining the respective slope in the first group, the second the respective slope in the second group. Thus, the slopes in the first and second group for `X -> M` are .2 and .4, for `M -> Y` .3 and .5, and for `X -> M` .25 and .25, respectively. `Lambda = diag(3)` implies that all variables are observed. Furthermore, in multiple group models the `N` argument also needs to be provided as a list, which in case of an a-priori power analysis gives the group weights. `N = list(1, 1)` requests equally sized groups. If using `N = list(2, 1)` instead, the first group would be twice as large as the second group. If a post-hoc or compromise power analysis is requested, `N` is a list providing the number of observations for each group. 

```{r eval=FALSE}
powerMed <- semPower.powerMediation(type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                                    nullEffect = 'indA = indB',
                                    bYX = list(.25, .25),
                                    bMX = list(.2, .4),
                                    bYM = list(.3, .5),
                                    Lambda = diag(3)
                                   )
```

The same as above can also be achieved using the `Beta` and `indirect` arguments, which generally offer greater flexibility. Key is to provide `Beta` as a list, where each component reflects the regression relationships for each group. 

```{r eval=FALSE}
# Beta for group 1
Beta1 <- matrix(c(
  c(.00, .00, .00),    # X = .00*X + .00*M + .00*Y 
  c(.20, .00, .00),    # M = .20*X + .00*M + .00*Y 
  c(.25, .30, .00)     # Y = .25*X + .30*M + .00*Y 
), byrow = TRUE, ncol = 3)
# Beta for group 2
Beta2 <- matrix(c(
  c(.00, .00, .00),    # X = .00*X + .00*M + .00*Y 
  c(.40, .00, .00),    # M = .40*X + .00*M + .00*Y 
  c(.25, .50, .00)     # Y = .25*X + .50*M + .00*Y 
), byrow = TRUE, ncol = 3)
powerMed <- semPower.powerMediation(type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                                    nullEffect = 'indA = indB',
                                    Beta = list(Beta1, Beta2),
                                    indirect = list(c(2, 1), c(3, 2)),
                                    Lambda = diag(3)
                                    )
```


## Generic path models {#powerPath}
`semPower.powerPath` is used to perform power analyses to reject hypotheses arising in a generic structural equation model specifying regression relations between the factors or between factors and observed covariates via the `Beta` and `Psi` matrices (see [this chapter](semDefinition) for details). `semPower.powerPath` provides interfaces to perform power analyses concerning the following hypotheses:

* whether a slope differs from zero (`nullEffect = 'beta = 0'`). 
* whether two slopes differ from each other (`nullEffect = 'betaX = betaZ'`).
* whether a slope differs across two or more groups (`nullEffect = 'betaA = betaB'`).

`semPower.powerPath` offers a generic and flexible way to address hypotheses involving regression relationships, so that power analyses can be performed for hypothesis not covered by a more specific convenience function, such as [`semPower.powerRegression`](#powerRegression), [`semPower.powerMediation`](#powerMediation), and [`semPower.powerCLPM`](#powerCLPM).

`semPower.powerPath` expects the following arguments:

* `Beta`: Matrix of regression slopes (all-Y notation); see [this chapter](semDefinition) for examples.
* `Psi`: Variance-covariance matrix of latent (residual) factors or `NULL` when all covariances shall be zero. 
* `nullEffect`: Defines the hypothesis of interest; one of `'beta = 0'`, `'betaX = betaZ'`, or `'betaA = betaB'`.
* `nullWhich`: Defines which slope(s) is targeted by the hypothesis defined in `nullEffect`.
* `nullWhichGroups`: Defines which groups are targeted when `nullEffect = 'betaA = betaB'`.
* additional arguments [specifying the type of power analysis](#commonArgs).
* additional arguments [defining the factor model](#factorDefinition) ordered as implied by `Beta`. 

`semPower.powerPath` provides a list as result, which contains the following components:

* `power`: The results of the power analysis, which contains the same information as the corresponding model-free counterpart (see [a-priori power analysis](#ap), [post-hoc power analysis](#ph), and [compromise hoc power analysis](#cp)). Use the `summary` method  to obtain formatted results.
* `Sigma` and `mu`: Variance-covariance matrix and means in the population.
* `SigmaHat` and `muHat`: Model implied variance-covariance matrix and means.
* `modelH0` and `modelH1`: `lavaan` model strings defining the H0 and the H1 model (only if `type = 'restricted'`).


##### Detect whether a slope differs from zero {-}
To perform a power analysis to detect whether a slope differs from zero, use `nullEffect = 'beta = 0'` (which is also the default and thus could be omitted). 

`semPower.powerRegression` requires to specify the regression relations between the factors via the `Beta` argument ([see this chapter for details](semDefinition)). For instance, in the example below, there are four factors (equal to number of columns/rows of `Beta`). The structure of `Beta` implies the relations $F_2 = .2 \cdot F_1$, $F_3 = .3 \cdot F_2$, and $F_4 = .1 \cdot F_1 + .4 \cdot F_3$. `nullWhich` is a vector of size two indicating the element of `Beta` (i.e. the specific slope) that is targeted by the null hypothesis. Below, the required sample (`type = 'a-priori'`) is requested to detect that the slope of $F_1$ in the prediction of $F_4$ (`nullWhich = c(4, 1)`) differs from zero (`nullEffect = 'beta = 0'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). Finally, the measurement model for the factors needs to be defined. The order of factors matches the order in `Beta`: The first factors is measured by 3 indicators, the second factor by 4 indicators, the third by 5, and the fourth by 6 indicators (`nIndicator = c(3, 4, 5, 6)`). The respective indicators of the first factor load by .7, on the second by .5, on the third by .6, and on the fourth factor by .8  (`loadM = c(.7, .5, .6, .8)`). See the chapter on [specifying a factor model](#factorDefinition) for alternative (more flexible) ways to define the factor loadings.


```{r eval=FALSE}
Beta <- matrix(c(
  c(.00, .00, .00, .00),       # f1 = .00*f1 + .00*f2 + .00*f3 + .00*f4
  c(.20, .00, .00, .00),       # f2 = .20*f1 + .00*f2 + .00*f3 + .00*f4
  c(.00, .30, .00, .00),       # f3 = .00*f1 + .30*f2 + .00*f3 + .00*f4
  c(.10, .00, .40, .00)        # f4 = .10*f1 + .00*f2 + .40*f3 + .00*f4
), byrow = TRUE, ncol = 4)
powerPath <- semPower.powerPath(
                                # define type of power analysis
                                type = 'a-priori', alpha = .05, power = .80,
                                # define hypothesis
                                Beta = Beta,
                                nullWhich = c(4, 1),
                                # define measurement model
                                nIndicator = c(3, 4, 5, 6),
                                loadM = c(.7, .5, .6, .8),
                                )
summary(powerPath$power)
```
The results of the power analysis are printed by calling the `summary` method on `powerPath$power`, which in this example provides the same information as a model-free [a-priori power analysis](#ap) counterpart.

If a post-hoc power analysis is desired, the arguments related to the power analysis need to be adapted accordingly:
```{r eval=FALSE}
powerPath <- semPower.powerPath(
                                # define type of power analysis
                                type = 'post-hoc', alpha = .05, N = 300,
                                # define hypothesis
                                Beta = Beta,
                                nullEffect = 'beta = 0',
                                nullWhich = c(4, 1),
                                # define measurement model
                                nIndicator = c(3, 4, 5, 6),
                                loadM = c(.7, .5, .6, .8),
                                )
```
Now, `summary(powerPath$power)` provides the same information as a model-free [post-hoc power analysis](#ph) counterpart. A  [compromise power analysis](#cp) (`type = 'compromise'`) is performed analogously.

In the example above, there were no correlations between the factors beyond those implied by the regression relations. The `Psi` argument can be used to add additional sources of covariation. For instance, the following defines four factors with a regression structure (`Beta`) corresponding to $F_3 = .2 \cdot F_1 + .3 \cdot F_2$ and $F_4 = .3 \cdot F_1 + .4 \cdot F_2$. In addition, `Psi` defines a correlation between $F_1$ and $F_2$ of .25 and a (residual) correlation between $F_3$ and $F_4$ of .3.
```{r eval=FALSE}
Beta <- matrix(c(
  c(.00, .00, .00, .00),       # f1 = .00*f1 + .00*f2 + .00*f3 + .00*f4
  c(.00, .00, .00, .00),       # f2 = .00*f1 + .00*f2 + .00*f3 + .00*f4
  c(.20, .30, .00, .00),       # f3 = .20*f1 + .30*f2 + .00*f3 + .00*f4
  c(.30, .40, .00, .00)        # f4 = .30*f1 + .40*f2 + .00*f3 + .00*f4
), byrow = TRUE, ncol = 4)
Psi <- matrix(c(
  c(1.0, .25, .00, .00),       # f1
  c(.25, 1.0, .00, .00),       # f2
  c(.00, .00, 1.0, .30),       # f3
  c(.00, .00, .30, 1.0)        # f4
), byrow = TRUE, ncol = 4)
powerPath <- semPower.powerPath(
                                # define type of power analysis
                                type = 'a-priori', alpha = .05, power = .80,
                                # define hypothesis
                                Beta = Beta,
                                Psi = Psi,
                                nullEffect = 'beta = 0',
                                nullWhich = c(4, 1),
                                # define measurement model
                                nIndicator = c(3, 4, 5, 6),
                                loadM = c(.7, .5, .6, .8),
                                )
```

Note that all examples above treated all parameters as completely standardized (by omitting the `standardized` argument, which defaults to `TRUE`). When `standardized = TRUE`, `semPower` defines the residual variances in $\Psi$ such that all variances are 1. When `Psi` is provided, the diagonal elements are ignored and all off-diagonal elements are treated as (residual-) correlations. When `standardized = FALSE`, `Psi` is unaltered (or replaced by a diagonal matrix, when `Psi = NULL`).

Further, all examples above performed a power analysis by comparing the implied H0 model against a less restrictive H1 model (by omitting the `comparison` argument which defaults to `'restricted'`). If one rather wants to compare the H0 model against the saturated model, use `comparison = 'saturated'`. See the chapter on [the definition of the comparison model](#comparisonModel) for a detailed discussion. 

If one (or all) of the involved variables is an observed variable (instead of a factor), the only change refers to the [definition of the factor model](#factorDefinition). For instance, the following defines the first and the third "factor" as dummy factors with a single indicator (`nIndicator = c(1, 4, 1, 6)`) and a loading of 1 (`loadM = c(1, .5, 1, .8)`), so these become equivalent to observed variables:

```{r eval=FALSE}
powerPath <- semPower.powerPath(
                                # define type of power analysis
                                type = 'a-priori', alpha = .05, power = .80,
                                # define hypothesis
                                Beta = Beta,
                                nullWhich = c(4, 1),
                                # define measurement model
                                nIndicator = c(1, 4, 1, 6),
                                loadM = c(1, .5, 1, .8),
                                )
```
Similarly, a path model involving observed variables only could be defined using `nIndicator = c(1, 1, 1, 1)` and `loadM = c(1, 1, 1, 1)` or, more simply, by just providing `Lambda = diag(4)` instead of `nIndicator` and `loadM`.


##### Detect whether two slopes differ from each other {-}
To perform a power analysis to detect whether two correlations differ from each other, use `nullEffect = 'betaX = betaZ'`.

For instance, the following defines the regression relationships (`Beta`) between four factors in the same way as described in detail above to be $F_3 = .2 \cdot F_1 + .3 \cdot F_2$ and $F_4 = .3 \cdot F_1 + .4 \cdot F_2$. The four factors are measured by 3, 4, 5, and 6 indicators (`nIndicator = c(3, 4, 5, 6)`), which load by .7, .5, .6, and .8, respectively (`loadM = c(.7, .5, .6, .8)`, see [definition of the factor model](#factorDefinition)). The the required sample (`type = 'a-priori'`) is requested to detect that the slopes for $F1$ and $F_2$ in the prediction of $F_4$ (`nullWhich = list(c(4, 1), c(4, 2))`) differ (`nullEffect = 'betaX = betaZ'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). 

```{r eval=FALSE}
Beta <- matrix(c(
  c(.00, .00, .00, .00),       # f1 = .00*f1 + .00*f2 + .00*f3 + .00*f4
  c(.00, .00, .00, .00),       # f2 = .00*f1 + .00*f2 + .00*f3 + .00*f4
  c(.20, .30, .00, .00),       # f3 = .20*f1 + .30*f2 + .00*f3 + .00*f4
  c(.30, .40, .00, .00)        # f4 = .30*f1 + .40*f2 + .00*f3 + .00*f4
), byrow = TRUE, ncol = 4)
powerPath <- semPower.powerPath(
                                # define type of power analysis
                                type = 'a-priori', alpha = .05, power = .80,
                                # define hypothesis
                                Beta = Beta,
                                nullEffect = 'betaX = betaZ',
                                nullWhich = list(c(4, 1), c(4, 2)),
                                # define measurement model
                                nIndicator = c(3, 4, 5, 6),
                                loadM = c(.7, .5, .6, .8),
                                )
```

Note that `nullWhich` is now a list of vectors defining which slopes should be set to equality. `nullWhich = list(c(4, 1), c(4, 2))` says that the slopes for $F1$ and $F_2$ in the prediction of $F_4$ shall be equal.

`nullWhich` can also comprise more than two elements to test for the equality of more than two slopes For instance, when there are 4 predictors, using `nullWhich = list(c(4, 1), c(4, 2), c(3, 1))` would constrain the slopes for for $F1$ and $F_2$ in the prediction of $F_4$ and the slope for $F1$ in the prediction of $F_3$ to equality.



##### Detect whether a slope differs across two or more groups {-}

To perform a power analysis to detect whether a slope differs across two or more groups, use `nullEffect = 'betaA = betaB'`. 

For instance, the following defines the regression relationships between three factors in the same way as described in detail above separately for two groups. In  the first group (`Beta1`), the relations are $F_2 = .2 \cdot F_1$ and $F_3 = .3 \cdot F_1 + .5 \cdot F_2$, whereas in the second group (`Beta2`) these are $F_2 = .4 \cdot F_1$ and $F_3 = .3 \cdot F_1 + .5 \cdot F_2$. In multiple group models, `Beta` must be provided as a list, where each component defines the regression relations for a  specific group (`Beta = list(Beta1, Beta2)`). The measurement model is identical across groups: All factors are measured by 5 indicators (`nIndicator =  c(5, 5, 5)`) which load by .7 on the first, by .5 on the second, and by .6 on the third factor (`loadM =  c(.7, .5, .6)`). Then, the required sample (`type = 'a-priori'`) is requested to detect that the slope of $F_1$ in the prediction of $F_2$ (`nullWhich = c(2, 1)`) differs across groups (`nullEffect = 'betaA = betaB'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). Furthermore, in multiple group models the `N` argument also needs to be provided as a list, which in case of an a-priori power analysis gives the group weights. `N = list(1, 1)` requests equally sized groups. If using `N = list(2, 1)` instead, the first group would be twice as large as the second group. If a post-hoc or compromise power analysis is requested, `N` is a list providing the number of observations for each group. 

```{r eval=FALSE}
# beta in group 1
Beta1 <- matrix(c(
  c(.00, .00, .00),       # f1 = .00*f1 + .00*f2 + .00*f3 + .00*f4
  c(.20, .00, .00),       # f2 = .20*f1 + .00*f2 + .00*f3 + .00*f4
  c(.30, .50, .00)        # f3 = .30*f1 + .30*f2 + .00*f3 + .00*f4
), byrow = TRUE, ncol = 3)
# beta in group 2
Beta2 <- matrix(c(
  c(.00, .00, .00),       # f1 = .00*f1 + .00*f2 + .00*f3 + .00*f4
  c(.40, .00, .00),       # f2 = .40*f1 + .00*f2 + .00*f3 + .00*f4
  c(.30, .50, .00)        # f3 = .30*f1 + .30*f2 + .00*f3 + .00*f4
), byrow = TRUE, ncol = 3)
powerPath <- semPower.powerPath(
                                # define type of power analysis
                                type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                                # define hypothesis
                                Beta = list(Beta1, Beta2),
                                nullEffect = 'betaA = betaB',
                                nullWhich = c(2, 1),
                                # define measurement model
                                nIndicator =  c(5, 5, 5),
                                loadM =  c(.7, .5, .6)
                                )
```


If there are more than two groups, the targeted slope is held equal across all groups by default. If the slope should only be constrained to equality in specific groups, `nullWhichGroups` is used to identify the groups to which the equality restrictions apply. For instance, the following defines three equally sized groups with a distinct slope for $F_1$ in the prediction of $F_2$, but only asks for the required sample to detect that this slope (`nullWhich = c(2, 1)`) in group 1 (of .20) differs from the second slope in group 3 (of .40; `nullWhichGroups = c(1, 3)`). 

```{r eval=FALSE}
# beta in group 1
Beta1 <- matrix(c(
  c(.00, .00),      # f1 = .00*f1 + .00*f2
  c(.20, .00)       # f2 = .20*f1 + .00*f2
), byrow = TRUE, ncol = 2)
# beta in group 2
Beta2 <- matrix(c(
  c(.00, .00),      # f1 = .00*f1 + .00*f2
  c(.30, .00)       # f2 = .30*f1 + .00*f2
), byrow = TRUE, ncol = 2)
# beta in group 3
Beta3 <- matrix(c(
  c(.00, .00),      # f1 = .00*f1 + .00*f2
  c(.40, .00)       # f2 = .40*f1 + .00*f2
), byrow = TRUE, ncol =2)
powerPath <- semPower.powerPath(
                                # define type of power analysis
                                type = 'a-priori', alpha = .05, power = .80, N = list(1, 1, 1),
                                # define hypothesis
                                Beta = list(Beta1, Beta2, Beta3),
                                nullEffect = 'betaA = betaB',
                                nullWhich = c(2, 1),
                                nullWhichGroups =  c(1, 3),
                                # define measurement model
                                nIndicator =  c(5, 5),
                                loadM =  c(.7, .5)
                                )
```



## Multiple group invariance {#powerMI}
`semPower.powerMI` is used to perform power analyses for hypothesis arising in multigroup measurement invariance models. The typical - but not in all parts necessary -
sequence is (a) configural, (b) metric, (c) scalar, and (d) residual invariance, where each level of invariance is usually compared against the previous level (e.g., scalar vs. metric). `semPower.powerMI` provides interfaces to perform power analyses concerning the hypothesis whether a particular level of invariance is tenable, implementing a model-based approach, so that non-invariant parameters need to be specified. When one does not expect (or is not interested in or does not have sufficiently specific hypotheses on) measurement non-invariance for certain parameters, but rather assumes that non-invariance spreads across multiple parameters (say, across most or all loadings), one should consider to perform model-free power analysis concerning the [overall difference](#diffPower) between two models.

`semPower.powerMI` only addresses hypotheses concerning multigroup measurement invariance. See the corresponding chapter for [other hypothesis arising in multigroup settings](#multipleGroups).

`semPower.powerMI` expects the following arguments:

* `comparison`: Defines the comparison model (see below for valid arguments). 
* `nullEffect`: Defines the level of invariance of interest (see below for valid arguments).
* additional arguments [specifying the type of power analysis](#commonArgs). 
* additional arguments [defining the factor model](#factorDefinition), which in must include a list structure for at least some model parameters, so that parameters that differ across groups are defined.   


The models defined in the `comparison` and `nullEffect` arguments can be one of:

* `'configural'`: no invariance constraints. Shows the same fit as the saturated model, so this only affects the df. 
* `'metric'`: all loadings are restricted to equality. 
* `'scalar'`: all loadings and (indicator-)intercepts are restricted to equality. 
* `'residual'`: all loadings, (indicator-)intercepts, and (indicator-)residuals are restricted to equality.
 
Alternatively, the models can also be defined using `lavaan` style `group.equal` restrictions as a vector to allow for greater flexibility: 

* `'none'`: no invariance constraints and thus representing a configural invariance model.
* `c('loadings')`: all loadings are restricted to equality. 
* `c('loadings', 'intercepts')`: all loadings and (indicator-)intercepts are restricted to equality. 
* `c('loadings', 'intercepts', 'residuals')`: all loadings, (indicator-)intercepts, and (indicator-)residuals are restricted to equality.
* `c('loadings', 'residuals')`: all loadings and (indicator-)residuals are restricted to equality.
* `c('loadings', 'intercepts', 'means')`: all loadings, (indicator-)intercepts, and latent factor means are restricted to equality.

Note that `semPower.powerMI` implements variance scaling of the factors (the variances of all factors are equal to 1 in all groups), so invariance of variances (`'lv.variances'`) is always met.

`semPower.powerMI` provides a list as result, which contains the following components:

* `power`: The results of the power analysis, which contains the same information as the corresponding model-free counterpart (see [a-priori power analysis](#ap), [post-hoc power analysis](#ph), and [compromise hoc power analysis](#cp)). Use the `summary` method  to obtain formatted results.
* `Sigma` and `mu`: Variance-covariance matrix and means in the population.
* `SigmaHat` and `muHat`: Model implied variance-covariance matrix and means.
* `modelH0` and `modelH1`: `lavaan` model strings defining the H0 and the H1 model (only if `type = 'restricted'`). Note that multiple group constraints are provided to `lavaan` via its `group.equal` argument, which is not returned by `semPower.powerMI`.


##### Detect metric non-invariance {-}
To perform a power analysis to detect whether a metric invariance model is significantly worse than a configural invariance model, use `nullEffect = 'metric'` in conjunction with `comparison = 'configural'`, and define the factor model in a way that at least one loading differs across groups (so that metric invariance is violated to the extent as defined by the difference in the loadings across groups).

For instance, the following requests the required sample (`type = 'a-priori'`) to detect that a metric invariance model (`nullEffect = 'metric'`) differs from a configural invariance model (`comparison = 'configural'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). The model comprises a single factor, which is measured by 5 indicators in both groups (`nIndicator = list(5, 5)`). In the first group, all indicators load by .5, whereas in the second group, all indicators load by .6 (`loadM = list(.5, .6)`). See the chapter on [specifying a factor model](#factorDefinition) for alternative (more flexible) ways to define the factor loadings. Furthermore, in multiple group models the `N` argument also needs to be supplied as a list, which in case of an a-priori power analysis gives the group weights. `N = list(1, 1)` requests equally sized groups. If using `N = list(2, 1)` instead, the first group would be twice as large as the second group. 

```{r eval=FALSE}
powerMI <- semPower.powerMI(
                           # define type of power analysis
                           type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                           # define hypothesis
                           comparison = 'configural', 
                           nullEffect = 'metric',
                           # define measurement model
                           nIndicator = list(5, 5),
                           loadM = list(.5, .6))
summary(powerMI$power)
```
The results of the power analysis are printed by calling the `summary` method on `powerMI$power`, which in this example provides the same information as a model-free [a-priori power analysis](#ap) counterpart.

If a post-hoc power analysis is desired, the arguments related to the power analysis need to be adapted accordingly, where `N` now provides the number of observations for each group:
```{r eval=FALSE}
powerMI <- semPower.powerMI(
                           # define type of power analysis
                           type = 'post-hoc', alpha = .05, N = list(300, 400),
                           # define hypothesis
                           comparison = 'configural', 
                           nullEffect = 'metric',
                           # define factor model
                           nIndicator = list(5, 5),
                           loadM = list(.5, .6))
```
Now, `summary(powerMI$power)` provides the same information as a model-free [post-hoc power analysis](#ph) counterpart. A [compromise power analysis](#cp) (`type = 'compromise'`) is performed analogously.

The values provided to the `comparison` and `nullEffect` arguments can also be specified according to `lavaan` conventions as vectors, so the following yields the same results as above: 
```{r eval=FALSE}
powerMI <- semPower.powerMI(
                           type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                           # define hypothesis
                           comparison = 'none', 
                           nullEffect = c('loadings'),
                           nIndicator = list(5, 5),
                           loadM = list(.5, .6))
```

Note that the arguments [specifying a factor model](#factorDefinition) must be provided as lists, where each component refers to a specific group. For instance, the following defines a two-factor model, where the first factor is measured by 3 indicators and the second factor is measured by 4 indicators (in both groups, `nIndicator = list(c(3, 4), c(3, 4))`). In the first group, all loadings are equal to .5. In the second group, the loadings on the first factor are also .5, but the loadings on the second factor are .6 (`list(c(.5, .5), c(.5, .6))`). In both groups, the factor correlation is .3 (`Phi = list(.3, 3)`).

```{r eval=FALSE}
powerMI <- semPower.powerMI(
                           type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                           comparison = 'configural', 
                           nullEffect = 'metric',
                           # define two factors 
                           nIndicator = list(c(3, 4), c(3, 4)),
                           loadM = list(c(.5, .5), c(.5, .6)),
                           Phi = list(.3, 3))
```

Measurement parameters that should be equal across groups can also be provided omitting the list structure, so the same as above can be achieved by using: 

```{r eval=FALSE}
powerMI <- semPower.powerMI(
                           type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                           comparison = 'configural', 
                           nullEffect = 'metric',
                           # define two factors
                           nIndicator = c(3, 4),
                           loadM = list(c(.5, .5), c(.5, .6)),
                           Phi = list(.3, .3))
```

In the examples above, all indicators of a certain factor exhibited measurement non-invariance. If only a subset of indicators should show a different loading by group, [specify the factor model](#factorDefinition) using the `loadings` argument. For instance, the following defines a two-factor model with a factor correlation of .3 (`Phi = list(.3, 3)`) in both groups. The first factor is measured by 3 indicators and the second factor is measured by 4 indicators (in both groups). In the first group, the loadings on the first factor are .7, .6, and .5, and those on the second factor are  .5, .5, and .7. In the second group, the loadings on the first factor are .7, .7, and .5, and those on the second factor are .5, .5, and .6. Thus, there are group differences concerning the loadings of the second indicator of the first factor (.6 vs .5) and concerning the loadings of the third indicator of the second factor (.7 vs .6). 


```{r eval=FALSE}
powerMI <- semPower.powerMI(
                           type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                           comparison = 'configural', 
                           nullEffect = 'metric',
                           # define measurement model 
                           loadings = list(
                             # loadings on the first and second factor in the first group
                             list(c(.7, .6, .5), 
                                  c(.5, .5, .7)),
                             # loadings on the first and second factor in the second group
                             list(c(.7, .7, .5), 
                                  c(.5, .5, .6))
                             ),
                           Phi = list(.3, .3))
```



##### Detect scalar non-invariance {-}
Detecting failure of another level of invariance proceeds largely identical as described in the case of the metric invariance model above. The differences concern that `nullEffect` now refers to the level of invariance of interest (such as `nullEffect = 'scalar'`) and that the comparison model would usually refer to the model one level lower in the hierarchy (such as `comparison = 'metric'`). In addition, when comparing a scalar against a metric invariance model, the factor model should be defined in a way that metric invariance holds (identical loadings across groups), whereas there must be at least one difference across groups concerning the indicator intercepts (`tau`).

For instance, the following requests the required sample (`type = 'a-priori'`) to detect that a scalar invariance model (`nullEffect = 'scalar'`) differs from a metric invariance model (`comparison = 'metric'`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). The model comprises a single factor, which is measured by 5 indicators that all load by .5 in both groups (`nIndicator = list(5, 5)`; see [definition of the factor model](#factorDefinition)). Importantly, the indicator intercepts (`tau`) partly differ across groups: the second intercept is 0 in the first, but .1 in the second group, the third intercept is 0 in the first, but -.3 in the second group:

```{r eval=FALSE}
powerMI <- semPower.powerMI(
                           # define type of power analysis
                           type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                           # define hypothesis
                           comparison = 'metric', 
                           nullEffect = 'scalar',
                           # define measurement model (same for all groups)
                           nIndicator = 5,
                           loadM = .5,
                           # define indicator intercepts
                           tau = list(
                             # intercepts in the first group
                             c(0, 0, 0, 0, 0),
                             # intercepts in the second group
                             c(0, .1, -.3, 0, 0)
                           ))
```

Equivalently, `comparison` and `nullEffect` can also be provided according to `lavaan` conventions: 
```{r eval=FALSE}
powerMI <- semPower.powerMI(
                           type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                           # define hypothesis
                           comparison = c('loadings'), 
                           nullEffect = c('loadings', 'intercepts'),
                           # define measurement model
                           nIndicator = 5,
                           loadM = .5,
                           tau = list(
                             c(0, 0, 0, 0, 0),
                             c(0, .1, -.3, 0, 0)
                           ))
```


##### Detect whether latent means differ across groups {-}
Detecting that latent factor means differ across groups again proceeds largely identical as described above for tests of other levels of invariance. However, `comparison` and `nullEffect` must now be provided in a `lavaan` format and the model definition must include a statement about indicator intercepts (`tau`) and latent means (`Alpha`).


For instance, the following requests the required sample (`type = 'a-priori'`) to detect that the latent means on a single factor differ across groups  (`nullEffect = c('loadings', 'intercepts', 'means')`) in comparison to a scalar invariance model (`comparison = c('loadings', 'intercepts')`) on alpha = .05 (`alpha = .05`) with a power of 80% (`power = .80`). The model comprises a single factor, which is measured by 5 indicators that all load by .5 in both groups (`nIndicator = 5` and `loadM = .5`; see [definition of the factor model](#factorDefinition)). All indicator intercepts (`tau`) equal zero in both groups. Importantly, the latent mean (`Alpha`) is 0 in the first, but .5 in the second group (thus corresponding to a standardized mean difference of .5, because the factor variances are fixed to 1):

```{r eval=FALSE}
powerMI <- semPower.powerMI(
                           # define type of power analysis
                           type = 'a-priori', alpha = .05, power = .80, N = list(1, 1),
                           # define hypothesis
                           comparison = c('loadings', 'intercepts'), 
                           nullEffect = c('loadings', 'intercepts', 'means'),
                           # define measurement model (same for all groups)
                           nIndicator = 5,
                           loadM = .5,
                           # define indicator intercepts
                           tau = list(
                             c(0, 0, 0, 0, 0),
                             c(0, 0, 0, 0, 0)
                           ),
                           # define latent means in the first and the second group
                           Alpha = list(
                             c(0.0), 
                             c(0.5)
                           ))
```



## CLPM models {#powerCLPM}
`semPower.powerCLPM` is used to perform power analyses for hypothesis arising in cross-lagged panel models (CLPM). In a standard CLPM implemented here, two variables $X$ and $Y$ are repeatedly assessed at two or more different time points (waves) yielding autoregressive effects (stabilities; `X1 -> X2` and `Y1 -> Y2`), synchronous effects (`X1 <-> Y1` and `X2 <-> Y2`), and cross-lagged effects (`X1 -> Y2` and `Y1 -> X2`). `semPower.powerCLPM`provides interfaces to perform power analyses concerning the following hypotheses:

* whether the autoregressive effects of $X$ (`X1 -> X2`, `autoregX = 0`) or $Y$ (`Y1 -> Y2`, `autoregY = 0`) differ from zero. 
* whether the crossed effect of $X$ on $Y$ (`X -> Y`, `crossedX`) or the crossed effect of $Y$ on $X$ (`Y -> X`, `crossedY`) differs from zero. 
* whether the autoregressive effects of $X$ and $Y$ are equal (`autoregX = autoregY`).
* whether the crossed effect of $X$ on $Y$ and the crossed effect of $Y$ on $X$ are equal (`crossedX = crossedY`).
* whether the autoregressive effect of $X$ (`autoregX`) or the autoregressive effect of $Y$ (`autoregY`) are equal across waves. 
* whether the crossed effect of $X$ on $Y$ (`crossedX`) or the crossed effect of $Y$ on $X$ (`crossedY`) are equal across waves. 
* whether the (residual-)correlations between $X$ and $Y$ are equal across waves (`corXY`). 

`semPower.powerCLPM` only addresses hypotheses arising in a CLPM structure. `semPower` provides other convenience functions for hypothesis arising in  [random intercep cross-lagged panel models](#powerRICLPM) and in [generic path models](#powerPath). For hypotheses regarding global model fit, a [model-free power analysis](#modelFreePower) should be performed. 

`semPower.powerCLPM` expects the following arguments:

* `nWaves`: the number of waves, must be at $\geq$ 2. 
* `autoregEffects`: vector of the autoregressive effects of $X$ and $Y$ (constant across waves), or a list of vectors of autoregressive effects for $X$ and $Y$ from wave to wave. 
* `crossedEffects`: vector of crossed effects of $X$ on $Y$ (`X -> Y`) and of $Y$ on $X$ (`Y -> X`) (both constant across waves), or a list of vectors of crossed effects for each wave. 
* `rXY`: vector of (residual-)correlations between $X$ and $Y$ for each wave or `NULL` for no (residual-)correlations. 
* `waveEqual`: parameters that are assumed to be equal across waves in both the H0 and the H1 model. Valid are `'autoregX'` and `'autoregY'` for autoregressive effects, `'crossedX'` and `'crossedY'` for crossed effects, `'corXY'` for residual correlations, or `NULL` for none. 
* `nullEffect`: Defines the hypothesis of interest. Valid are the same arguments as in `waveEqual` and additionally `'autoregX = 0'`, `'autoregY = 0'`, `'crossedX = 0'`, `'crossedY = 0'` to constrain the $X$ or $Y$ autoregressive effects or the crossed effects to zero, `'autoregX = autoregY'` and `'crossedX = crossedY'` to constrain them to be equal for $X$ and $Y$.
* `nullWhich`: Defines which parameter(s) is targeted by the hypothesis defined in `nullEffect` when there are > 2 waves.
* `standardized`: Whether all parameters are standardized (`TRUE`, the default) or unstandardized (`FALSE`).
* `metricInvariance`: whether metric invariance over waves is assumed (`TRUE`, the default) or not (`FALSE`). This generally affects power and may also affect the df, depending on the comparison model. 
* additional arguments [specifying the type of power analysis](#commonArgs). 
* additional arguments [defining the factor model](#factorDefinition), where the order of factors is ($X_1$, $Y_1$, $X_2$, $Y_2$, ..., $X_{nWaves}$, $Y_{nWaves}$). 

`semPower.powerCLPM` provides a list as result, which contains the following components:

* `power`: The results of the power analysis, which contains the same information as the corresponding model-free counterpart (see [a-priori power analysis](#ap), [post-hoc power analysis](#ph), and [compromise hoc power analysis](#cp)). Use the `summary` method  to obtain formatted results.
* `Sigma` and `mu`: Variance-covariance matrix and means in the population.
* `SigmaHat` and `muHat`: Model implied variance-covariance matrix and means.
* `modelH0` and `modelH1`: `lavaan` model strings defining the H0 and the H1 model (only if `type = 'restricted'`).


##### Detect whether an autoregressive effect differs from zero {-}
to be written


In the examples above, a power analysis was performed by comparing the implied H0 model against a less restrictive H1 model (by omitting the `comparison` argument which defaults to `'restricted'`). If one rather wants to compare the H0 model against the saturated model, use `comparison = 'saturated'`. See the chapter on [the definition of the comparison model](#comparisonModel) for a detailed discussion. 


##### Detect whether a crossed effect differs from zero {-}
to be written

##### Detect whether the autoregressive effects of $X$ and $Y$ differ {-}
to be written

##### Detect whether the crossed effects of $X$ and $Y$ differ {-}
to be written

##### Detect whether the autoregressive effects differ across waves  {-}
to be written

##### Detect whether the crossed effects differ across waves  {-}
to be written

##### Detect whether the residual correlations between $X$ and $Y$ differ across waves  {-}
to be written


## RI-CLPM models {#powerRICLPM}
`semPower.powerRICLPM` is used to perform power analyses for hypothesis arising in random intercept cross-lagged panel models (RI-CLPM). In a standard ri-CLPM implemented here, two variables $X$ and $Y$ are repeatedly assessed at three or more different time points (waves) yielding autoregressive effects (`X1 -> X2`, `X2 -> X3`, `Y1 -> Y2`, and `Y2 -> Y3`), synchronous effects (`X1 <-> Y1`, `X2 <-> Y2` , `X3 <-> Y3`), and cross-lagged effects (`X1 -> Y2`, `Y1 -> X2`, `X2 -> Y3`, and `Y2 -> X3`). `semPower.powerRICLPM`provides interfaces to perform power analyses concerning the following hypotheses:

* whether the autoregressive effects of $X$ (e.g., `X1 -> X2`, `autoregX = 0`) or $Y$ (e.g.,`Y1 -> Y2`, `autoregY = 0`) differ from zero. 
* whether the crossed effects of $X$ on $Y$ (`X -> Y`, `crossedX`) or the crossed effects of $Y$ on $X$ (`Y -> X`, `crossedY`) differs from zero. 
* whether the autoregressive effects of $X$ and $Y$ are equal (`autoregX = autoregY`).
* whether the crossed effects of $X$ on $Y$ and the crossed effects of $Y$ on $X$ are equal (`crossedX = crossedY`).
* whether the autoregressive effect of $X$ (`autoregX`) or the autoregressive effect of $Y$ (`autoregY`) are equal across waves. 
* whether the crossed effects of $X$ on $Y$ (`crossedX`) or the crossed effects of $Y$ on $X$ (`crossedY`) are equal across waves. 
* whether the (residual-)correlations between $X$ and $Y$ are equal across waves (`corXY`). 
* whether the correlation between the random intercept factors of X and Y differs from zero (`corBXBY = 0`).

`semPower.powerRICLPM` only addresses hypotheses arising in a RI-CLPM structure. `semPower` provides other convenience functions for hypothesis arising in standard [cross-lagged panel models](#powerCLPM) and in [generic path models](#powerPath). For hypotheses regarding global model fit, a [model-free power analysis](#modelFreePower) should be performed. 

`semPower.powerRICLPM` expects the following arguments:

* `nWaves`: the number of waves, must be at $\geq$ 3. 
* `autoregEffects`: vector of the autoregressive effects of $X$ and $Y$ (constant across waves), or a list of vectors of autoregressive effects for $X$ and $Y$ from wave to wave. 
* `crossedEffects`: vector of crossed effects of $X$ on $Y$ (`X -> Y`) and of $Y$ on $X$ (`Y -> X`) (both constant across waves), or a list of vectors of crossed effects for each wave. 
* `rXY`: vector of (residual-)correlations between $X$ and $Y$ for each wave or `NULL` for no (residual-)correlations. 
* `rBXBY`: correlation between random intercept factors, or `NULL` for no correlation.
* `waveEqual`: parameters that are assumed to be equal across waves in both the H0 and the H1 model. Valid are `'autoregX'` and `'autoregY'` for autoregressive effects, `'crossedX'` and `'crossedY'` for crossed effects, `'corXY'` for residual correlations, or `NULL` for none. 
* `nullEffect`: Defines the hypothesis of interest. Valid are the same arguments as in `waveEqual` and additionally `'autoregX = 0'`, `'autoregY = 0'`, `'crossedX = 0'`, `'crossedY = 0'` to constrain the $X$ or $Y$ autoregressive effects or the crossed effects to zero, `'autoregX = autoregY'` and `'crossedX = crossedY'` to constrain them to be equal for $X$ and $Y$, and `'corBXBY = 0'` to constrain the correlation between the random intercepts to zero.
* `nullWhich`: Defines which parameter(s) is targeted by the hypothesis defined in `nullEffect` when there are > 2 waves.
* `standardized`: Whether all parameters are standardized (`TRUE`, the default) or unstandardized (`FALSE`).
* `metricInvariance`: whether metric invariance over waves is assumed (`TRUE`, the default) or not (`FALSE`). This generally affects power and may also affect the df, depending on the comparison model. 
* additional arguments [specifying the type of power analysis](#commonArgs). 
* additional arguments [defining the factor model](#factorDefinition), where the order of factors is ($X_1$, $Y_1$, $X_2$, $Y_2$, ..., $X_{nWaves}$, $Y_{nWaves}$). 

`semPower.powerRICLPM` provides a list as result, which contains the following components:

* `power`: The results of the power analysis, which contains the same information as the corresponding model-free counterpart (see [a-priori power analysis](#ap), [post-hoc power analysis](#ph), and [compromise hoc power analysis](#cp)). Use the `summary` method  to obtain formatted results.
* `Sigma` and `mu`: Variance-covariance matrix and means in the population.
* `SigmaHat` and `muHat`: Model implied variance-covariance matrix and means.
* `modelH0` and `modelH1`: `lavaan` model strings defining the H0 and the H1 model (only if `type = 'restricted'`).


##### Detect whether an autoregressive effect differs from zero {-}
to be written


In the examples above, a power analysis was performed by comparing the implied H0 model against a less restrictive H1 model (by omitting the `comparison` argument which defaults to `'restricted'`). If one rather wants to compare the H0 model against the saturated model, use `comparison = 'saturated'`. See the chapter on [the definition of the comparison model](#comparisonModel) for a detailed discussion. 


##### Detect whether a crossed effect differs from zero {-}
to be written

##### Detect whether the correlations between the random intercept factors differs from zero {-}
to be written

##### Detect whether the autoregressive effects of $X$ and $Y$ differ {-}
to be written

##### Detect whether the crossed effects of $X$ and $Y$ differ {-}
to be written

##### Detect whether the autoregressive effects differ across waves  {-}
to be written

##### Detect whether the crossed effects differ across waves  {-}
to be written

##### Detect whether the residual correlations between $X$ and $Y$ differ across waves  {-}
to be written



## Generic model-based power analysis {#powerLav}

All of the functions described above implement a high-level approach towards performing model-based power analysis to facilitate the definition of the relevant models and hypotheses. At times, one might want to perform model-based power analysis that are not immediately covered by one of these functions. Therefore, `semPower` also provides a more generic way to perform a model-based power analysis via the `semPower.powerLav` function that requires the direct specification of the H0 and H1 `lavaan` model strings as well as either the population covariance matrix (and means) or the population model.

Consider the situation that one is interested in determining whether the observed responses on 8 items reflect two separate (but correlated) factors or can be described by assuming just a single factor. A suitable model to test this hypothesis would specify two factors and constrain their correlation to 1. When this constrained model fits the data, a single factor is sufficient. Otherwise, two factors are required.   

Suppose that a correlation between two factors of $r > .9$ is considered as implying that these are practically equivalent, so these could be collapsed into a single factor. The misfit associated with a model assuming a correlation of 1 when, in reality, the true correlation is $\leq$ .9 is supposed to define the magnitude of effect. This scenario is not covered by the [`semPower.powerCFA`](#powerCFA) function, so we need some manual work to achieve this. Three steps are required to perform a power analysis in this scenario: 

* Define the population model that describes the true situation in the population or provide the population covariance matrix. 
* Define an (incorrect) analysis model that reflects the null hypothesis of interest.
* Optionally define a (correct) analysis model that reflects the alternative hypothesis.
 
##### Define the true affairs in the population {-}

One option to define the true affairs in the population is to use a `lavaan` model string specifying the values for each single (non-zero) parameter. Thus, recurring to the example above, one needs to define each loading, each residual variance, the variance of the factors as well as their covariance. Suppose the standardized loadings vary between .4 and .8, and that - in the population - the two factors correlate by .9. 

```{r eval=FALSE}
# define (true) population model
modelPop <- '
# define relations between factors and items in terms of loadings
f1 =~ .7*x1 + .7*x2 + .5*x3 + .5*x4
f2 =~ .8*x5 + .6*x6 + .6*x7 + .4*x8
# define unique variances of the items to be equal to 1-loading^2, 
# so that the loadings above are in a standardized metric
x1 ~~ .51*x1
x2 ~~ .51*x2
x3 ~~ .75*x3
x4 ~~ .75*x4
x5 ~~ .36*x5
x6 ~~ .64*x6
x7 ~~ .64*x7
x8 ~~ .84*x8
# define variances of f1 and f2 to be 1
f1 ~~ 1*f1   
f2 ~~ 1*f2   
# define covariance (=correlation, because factor variances are 1) 
# between the factors to be .9
f1 ~~ 0.9*f2 
'
```

Instead of defining a population model via a `lavaan` model string, it is also possible to provide the population covariance matrix directly. A useful utility function supporting this process is [`semPower.genSigma`](#genSigma). For instance, the following returns the very same population covariance matrix (`generated$Sigma`) as implied by the `lavaan` model string defined above:
```{r eval=FALSE}
generated <- semPower.genSigma(Phi = .90, 
                               loadings = list(
                                 c(.7, .7, .5, .5),
                                 c(.8, .6, .6, .4)))
```
See [`semPower.genSigma`](#genSigma) for more information.


##### Define the H0 and the H1 models {-}

Having defined the true affairs in the population, one now needs to define the (incorrect) analysis model reflecting the null hypothesis of interest. This model should make at least one restriction which is factually wrong and thereby defines the effect of interest, in the present case the restriction that the two factors correlate to 1:

```{r eval=FALSE}
# define (wrong) analysis model
modelH0 <- '
f1 =~ NA*x1 + x2 + x3 + x4
f2 =~ NA*x5 + x6 + x7 + x8
# define variances of f1 and f2 to be 1
f1 ~~ 1*f1   
f2 ~~ 1*f2   
# set correlation between the factors to 1
f1 ~~ 1*f2
' 
```

Whereas the population model (or the population covariance matrix) and the H0 model are sufficient to perform a power analysis, one can also define an explicit H1 model which is to be compared against the H0 model. Below, the H1 model is defined in a way that it is correct, so this only affects the df for the power analysis. 

```{r eval=FALSE}
# define (correct) comparison model
modelH1 <- '
f1 =~ NA*x1 + x2 + x3 + x4
f2 =~ NA*x5 + x6 + x7 + x8
# define variances of f1 and f2 to be 1
f1 ~~ 1*f1   
f2 ~~ 1*f2   
# freely estimate the correlation between the factors to 1
f1 ~~ f2
' 
```

When [`semPower.genSigma`](#genSigma) is used to obtain a population covariance matrix, a correct H1 model string is also returned (`generated$modelTrue`), so one might skip the explicit definition of the H1 model and just use the returned string. Indeed, using this approach one might also modify the returned model string to reflect the actual hypothesis of interest. Instead of writing the complete H0 and H1 model strings as tediously done in full detail above, the very same may also be achieved through

```{r eval=FALSE}
generated <- semPower.genSigma(Phi = .90, 
                               loadings = list(
                                 c(.7, .7, .5, .5),
                                 c(.8, .6, .6, .4)))
modelH1 <- generated$modelTrue
# define modelH0 as function of modelH1 plus the 
# additional constraint of interest
modelH0 <- paste(modelH1, 'f1 ~~ 1*f2', sep = '\n')
```



##### Perform a power analysis {-}

Finally, all these information are plugged into `semPower.powerLav()`, in this example requesting an a-priori power analysis:

```{r eval=FALSE}
# using the population model 
ap <- semPower.powerLav(type = 'a-priori', alpha = .05, power = .80,
                        modelPop = modelPop, modelH0 = modelH0, modelH1 = modelH1)
# using the population covariance matrix 
ap <- semPower.powerLav(type = 'a-priori', alpha = .05, power = .80,
                        Sigma = generated$Sigma, modelH0 = modelH0, modelH1 = modelH1)
summary(ap$power)
```

The output shows that 323 observations are required to detect a correlation between the factors of $r \leq .9$ with a power of 80% on alpha = .05. Note that there is only a single df, because the H1 model was also explicitly defined and just differs from the H0 model in a single parameter (namely the freely estimated correlation between the factors). If the H1 model is omitted from the function call, power will be determined relative to the saturated model, which in this example leads to 20 df and thus to a very different sample size required to detect the specified effect (namely 860 observations).

Beyond the results of power analysis, the result returned by `semPower.powerLav` also contains a number of additional information, in particular the generated population covariance matrix $\Sigma$ (`Sigma`) and the model-implied covariance matrix $\hat{\Sigma}$ (`SigmaHat`). If using `lavaan` to generate the population covariance matrix, it is a good idea to check whether the population model actually defined all parameters in line with the expectations. To verify, one can just fit the correct H1 model to the population covariance matrix which should yield a perfect fit and give the same parameter estimates used in the definition of the model string.
```{r eval=FALSE}
library(lavaan)
summary(sem(modelH1, sample.cov = ap$Sigma, 
            sample.nobs = 1000, sample.cov.rescale = FALSE), 
        stand = TRUE, fit = TRUE)
```
Note that the `sample.nobs` are arbitrary and that `sample.cov.rescale = FALSE` must added to prevent `lavaan` from modifying the provided covariance matrix.


# Power Plots {#plots}

Power plots show the implied power as function of some other variable. `semPower` provides two different types of power plots. One can either plot the achieved power to detect a certain effect over a range of different sample sizes (`semPower.powerPlot.byN`). Alternatively, one can plot the achieved power with a given $N$ to detect a range of different effect size magnitudes (`semPower.powerPlot.byEffect`).   

## Power by N for a given effect {#plotByN}
The function `semPower.powerPlot.byN` creates a plot showing the achieved power to detect a given effect on a given alpha error over a range of sample sizes. However, because it is difficult to specify diagnostic sample sizes for a given effect, the `semPower.powerPlot.byN` instead asks to provide the desired power range. For example, suppose we are interested in how the power to detect an effect corresponding to RMSEA = .05 changes as function of the number of observations N. We are interested in a power ranging from .05 to .99 (note that the power cannot be smaller than alpha). This is achieved by setting the arguments `power.min = .05` and `power.max = .99`. In addition, as in any a-priori power analysis, the type and magnitude of effect, the df, and the alpha error need to be defined: `effect = .05`, `effect.measure = 'RMSEA'`,`alpha = .05`, `df = 100`.

```{r figurePowerPlotByN, fig.cap = "Power as function of the N to detect RMSEA >= .05"}
semPower.powerPlot.byN(effect = .05, effect.measure = 'RMSEA', 
                       alpha = .05, df = 100, power.min = .05, power.max = .99)
```

This shows that a model with an associated RMSEA = .05 is rejected with a very high power when N > 250, whereas power is small when N < 100. 



## Power by the magnitude of effect for a given N {#plotByEffect}
The function `semPower.powerPlot.byEffect` creates a plot showing the achieved power at a given sample size over a range of effect sizes. For example, suppose we are interested in how the power at N = 500 changes as function of effect size magnitude, corresponding to an RMSEA ranging from .001 to .10. This is achieved by setting the arguments `effect.measure = 'RMSEA'`, `effect.min = .001` and `effect.max = .10`. In addition, as in any post-hoc power analysis, the sample size, the df, and the alpha error need to be defined: `effect = .05`, `effect.measure = 'RMSEA'`, `alpha = .05`, `df = 100`.


```{r figurePowerPlotByEffect, fig.cap = "Power as function of the RMSEA with N = 500."}
semPower.powerPlot.byEffect(effect.measure = 'RMSEA', alpha = .05, N = 500, 
                            df = 100, effect.min = .001, effect.max = .10)
```

This shows that with N = 500, a model with an associated RMSEA > .04 is detected with a very high power, whereas power for RMSEA < .03 is rather modest. 



# Further topics {#furtherTopics}

## Obtain the model degrees of freedom {#getDf}

Knowledge of the degrees of freedom (df) is required to perform a power analysis. When power refers to the comparison of two explicitly specified, nested models, the resulting df are just the difference between the df of the two models (or equivalently, the number of free parameters removed by the more restrictive model). When power is requested for the comparison of a hypothesized model to the saturated model, the model df are given by 

$$df = p\cdot(p+1)/2 - q$$

where $p$ is the number of observed variables and $q$ is the number of free parameters of the hypothesized model. The [next chapter](#comparisonModel) discusses the difference between a saturated and less restricted H1 comparison model in greater detail.

To obtain the df in a typical SEM, one needs to count (a) loadings, (b) indicator residual variances, and (c) covariance/regression parameters between factors and between indicator residuals.^[Note that factor (residual) variances can be omitted, because each factor needs to be assigned a scale, so one free parameter is lost for each factor anyway. Concerning the number of free parameters, it does not matter whether factors are identified by fixing their variance or by fixing a loading.] For instance, consider a correlated two factor CFA model with each factor measured by 4 indicators and no secondary loadings or residual correlations. Thus, there are (a) $2\cdot4$ loadings, (b) 8 indicator residual variances, and (c) 1 covariance between the factors, which results in 17 free parameters. The df are thus $8\cdot9/2 - 17 = 19$.

If you are unsure or have a more complicated model (or both), `semPower` also includes a utility function called `semPower.getDf` that determines the df for a model provided in the `lavaan` syntax (note that this function requires the `lavaan` package). For the model sketched above, running 

```{r eval=FALSE}
# define model using standard lavaan syntax
lavmodel <- '
f1 =~ x1 + x2 + x3 + x4
f2 =~ x5 + x6 + x7 + x8
'
# obtain df
semPower.getDf(lavmodel)
```

gives 19 as result, which matches what we calculated by hand. Similarly, when adding the null hypothesis of a zero correlation between the factors (`f1 ~~ 0*f2`), one additional df is gained:

```{r eval=FALSE}
# define model using standard lavaan syntax
lavmodel <- '
f1 =~ x1 + x2 + x3 + x4
f2 =~ x5 + x6 + x7 + x8
f1 ~~ 0*f2
'
# obtain df
semPower.getDf(lavmodel)
```


`semPower.getDf` can also be used to obtain the df in multigroup settings by setting the arguments `nGroups` (and `group.equal` for models involving equality constraints).

```{r eval=FALSE}
# configural invariance
semPower.getDf(lavmodel, nGroups = 3)
# metric invariance
semPower.getDf(lavmodel, nGroups = 3, group.equal = c('loadings'))
# scalar invariance
semPower.getDf(lavmodel, nGroups = 3, group.equal = c('loadings', 'intercepts'))
```

In determining the df, `semPower.getDf` also accounts for any additional restrictions on (defined) parameters and should generally match the df reported by `lavaan`, excempt the case that a $\chi^2$ statistic is employed that include a correction of the df (such as the third-moment adjustment by Lin and Bentler, 2012).


## Definition of the comparison (H1) model {#comparisonModel}
Power analyses always refer to a certain hypothesis (H0) that is to be rejected, so that an (implicit) alternative hypothesis (H1) shall be accepted. All convenience functions performing a  [model-based power analysis](#modelBasedPower) accept the `comparison` argument which sets the relevant comparison (H1) model, which can either refer to a saturated model (`comparison = 'saturated'`) or to a model (`comparison = 'restricted'`) that omits the H0 constraints reflecting the hypothesis of interest, but is otherwise identical to the H0 model. 

For instance, consider a two factor CFA model, where each factor is measured by 4 indicators, and suppose power is to be determined to detect a factor correlation of $r \geq .20$. A standard CFA model freely estimating the factor correlation involves 19 df. The relevant H0 model, however, would constrain the factor correlation to zero, and thus involves one additional and thus 20 df. 

To determine power to reject the H0 model, there are now two relevant comparison models defining the H1. One option would be to compare the H0 model against the saturated model (`comparison = 'saturated'`), so power analysis is based on 20 df. Practically speaking, one would just ask whether the model $\chi^2$ test associated with H0 model turns out significant. Alternatively, one might want to compare the H0 model against a model that is identical to the H0, except that it freely estimates the factor correlation (`comparison = 'restricted'`). The difference in the df between these two models (and the difference in model fit) now enter power analyses, which then would be based on $20 - 19 = 1$ df.

Either approach is valid. Usually, however, it is more sensible to compare the H0 model against a less restricted H1 model that only differs in the parameter(s) relevant for the tested hypothesis, which has the added benefit of a generally higher power. Thus, the functions performing [model-based power analyses](#modelBasedPower) by default use `comparison = 'restricted'`. Of note, the measures of effect given in the output of a power-analysis are based on the df provided to the respective power-analysis. Thus, in case of a restricted comparison model, these will generally differ from the ones obtained by just fitting the H0 model to some data, and will, in general, also not reflect the simple differences between the indices of the H0 and the H1 model.


## Multiple Group Models {#multipleGroups}
A common application of SEM is to fit a model simultaneously to multiple distinct groups (such as gender or age groups). If a power analysis concerning the hypothesis of whether the model as a whole describes the data is desired, the grouping structure is only relevant with respect to the  degrees of freedom of the test (see the [chapter on how to obtain the df](#getDf)), so a regular [model-free power analysis](#modelFreePower) can be performed. This also holds for the common case of invariance testing when the source of non-invariance is assumed to spread across many parameters and just the overall difference in terms of an effect size is of interest (see [power analysis for overall differences](#diffPower)).

Other hypotheses arising in multiple group settings are supported in most functions performing a [model-based power analyses](#modelBasedPower):

* See [`semPower.powerMI`](#powerMI) for various hypothesis concerning measurement invariance across groups (specifying non-invariant parameters).
* See [`semPower.powerCFA`](#powerCFA) to detect that the correlation between two factors differs across groups.
* See [`semPower.powerBifactor`](#powerBifactor) to detect that a correlation involving a bifactor differs across groups.
* See [`semPower.powerRegression`](#powerRegression) and [`semPower.powerPath`](#powerPath) to detect that a regression slope differs across groups.
* See [`semPower.powerMediation`](#powerMediation) to detect that a mediation effect differs across groups.


## Simulated power {#simulatedPower}
to be written




## Power analyses based on covariance matrices {#powerCov}

All [model-free power analyses](#modelFreePower) provided in `semPower` also accept [covariance matrices as input](#InputCovMatrix) and then determine the associated effect through trough the supplied covariance matrices. This offers a very high degree of flexibility, but obviously requires a specification of proper covariance matrices. In this section, various options to obtain these matrices are illustrated in detail, ordered by most to least cumbersome.

##### The full way {-}
One way is to employ some other SEM software to generate the population and the model-implied covariance matrix (note that a proper definition of the latter usually requires fitting the model to the population data). For illustration, let's consider how this could be done using `lavaan`. 

```{r eval=FALSE}
library(lavaan)

# define (true) population model
modelPop <- '
f1 =~ .8*x1 + .7*x2 + .6*x3
f2 =~ .7*x4 + .6*x5 + .5*x6
f1 ~~ 1*f1
f2 ~~ 1*f2
f1 ~~ 0.5*f2
x1 ~~ .36*x1
x2 ~~ .51*x2
x3 ~~ .64*x3
x4 ~~ .51*x4
x5 ~~ .64*x5
x6 ~~ .75*x6
'
# define (wrong) H0 model
modelH0<- '
f1 =~ x1 + x2 + x3
f2 =~ x4 + x5 + x6
f1 ~~ 0*f2
'
```

After loading the `lavaan` package, two lavaan model strings are defined. The first model string (`modelPop`) is used to define the population covariance matrix, so all model parameters are defined (at least implicitly, consult the lavaan documentation for defaults). Here, we define a model comprising two latent factors (each with variance of 1), that are correlated by .5. Each latent factor is measured through three observed indicators (x1 to x6) with (standardized) loadings ranging from .5 to .8. The second model string (`modelH0`) is used to define the hypothesized (wrong) H0 model. To obtain the model-implied covariance matrix, we need to fit this model to the population data; so this is basically an ordinary CFA model string with many free parameters. Note that this model contraints the correlation between the two latent factors to zero. When fitting this model to the population data defined previously, the model is thus wrong, since we defined the correlation between these factors in the population to be .50.

Having defined the model strings, we proceed by actually obtaining the relevant covariance matrices.

```{r eval=FALSE}
# get population covariance matrix; equivalent to a perfectly fitting model
covPop <- fitted(sem(modelPop))$cov

# get covariance matrix as implied by H0 model; note the nobs are arbitrary
fitH0 <- sem(modelH0, sample.cov = covPop, 
              sample.nobs = 250, sample.cov.rescale = FALSE, 
              likelihood='wishart')
df <- fitH0@test[[1]]$df
covH0 <- fitted(fitH0)$cov
```

`covPop` is now the covariance matrix in the population ($\Sigma$). To obtain the model implied covariance matrix ($\hat{\Sigma}$), we need to fit our hypothesized, wrong, H0 model (`modelH0`) to the population data (`covPop`). The model-implied covariance matrix then can be obtained by calling `covH0 <- fitted(fitH0)$cov`.

We are now in the position to use the obtained covariance matrix in power analyses. Let's do a post-hoc power-analysis assuming N = 1000 and alpha =.05 by calling `semPower.postHoc` with the arguments `SigmaHat = covH0` and `Sigma = covPop`.

```{r eval=FALSE}
ph <- semPower.postHoc(SigmaHat = covH0, Sigma = covPop, alpha = .05, N = 250, df = df)
summary(ph)
```

The output (which is omitted here) indicates that fitting the hypothesized model to the population data is  associated with a discrepancy of $F_0$ = 0.133 (or RMSEA = .121 or SRMR = .140 or ...) and that the power to reject the H0 model is very high, 1 - beta = .993. 

It is instructive to compare the expected chi-square (computed via the obtained $F_0$) with the chi-square model test statistics as reported by `lavaan` when fitting the H0 model using the same number of observations as requested in the power analysis:

```{r eval=FALSE}
fitmeasures(fitH0, 'chisq')
ph$fmin * (ph$N-1)
```

`fitmeasures(fitH0, 'chisq')` prints the model chi-square test as obtained by `lavaan` when fitting `modelH1` to the population data (`covPop`, see above). The line `ph$fmin * (ph$N-1)` computes the expected chi-square, i. e.,  $F_0$ multiplied by $(N - 1)$. Obviously, both values match (= 33.17).

##### Using semPower.powerLav {-}

The process illustrated above can be simplified by calling [`semPower.powerLav`](#powerLav). In essence, all we need now are the population model string (`modelPop`) and the H0 model string (`modelH0`). This leads to:

```{r eval=FALSE}
ph <- semPower.powerLav(type = 'post-hoc',
                        modelPop = modelPop, modelH0 = modelH0,
                        alpha = .05, N = 250)
summary(ph$power)
```

which - of course - yields the same results.

#####  Using semPower.powerLav in conjunction with semPower.genSigma  {-}

Instead of defining the population covariance matrix through a `lavaan` model string, It is often easier to obtain the population covariance matrix through the [`semPower.genSigma`](#genSigma) function, because this takes care for many intricacies such as the correct definition of the residual variances. In the scenario above, the following leads to the same results:

```{r eval=FALSE}
generated <- semPower.genSigma(Phi = .50, 
                               loadings = list(
                                 c(.8, .7, .6),
                                 c(.7, .6, .5)))
ph <- semPower.powerLav(type = 'post-hoc', 
                        Sigma = generated$Sigma, modelH0 = modelH0, 
                        alpha = .05, N = 250)
summary(ph$power)
```

##### Using semPower.powerCFA {-}

As the scenario above involves a model and a hypothesis that can be immediately handled by the [`semPower.powerCFA`](#powerCFA) convenience function, actually none of the above is required and could be achieved by a simple call to `semPower.powerCFA`:

```{r eval=FALSE}
ph <- semPower.powerCFA(type = 'post-hoc', 
                        comparison = 'saturated',
                        Phi = .5,
                        loadings = list(
                          c(.8, .7, .6),
                          c(.7, .6, .5)),
                        alpha = .05, N = 250)
summary(ph$power)
```

In the background, `semPower.powerCFA` performs all the necessary steps outlined above, so the results will - of course - be the same. Note the argument `comparison = 'saturated'` was set so to obtain power to reject the H0 model when compared against the saturated model, which was also done in all examples above.


## Generate a covariance matrix and lavaan model strings {#generateCov}
Internally, all convenience functions performing a [model-based power analysis](#modelBasedPower) generate a population variance-covariance matrix and plug it along with proper model strings into the [`semPower.powerLav`](#powerLav) function, which in turn transforms a model-based power analysis into a [model-free power analyses](#modelFreePower) providing [population and model-implied covariance matrices](#InputCovMatrix) as input. 

For situations not covered in the `semPower` convenience functions, greater flexibility is offered when calling either [`semPower.powerLav`](#powerLav) or a model free power analysis providing [population and model-implied means and covariance matrices](#InputCovMatrix) directly. A useful utility function described in this section is [semPower.generateSigma](#genSigma), which offers several ways to generate a population variance-covariance matrix (and means) based on the model matrices or model features, respectively.

`semPower.genSigma` expects model matrices as input parameters, so different matrices need to be provided depending on whether a CFA or a SEM model is specified. 

##### CFA model{-}
In the CFA model, the model implied variance-covariance matrix is given by
$$\Sigma = \Lambda \Phi \Lambda' + \Theta$$
where $\Lambda$ is the $p \cdot m$ loading matrix, $\Phi$ is the variance-covariance matrix of the $m$ factors, and $\Theta$ is the residual variance-covariance matrix of the observed variables. The means are
$$\mu = \tau + \Lambda \alpha$$
with the $p$ indicator intercepts $\tau$ and the $m$ factor means $\alpha$.

Thus, to generate a CFA model implied covariance matrix, `Phi` and `Lambda` (or respective  [shortcuts](#factorDefinition)) need to be provided. For instance, the following generates the implied covariance matrix for a three factor model with factor-correlations as defined in `Phi` and the loading matrix as defined in `Lambda`:

```{r eval=FALSE}
  Phi <- matrix(c(
    c(1.0, 0.5, 0.1),
    c(0.5, 1.0, 0.2),
    c(0.1, 0.2, 1.0)
  ), byrow = T, ncol=3)
  Lambda <- matrix(c(
    c(0.4, 0.0, 0.0),
    c(0.7, 0.0, 0.0),
    c(0.8, 0.0, 0.0),
    c(0.0, 0.6, 0.0),
    c(0.0, 0.7, 0.0),
    c(0.0, 0.4, 0.0),
    c(0.0, 0.0, 0.8),
    c(0.0, 0.0, 0.7),
    c(0.0, 0.0, 0.8)
  ), byrow = T, ncol = 3)
  
  gen <- semPower.genSigma(Phi = Phi, Lambda = Lambda)
```

`semPower.genSigma` returns a list comprising all model matrices (in the example above, `Lambda`, `Phi`, and the variance-covariance matrix of the manifest residuals, `Theta`) and the  model-implied variance-covariance matrix `Sigma`. In addition, various `lavaan` model strings are also returned:

* `modelPop` model string defining a population model corresponding to the model matrices. 
* `modelTrue` analysis model (yielding a perfect model fit) .
* `modelTrueCFA` only for SEM models: a pure CFA analyses model string omitting any regression relationships between the latent factors and rather allowing all latent factors to be correlated.  

Thus, plugging the generated variance-covariance matrix and the generated true model string into  `lavaan` yields estimates that mirror the defined population matrices: 

```{r eval=FALSE}
library(lavaan)
summary(sem(gen$modelTrue, 
            sample.cov = gen$Sigma, 
            sample.nobs = 1000,
            sample.cov.rescale = FALSE))

```

Instead of providing the complete loading matrix as argument for `Lambda`,  `semPower.genSigma`  also understands the shortcuts as described in detail [here](#factorDefinition).

If any of the arguments is provided as a list (e. g., `Lambda = list(Lambda1, Lambda2)`), multiple implied covariance matrices are returned. This is particularly useful for multigroup analyses, where the covariance matrices differ across groups. For instance, the following defines the same measurement model for two groups (by providing a single argument to `Lambda`), but defines different factor correlations for the groups, so that two model-implied covariance matrices are returned:

```{r eval=FALSE}
  Phi1 <- matrix(c(
     c(1.0, 0.5, 0.1),
     c(0.5, 1.0, 0.2),
     c(0.1, 0.2, 1.0)
  ), byrow = T, ncol=3)
  Phi2 <- matrix(c(
     c(1.0, 0.6, 0.2),
     c(0.6, 1.0, 0.3),
     c(0.2, 0.3, 1.0)
  ), byrow = T, ncol=3)
  
  gen <- semPower.genSigma(Phi = list(Phi1, Phi2), Lambda = Lambda)
```

##### SEM model {-#semDefinition} 
If the model-implied covariance-matrix is rather to be determined from a structural equation model (instead a CFA model), `semPower.genSigma` expects as arguments 
`Beta`, `Psi`, and `Lambda` (or respective  [shortcuts](#factorDefinition)). In the structural equation model, the model-implied covariance matrix is given by

$$\Sigma = \Lambda (I - \mathbf{B})^{-1} \Psi [(\mathbf{I} - \mathbf{B})^{-1}]'  \Lambda' + \Theta$$
where $\mathbf{B}$ is the $m \cdot m$ matrix containing the regression slopes and $\Psi$ is the (residual) variance-covariance matrix of the $m$ factors. The means are
$$\mu = \tau + \Lambda (\mathbf{I} - \mathbf{B})^{-1} \alpha$$

The structural part of the model is primarily defined through Beta ($\mathbf{B}$). As an example, suppose there are four factors ($X_1$, $X_2$, $X_3$, $X_4$), and Beta is defined as follows:
$$
\begin{array}{lrrr}
    & X_1 & X_2 & X_3 & X_4\\
X_1 & 0.0 & 0.0 & 0.0 & 0.0 \\
X_2 & 0.0 & 0.0 & 0.0 & 0.0  \\
X_3 & 0.2 & 0.3 & 0.0 & 0.0  \\
X_4 & 0.3 & 0.5 & 0.0 & 0.0  \\
\end{array}
$$
Each row specifies how a particular factor is predicted by the available factors,
so the above implies the following regression relations:

$$
X_1 = 0.0 \cdot X_1 +  0.0 \cdot X_2 + 0.0 \cdot X_3 + 0.0 \cdot X_4 \\
X_2 = 0.0 \cdot X_1 +  0.0 \cdot X_2 + 0.0 \cdot X_3 + 0.0 \cdot X_4 \\
X_3 = 0.2 \cdot X_1 +  0.3 \cdot X_2 + 0.0 \cdot X_3 + 0.0 \cdot X_4 \\
X_4 = 0.3 \cdot X_1 +  0.5 \cdot X_2 + 0.0 \cdot X_3 + 0.0 \cdot X_4
$$

which simplifies to

$$
X_3 = 0.2 \cdot X_1 + 0.3 \cdot X_2 \\
X_4 = 0.3 \cdot X_1 + 0.5 \cdot X_2
$$

Psi ($\Psi$) defines whether there are (residual-)correlations between the factors. Suppose that $\Psi$ is
$$
\begin{array}{lrrr}
    & X_1 & X_2 & X_3 & X_4\\
X_1 & 1.0 & 0.3 & 0.0 & 0.0 \\
X_2 & 0.3 & 1.0 & 0.0 & 0.0 \\
X_3 & 0.0 & 0.0 & 1.0 & 0.2 \\
X_4 & 0.0 & 0.0 & 0.2 & 1.0 \\
\end{array}
$$

which implies a correlation between $X_1$ and $X_2$ of .3 and a residual correlation
between $X_3$ and $X_4$ of .2.

The scenario just described can be achieved by defining `Beta` and `Psi` accordingly and plugging these as input to `semPower.genSigma`:

```{r eval=FALSE}
Beta <- matrix(c(
  c(.00, .00, .00, .00),
  c(.00, .00, .00, .00),
  c(.20, .30, .00, .00),
  c(.30, .50, .00, .00)
), byrow = TRUE, ncol = 4)
Psi <- matrix(c(
  c(1, .30, .00, .00),
  c(.30, 1, .00, .00),
  c(.00, .00, 1, .20),
  c(.00, .00, .20, 1)
), byrow = TRUE, ncol = 4)

gen <- semPower.genSigma(Beta = Beta, Psi = Psi, Lambda = diag(ncol(Beta)))

```

In the example above, a manifest-variable only model is defined by setting `Lambda = diag(ncol(Beta))`. If providing a loading matrix or any of the shortcuts instead, a genuine SEM model is defined, as in the following example:

```{r eval=FALSE}
gen <- semPower.genSigma(Beta = Beta, Psi = Psi, 
                         loadM = .5, nIndicator = c(5, 4, 5, 6))

```

`semPower.genSigma` again returns a list comprising all model matrices (here: `Lambda`, `Beta`, `Psi`, and the variance-covariance matrix of the manifest residuals, `Theta`) and the  model-implied variance-covariance matrix `Sigma`. In addition, the same `lavaan` model strings as in the CFA case are returned. Thus, again, plugging the generated variance-covariance matrix and the generated true model string into  `lavaan` yields estimates that mirror the defined population matrices: 

```{r eval=FALSE}
summary(sem(gen$modelTrue, 
            sample.cov = gen$Sigma, 
            sample.nobs = 1000,
            sample.cov.rescale = FALSE))
```

In the SEM case, `semPower.genSigma` also returns a pure CFA model which just estimates the factors and allows these to correlate, but discards any regression relationships between the factors:

```{r eval=FALSE}
summary(sem(gen$modelTrueCFA, 
            sample.cov = gen$Sigma, 
            sample.nobs = 1000,
            sample.cov.rescale = FALSE))
```



# References {-}

* Browne, M. W., & Cudeck, R. (1992). Alternative ways of assessing model fit. *Sociological Methods & Research, 21*, 230–258.

* Jöreskog, K. G., & Sörbom, D. (1984). *LISREL VI user’s guide* (3rd ed.). Mooresville: Scientific Software.

* Lin, J., & Bentler, P. M. (2012). A third moment adjusted test statistic for small sample factor analysis. *Multivariate Behavioral Research, 47(3),* 448-462.

* McDonald, R. P. (1989). An index of goodness-of-fit based on noncentrality. *Journal of Classification, 6*, 97–103.

* MacCallum, R. C., Browne, M. W., & Sugawara, H. M. (1996). Power analysis and determination of sample size for covariance structure modeling. *Psychological Methods, 1*, 130–149.

* Moshagen, M., & Erdfelder, E. (2016). A new strategy for testing structural equation models. *Structural Equation Modeling, 23*, 54–60.

* Rosseel, Y. (2012). lavaan: An R package for structural equation modeling. *Journal of Statistical Software, 48,* 1-36.

* Steiger, J. H. (1990). Structural model evaluation and modification: An interval estimation approach. *Multivariate Behavioral Research, 25*, 173–180.

* Steiger, J. H., & Lind, J. C. (1980). *Statistically based tests for the number of common factors*. Presented at the Annual meeting of the Psychometric Society, Iowa City.

